{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project - Who Wrote This? \n",
    "... or how to identify the author of anonymous articles using natural languange processing.\n",
    "\n",
    "Stefan Dittforth  \n",
    "February 27th, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of contents\n",
    "\n",
    "1. [Project Overview](#Project Overview)\n",
    "2. [Data Acquisition](#Data Acquisition)\n",
    "3. [Exploring the Data Set](#Exploring the Data Set)\n",
    "4. [Cleaning the Dataset](#Cleaning the Dataset)\n",
    "5. [Balancing the Data Set](#Balancing the Data Set)\n",
    "6. [Models](#Models)  \n",
    "    6.1. [Model 1: The Baseline Model - Just Guess the Author](#Model 1 The Baseline Model)  \n",
    "    6.2. [Model 2: Basic Article Metrics](#Model 2 Basic Article Metrics)  \n",
    "    6.3. [Model 3: Bag-of-Words - Word Count](#Model 3 Bag-of-Words - Word Count)  \n",
    "    6.4. [Model 4: Bag-of-Words - TFIDF](#Model 4 Bag-of-Words - TFIDF)  \n",
    "    6.5. [Model 5: Bag-of-Words - Reduced Vocabulary](#Model 5 Bag-of-Words - Reduced Vocabulary)  \n",
    "    6.6. [Model 6: Bag-of-Words - Bigrams](#Model 6 Bag-of-Words - Bigrams)  \n",
    "    6.7. [Model 7: Bag-of-Words - Reduced Bigrams](#Model 7 Bag-of-Words - Reduced Bigrams)  \n",
    "    6.8. [Model 8: Learn Word Embeddings & CNN](#Model 8 Learn Word Embeddings & CNN)  \n",
    "    6.9. [Model 9: GloVe Word Embeddings & CNN](#Model 9 GloVe Word Embeddings & CNN)  \n",
    "7. [Conclusion](#Conclusion)\n",
    "8. [Let's have some fun: the WhoWroteThis app](#WhoWroteThis app)\n",
    "9. [Sources](#Sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------\n",
    "## Project Overview <a name=\"Project Overview\"></a>\n",
    "\n",
    "When writing, authors leave distinctive marks in their stories influenced by the style of their writing. Well know writers are famous for their techniques with which they express ideas and manipulate language. Erika Rasso published a well written introduction to [Famous Authors and Their Writing Styles](https://www.craftyourcontent.com/famous-authors-writing-styles/). We learn, for example, that Ernest Hemingway \"pioneered concise, objective prose in fiction—which had, up until then, primarily been used in journalism.\". Another author discussed in Rasso's article is Franz Kafka. His stories present \"surrealist, nightmarish writing in contemporary settings\" which invoke feelings of confusion and helplessness. Agatha Christie's style was influenced by \"mentions of war\". Furthermore, \"she utilized a variety of poisons to carry out the murders in her stories\". And being interested in archarology \"resulted in ancient artifacts and archaeologists being heavily featured in her novels\". A last author worthwhile to highlight here is Zora Neale Hurston. Her style is quite unique: \"She wrote in colloquial Southern dialects that mimicked the language she grew up hearing.\".\n",
    "\n",
    "Our intuition and experience as readers tells us that the style of writing is like a \"finger print\" that differentiates authors. If we come across a text with no information about the author or a text written under a preudonym, would we be able to tell the author based of the style of writing? An interesting article about [\"Who’s At The Keyboard? Authorship Attribution in Digital Evidence Investigations\"](www.utica.edu/academic/institutes/ecii/publications/articles/B49F9C4A-0362-765C-6A235CB8ABDFACFF.pdf) has been published by Carole E. Chaski. The article discusses the question to what extend text can be attributed to an author as part of crime investigations.\n",
    "\n",
    "In this project we will explore to what extend machine learning techniques can learn the style of writing for a set of authors. A learned system would then be given a text not seen before and tries to predict who the author is.\n",
    "\n",
    "This Jupyter Notebook has been structured in the sequential order in which the problem has been approached. The next few sections describe the data set that has been acquired and the data cleaning activities applied. This is then followed by the main section that discusses various machine learning models. The models utilize various natural language processing (NLP) and classification algorithms. The models represent different approaches to capture the \"uniqueness\" of writing of the different authors. The key metric against which we measure each model is the accuracy in correctly predicting the authors for a set of articles that have not been seen before (the test data set).\n",
    "\n",
    "Lastly, we will have some fun and use the best performing model to build a small WhoWroteThis application. The user can copy and paste any text into a text input field and ask the system to guess who the author is.\n",
    "\n",
    "#### A few notes on running this Notebook:\n",
    "\n",
    "The total run time for this notebook is 7.3 hours on a laptop with Intel i5-5300 CPU, 2.3GHz and 8GB RAM. The notebook requires a number of libraries. All required libraries are loaded in the first code cell. Make sure this completes without any error.\n",
    "\n",
    "----------------\n",
    "## Data Acquisition <a name=\"Data Acquisition\"></a>\n",
    "\n",
    "In order to allow the system to learn the writing characteristics of different authors we require a dataset that provides a large number of articles for individual authors. There are rich datasets for NLP research available in the public domain. A list, as an example, can be found [here](https://github.com/niderhoff/nlp-datasets). However, as part of this project we will build our own dataset. We will develop a web scraper that will collect articles from the publishing platform Medium. The articles on Medium seem to be reasonably long (at least several hundred words). There are enough authors that have published several hundreds articles. With this, it appears feasible to acquire a large enough data set to learn patterns in the writing characteristics to distinguish between individual authors.\n",
    "\n",
    "This approach has been chosen as an opportunity to develop practical experience not only in machine learning but also around data acquisition. In data science and machine learning the acquisition and preparation of high quality data is often the bigger challenge than the actual development of the machine learning system itself. In \"[Datasets Over Algorithms](https://www.edge.org/response-detail/26587)\" author Alexander Wissner-Gross notes that \n",
    "\n",
    ">*\"the average elapsed time between key [machine learning] algorithm proposals and corresponding advances was about eighteen years, whereas the average elapsed time between key dataset availabilities and corresponding advances was less than three years, or about six times faster, suggesting that datasets might have been limiting factors in the advances.\"*.\n",
    "\n",
    "Conveniently the website [Top Authors](https://topauthors.xyz/) has published a list of 300+ top Medium authors. The project folder contains the short script `get_list_of_Medium_authors.py` that has been used to extract the Medium URL for each author. The initial list of 300+ authors has been reduced to 25. The criteria for this reduction was the number of published articles. For the 25 authors there are at least 300 articles available. The Medium URLs for these authors can be found in file `Medium_authors_25.txt`.\n",
    "\n",
    "<img src=\"notebook/Top Authors.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<center>*getting a list of Medium authors*</center>\n",
    "\n",
    "The actual collection of the articles is done with the script `pull_Medium_articles.py`. The script performs two steps. First, it builds a list of all article URLs and for each article saves author URL and article URL in JSON format in the file `Medium_article_urls.json`. Below is an example how the entries for three articles look like.\n",
    "\n",
    "```javascript\n",
    "{\"author_URL\": \"https://medium.com/@tedr/latest\\n\",\n",
    " \"article_URL\": \"https://medium.com/season-of-the-witch/etiquette-and-the-cancer-patient-630a50047448?source=user_profile---------1----------------\"}\n",
    "{\"author_URL\": \"https://medium.com/@esterbloom/latest\\n\",\n",
    " \"article_URL\": \"https://medium.com/the-billfold/failing-at-shoplifting-life-with-kesha-bc2600b1f440?source=user_profile---------789----------------\"}\n",
    "{\"author_URL\": \"https://medium.com/@gassee/latest\",\n",
    " \"article_URL\": \"https://mondaynote.com/the-ny-times-un-free-at-last-df2eddba360b?source=user_profile---------281----------------\"}\n",
    "```\n",
    "\n",
    "The second part performs the actual download of the articles. The script reads the article URL saved in `Medium_article_urls.json`, navigates to the website and reads the text information from the html code. Each article is saved in text format in its own file. For each author a folder is generated that contains the articles for that author. Initially it was intended to store all articles in JSON format in one file. This turned out to be very cumbersome when troubleshooting the `pull_Medium_articles.py` script. Having a folder structure that allows to do quick visual inspections over the list of files in a file manager proved very helpful. In addition, the smaller article files made it easier to spot check the downloaded text information in a text editor.\n",
    "\n",
    "<table><tr><td><img src=\"notebook/article 1.PNG\" style=\"width: 300px\"></td>\n",
    "           <td><img src=\"notebook/article 2.PNG\" style=\"width: 300px\"></td>\n",
    "           <td><img src=\"notebook/article 3.PNG\" style=\"width: 300px\"></td></tr></table>\n",
    "<center>*It's not good working with text. Got stuck reading instead of getting on the with project.*</center>\n",
    "\n",
    "During research for this project several Python libraries for interacting with websites have been explored: [mechanize](https://pypi.python.org/pypi/mechanize/0.3.6), [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#), [scrapy](https://scrapy.org/) and the [Selenium WebDriver](https://www.seleniumhq.org/projects/webdriver/). Eventually the decision was made to use the Selenium WebDriver. The key reason for this was: the Medium website uses a two step login process. The users provides its email address and then receives a one time login link via this email. That made it difficult to automate the login via script and ruled out all the libraries that don't allow user interaction with the website.\n",
    "\n",
    "Once an article website is loaded, the required information can be pulled from the text attribute of specific html elements. The code snipped below shows the commands used to get the author name and the article text.\n",
    "\n",
    "```python\n",
    "author = self.browser.find_element_by_xpath('//a[@rel=\"author cc:attributionUrl\"]').text\n",
    "body = self.browser.find_element_by_xpath('//div[@class=\"postArticle-content js-postField js-notesSource js-trackedPost\"]').text\n",
    "```\n",
    "\n",
    "As shown in the code snippet above the right elements are addressed by their respective xpath. Finding these xpaths required a bit of trail and error. A valuable tool for this is the FireFox Inspector. It allows to inspect the code and structure of a website and to find the right path to the right html element.\n",
    "\n",
    "![Top Authors](notebook\\Firefox Inspector.jpg)\n",
    "<center>*finding the right xpath with Firefox Inspector*</center>\n",
    "\n",
    "After the `pull_Medium_articles.py` script completed, the folder `Medium_articles` containing all article files has been compressed into a ZIP archive to preserve storage. With [`zipfile`](https://docs.python.org/3/library/zipfile.html) Python provides a library to work with ZIP archives. Going forward in this Notebook we will make use of this library to work with the files directly within the ZIP archive without the need to extract the archive.\n",
    "\n",
    "Developing a web scraper script poses its own challenges. The initial idea is pretty straightforward: here is a list of URLs, go to each website, download the text part and save it in a file. As always, the pitfalls are discovered during implementation. Some time had to be invested to understand the structure of the Medium article websites and figure out the best way to find the right html elements that contain the required information. The Selenium WebDriver is not the most effective tool when it comes to scraping several thousand websites. The time to render each and every website adds up. An attempt has been made by parallelising the article download with multi-threading and spawning of several instances of the Firefox browser. This failed. It turned out that the fast sequence of websites caused Firefox to slowly consume all available memory and eventually Firefox stopped fetching new websites. In a parallelised version of the script the problem was only exaggerated. Finally, a pragamatic approach was taken and the script has been amended with the capability to continue the work where it has left off from a previous run. Over the course of several days the script has been restarted several times and eventually saved all articles.\n",
    "\n",
    "In defense for Selenium, it needs to be noted that Selenium first and foremost is a tool to automate testing of websites and not a tool for scaping several thousand websites. The primary goal behind the `pull_Medium_articles.py` script was to get the data for this capstone project and not to develop a sophisticated web scraper. In this respect Selenium did the job. Despite the challenges, developing the web scraper script has been a worthwhile learing experience. It provided an opportunity to develop practical experience around data acquisition.\n",
    "\n",
    "In the next section we will explore the data set. We will also check what data preparation or cleaning activities might be required before we can apply learning algorithms to the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------\n",
    "## Exploring the Data Set <a name=\"Exploring the Data Set\"></a>\n",
    "\n",
    "All articles have been downloaded in individual text files and into folders for each author. This folder and file structure has been archived into `Medium_articles.zip`. Let's go through the files and get an overview about the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this code block to load all the libraries we will need\n",
    "# throughout the Notebook. Keeping all library calls in one place at\n",
    "# beginning allows to run other code cells more independently and gives\n",
    "# immideate feedback about missing libraries\n",
    "\n",
    "import wget\n",
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import pprint\n",
    "import pickle\n",
    "import hashlib\n",
    "import time\n",
    "import itertools\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from cycler import cycler\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_article_from_zip_file(file_name):\n",
    "    \"\"\"\n",
    "    A generator function that reads the files in a zip archive one by one.\n",
    "    \"\"\"\n",
    "    zf = zipfile.ZipFile(file_name, 'r')\n",
    "    names = zf.namelist()\n",
    "    for name in names:\n",
    "        if name.endswith('.json'):\n",
    "            data = zf.read(name)\n",
    "            yield data\n",
    "    zf.close()\n",
    "    return\n",
    "\n",
    "# read all articles into a pandas data frame\n",
    "articles = pd.DataFrame(columns=['url', 'author', 'headline', 'body'])\n",
    "for article_file in get_next_article_from_zip_file('Medium_articles.zip'):\n",
    "    article = json.loads(article_file)\n",
    "    articles = articles.append(json.loads(article_file), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a list of authors and the number of articles for each author\n",
    "summary = articles[['author', 'body']].groupby('author').count()\n",
    "summary.columns = ['number of articles']\n",
    "display(summary.sort_values('number of articles', ascending=False))\n",
    "print('Total number of articles: {:,}'.format(len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleaning the Dataset <a name=\"Cleaning the Dataset\"></a>\n",
    "\n",
    "The articles were downloaded by extracting the 'text' attribute from html elements that contain the body text of the article. Many articles contain images, URLs to other pages, etc. We are interested in to see if the text extracts still contain html fragments we might need to clean out. We do a check by listing the articles that contain the '<' and '>' characters. Those two characters enclose HTML tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'<.*>' # find any HTML tag enclosing brackets\n",
    "counter = 0\n",
    "for index, article in articles.iterrows():\n",
    "    matches = re.finditer(regex, article['body'], re.DOTALL)\n",
    "    for matchNum, match in enumerate(matches):\n",
    "        counter = counter + 1\n",
    "        print('Match number: {}'.format(counter))\n",
    "        print('Match in article index: {}'.format(index))\n",
    "        print(article['url'])\n",
    "        # show the first and last 20 characters of the text found\n",
    "        print(match.group()[:20] + ' ... ' + match.group()[len(match.group())-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 156 occurrences for the '<' and '>' characters in over 18,500 articles. Doing some spot checks it appears that the tag brackets are genuine parts of the article text (for example the text is about HTML coding). We will leave them in for now. No need to clean them out.\n",
    "\n",
    "Let's have a look at some articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A few example articles:')\n",
    "for _ in range(1,4):\n",
    "    i = random.randrange(len(articles))\n",
    "    print('----------------------')\n",
    "    print('Index: {}'.format(i))\n",
    "    print('URL: {}'.format(articles.iloc[i]['url']))\n",
    "    print('Author: {}'.format(articles.iloc[i]['author']))\n",
    "    print('Headline: {}'.format(articles.iloc[i]['headline']))\n",
    "    print('Body: {}'.format(articles.iloc[i]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above randomly selects 3 articles. We run it a few times. At each run we spot check the text in the 'body' section. It appears that the text is clean and good to go for our next step.\n",
    "\n",
    "Typically NLP text cleaning tasks include removing punctuation characters. For now we will keep them. In one of our first classification approaches we will use them to engineer features around number and length of sentences and paragraphs in articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Balancing the Data Set <a name=\"Balancing the Data Set\"></a>\n",
    "\n",
    "As seen above in the table with the number of articles per authors the data set is skewed. The number of articles ranges from 1,948 for Nicole Dieker to 307 for Gary Vaynerchuk. To avoid that our system develops a bias towards authors with a high number of articles we will balance the data set. This will be done by keeping the number of articles for each author equal to the author with the lowest number of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the smallest number for articles for one author \n",
    "num_articles_to_keep = summary.min(axis=0)['number of articles']\n",
    "\n",
    "# only keep the smallest number of articles for each author\n",
    "indices_to_keep = []\n",
    "for author, num_of_articles in summary.iterrows():\n",
    "    indices_to_keep = indices_to_keep + \\\n",
    "                      list(articles[articles.author == author]\\\n",
    "                      [:num_articles_to_keep].index.values)\n",
    "articles = articles.iloc[indices_to_keep]\n",
    "\n",
    "# check number of articles for each author\n",
    "summary = articles[['author', 'body']].groupby('author').count()\n",
    "summary.columns = ['number of articles']\n",
    "display(summary.sort_values('number of articles', ascending=False))\n",
    "print('Total number of articles: {:,}'.format(len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Models <a name=\"Models\"></a>\n",
    "\n",
    "### Model 1: Baseline Model - Just Guess the Author <a name=\"Model 1 The Baseline Model\"></a>\n",
    "\n",
    "In the course of this notebook we will try several approaches to predict the author of a given text. To assess the quality of the prediction we will need to compare against some baseline. In our case we will simply do a random guess of who the author of an article is. This should get us in the order of $\\frac{1}{n}\\cdot100$ percent accuracy, where $n$ represents the number of authors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a global variable that will consolidate results from all models being tested\n",
    "results = pd.DataFrame(columns=['group', 'classifier', 'result', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "labels = articles[['author']]\n",
    "authors = labels.author.unique()\n",
    "y_predict = pd.DataFrame()\n",
    "y_predict['author'] = np.random.choice((authors), len(labels))\n",
    "score_test = accuracy_score(labels, y_predict)\n",
    "\n",
    "# report random guess results\n",
    "print('Just guess the author:')\n",
    "print('----------------------\\n')\n",
    "print('number of authors in data set: {}'.format(len(authors)))\n",
    "print('expected prediction accuracy around: {:.2f}%'.format(1/len(authors)*100))\n",
    "print('prediction accuracy score on data set: {:.2f}%'.format(score_test*100))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\ntotal run time: {:.2f} min\\n'.format(total_time/60))\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 1: Baseline Model',\n",
    "                          'classifier': 'random selection',\n",
    "                          'result': score_test,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Basic Article Metrics <a name=\"Model 2 Basic Article Metrics\"></a>\n",
    "\n",
    "Let's first come up with some simple metrics as features that might help us to predict the author of an article.  The metrics that come to mind are: the total, mean, median, min and max number of words in paragraphs, sentences and the article itself. When looking at different authors it appears that these features might be useful differentiators. Some authors have a tendency to longer articles. Others use longer sentences or shorter paragraphs.\n",
    "\n",
    "The code below \"tokenizes\" the articles into paragraphs, sentences and words. In addition, it counts the number of each. The tokenized article is returned as a nested dict object. The next code cell displays an example for a tokenized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "def tokenize_paragraphs(text):\n",
    "    paragraphs = [p for p in text.split('\\n')]\n",
    "    paragraphs_tokenized = []\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = tokenize_sentences(paragraph)\n",
    "        paragraphs_tokenized.append(sentences)\n",
    "    num_paragraphs = len(paragraphs_tokenized)\n",
    "    text_tokenized = {'num_paragraphs': num_paragraphs,\n",
    "                      'paragraphs': paragraphs_tokenized}\n",
    "    return text_tokenized\n",
    "\n",
    "def tokenize_sentences(paragraph):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    sentences_tokenized = []\n",
    "    for sentence in sentences:\n",
    "        words = tokenize_words(sentence)\n",
    "        sentences_tokenized.append(words)\n",
    "    num_sentences = len(sentences_tokenized)\n",
    "    paragraph_tokenized = {'num_sentences': num_sentences,\n",
    "                           'sentences': sentences_tokenized}\n",
    "    return paragraph_tokenized\n",
    "\n",
    "def tokenize_words(sentence):\n",
    "    words_tokenized = word_tokenize(sentence)\n",
    "    # remove punctuations from word list\n",
    "    words_tokenized = remove_punctuation(words_tokenized)\n",
    "    num_words = len(words_tokenized)\n",
    "    sentence_tokenized = {'num_words': num_words,\n",
    "                          'words': words_tokenized}\n",
    "    return sentence_tokenized\n",
    "\n",
    "def remove_punctuation(words_tokenized):\n",
    "    return [w for w in words_tokenized \n",
    "            if not re.fullmatch('[' + string.punctuation + '’“‘”–…' ']', w)]\n",
    "\n",
    "def get_article_metrics(body):\n",
    "    body_tokenized = tokenize_paragraphs(body)\n",
    "    paragraphs_sentences = []\n",
    "    sentences_words = []\n",
    "    for paragraph in body_tokenized['paragraphs']:\n",
    "        paragraphs_sentences.append(paragraph['num_sentences'])\n",
    "        for sentence in paragraph['sentences']:\n",
    "            sentences_words.append(sentence['num_words'])\n",
    "    paragraphs_sentences = pd.Series(paragraphs_sentences)\n",
    "    sentences_words = pd.Series(sentences_words)\n",
    "    metrics = {'article_num_paragraphs': body_tokenized['num_paragraphs'],\n",
    "               'article_num_sentences': paragraphs_sentences.sum(),\n",
    "               'article_num_words': sentences_words.sum(),\n",
    "               'paragraphs_min_sentences': paragraphs_sentences.min(),\n",
    "               'paragraphs_max_sentences': paragraphs_sentences.max(),\n",
    "               'paragraphs_mean_sentences': paragraphs_sentences.mean(),\n",
    "               'paragraphs_median_sentences': paragraphs_sentences.median(),\n",
    "               'sentences_min_words': sentences_words.min(),\n",
    "               'sentences_max_words': sentences_words.max(),\n",
    "               'sentences_mean_words': sentences_words.mean(),\n",
    "               'sentences_median_words': sentences_words.median()}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the basic metrics for each article\n",
    "articles_metrics = pd.DataFrame(list(articles['body'].map(get_article_metrics)),\n",
    "                                index=articles.index)\n",
    "articles_metrics.insert(loc=0, column='author', value=articles['author'])\n",
    "display(articles_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how a tokenized article looks like ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display an example for a tokenized article\n",
    "sample_article_index = 1 # just a random pick\n",
    "body_tokenized = tokenize_paragraphs(articles.loc[sample_article_index]['body'])\n",
    "print('\\nSample article tokenized:\\n')\n",
    "pp.pprint(body_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the basic metrics as features in a pandas data frame we will plot a few scatter diagrams to get a feeling for how strong they separate different authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_scatters(articles_list, authors, plot_features, plot_labels):\n",
    "    num_features = plot_features.__len__()\n",
    "    \n",
    "    # set up the figure \n",
    "    fig, axes = plt.subplots(nrows=num_features, ncols=num_features)\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    fig.suptitle(\"Feature Relationships\", fontsize=18)\n",
    "    \n",
    "    # set up the axes\n",
    "    for ax in axes.flat:\n",
    "        ax.tick_params(axis='both',\n",
    "                       bottom='off', top='off', left='off', right='off',\n",
    "                       labelbottom='off', labeltop='off',\n",
    "                       labelleft='off', labelright='off')\n",
    "        if ax.is_first_col():\n",
    "            ax.yaxis.set_ticks_position('left')\n",
    "            ax.tick_params(axis='y', left='on', labelleft='on')\n",
    "        if ax.is_last_col():\n",
    "            ax.yaxis.set_ticks_position('right')\n",
    "            ax.tick_params(axis='y', right='on', labelright='on')\n",
    "        if ax.is_first_row():\n",
    "            ax.xaxis.set_ticks_position('top')\n",
    "            ax.tick_params(axis='x', top='on', labeltop='on')\n",
    "        if ax.is_last_row():\n",
    "            ax.xaxis.set_ticks_position('bottom')\n",
    "            ax.tick_params(axis='x', bottom='on', labelbottom='on')\n",
    "        # set colour cycler, each author a different colour \n",
    "        ax.set_prop_cycle(cycler('color', ['blue', 'green', 'red', 'cyan',\n",
    "                                           'magenta', 'yellow', 'black', 'white']))\n",
    "        \n",
    "    # plot the scatter plots\n",
    "    for c in range(num_features):\n",
    "        for r in range(num_features):\n",
    "            if c == r:\n",
    "                # leave diagonal plots empty and remove ticks and labels\n",
    "                axes[r, c].xaxis.set_visible(False)\n",
    "                axes[r, c].yaxis.set_visible(False)\n",
    "            elif r > c:\n",
    "                axes[r, c].axis('off')\n",
    "            else:\n",
    "                axes[r, c].set_prop_cycle(None) # reset colour cycler\n",
    "                for author in authors:\n",
    "                    x = articles_list[articles_list.author == author][plot_features[c]]\n",
    "                    y = articles_list[articles_list.author == author][plot_features[r]]\n",
    "                    axes[r, c].scatter(x, y, s=2, alpha=0.5, label=author)\n",
    "\n",
    "    # place a legend below the top left plot\n",
    "    axes[0, 1].legend(bbox_to_anchor=(-1.1, 1), loc=3, prop={'size': 12}, markerscale=6)\n",
    "\n",
    "    # add axis labels in diagonal boxes\n",
    "    for i, label in enumerate(plot_labels):\n",
    "        axes[i,i].annotate(label, (0.5, 0.5), \n",
    "                           xycoords='axes fraction', ha='center', va='center',\n",
    "                           size=12)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure1 = plot_feature_scatters(articles_metrics,\n",
    "                ['Brad Feld', 'Jean-Louis Gassée',\n",
    "                 'James Altucher', 'Gary Vaynerchuk', 'Ethan Siegel'],\n",
    "                ['article_num_paragraphs', 'article_num_sentences', 'article_num_words',\n",
    "                 'paragraphs_max_sentences', 'paragraphs_median_sentences',\n",
    "                 'sentences_max_words', 'sentences_median_words'],\n",
    "                ['paragraphs /\\narticle', 'sentences /\\narticle', 'words /\\narticle',\n",
    "                 'max sentences /\\nparagraph', 'median\\nsentences /\\nparagraph',\n",
    "                 'max words /\\nsentence', 'median words /\\nsentence'])\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams[\"figure.figsize\"] = [13, 13]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plots above it appears that a separation to some degree would be possible. For example, the plot for \"words / article\" vs \"max words / sentence\" shows distinct clusters for individual authors. Although the clusters seem to overlap quite a bit.\n",
    "\n",
    "We will run a few classifier models and see what prediction accuracy we can achieve on a training data set. First, we will try the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels into separate data frame\n",
    "data = articles_metrics\n",
    "labels = data[['author']]\n",
    "data = data.drop('author', 1)\n",
    "\n",
    "# shuffle training test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=23)\n",
    "start_time = time.time()\n",
    "\n",
    "# decision tree prediction model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "score_train = accuracy_score(y_train, clf.predict(X_train))\n",
    "score_test = accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "# report decision tree results\n",
    "print('decision tree classifier:')\n",
    "print('-------------------------\\n')\n",
    "print('prediction accuracy score on training data set: {:.2f}%'.format(score_train*100))\n",
    "print('prediction accuracy score on test data set: {:.2f}%'.format(score_test*100))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\ntotal run time: {:.2f} min\\n'.format(total_time/60))\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 2: Basic Article Metrics',\n",
    "                          'classifier': 'decision tree',\n",
    "                          'result': score_test,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly 100% on the training data! That's awesome!\n",
    "\n",
    "Ok.... just joking ;-)\n",
    "\n",
    "On unseen data the accuracy is around 33% (the value will vary slightly with each run). I think that's not too bad for a model that tries to guess the author of an article only based on the number of words in articles, paragraphs and sentences. The model does not take anything of the article content into account. The 100% accuracy on the training data means the model is extremly biased. It would be interesting to see if a grid search can find a better set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(X_train, y_train):\n",
    "    # setup grid search\n",
    "    cv_sets = ShuffleSplit(n_splits=10, test_size = 0.25, random_state = 23)\n",
    "    learning_model = tree.DecisionTreeClassifier()\n",
    "    params = {'max_depth': list(range(1,30)),\n",
    "              'min_samples_split': list(range(2,50))}\n",
    "    scoring_fnc = make_scorer(accuracy_score, greater_is_better=True)\n",
    "    grid = GridSearchCV(estimator=learning_model,\n",
    "                        param_grid=params,\n",
    "                        scoring=scoring_fnc,\n",
    "                        cv=cv_sets,\n",
    "                        verbose=2)\n",
    "\n",
    "    # run grid search\n",
    "    grid = grid.fit(X_train, y_train)\n",
    "    best_clf = grid.best_estimator_\n",
    "    return best_clf\n",
    "\n",
    "def get_classifier():\n",
    "    '''\n",
    "    Get the best classifier. Run grid search and save the best classifier to\n",
    "    a pickle file. If the pickle file exists load the best classifier from file\n",
    "    instead of running grid search again. This saves time in subsequent runs \n",
    "    of the Notebook.\n",
    "    '''\n",
    "    pickle_file = 'Model 2 Basic Article Metrics - grid search best estimator.pickle'\n",
    "    if os.path.isfile(pickle_file):\n",
    "        with open(pickle_file,'rb') as f:\n",
    "            best_clf = pickle.loads(f.read())\n",
    "    else:\n",
    "        best_clf = grid_search(X_train, y_train)\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            s = pickle.dumps(best_clf)\n",
    "            f.write(s)\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "best_clf = get_classifier()\n",
    "score_test = accuracy_score(y_test, best_clf.predict(X_test))\n",
    "print('GridSearch prediction accuracy score on test data set: {:.2f}%'.format(score_test*100))\n",
    "print('best parameters:')\n",
    "print('    max_depth:{0}'.format(best_clf.get_params()['max_depth']))\n",
    "print('    min_samples_split:{0}'.format(best_clf.get_params()['min_samples_split']))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\ntotal run time: {:.2f} min\\n'.format(total_time/60))\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 2: Basic Article Metrics',\n",
    "                          'classifier': 'grid search decision tree',\n",
    "                          'result': score_test,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation of two parameters with grid search gains us three more percentage points to 36%. A little bit better.\n",
    "\n",
    "For comparison we will run the logistic regression algorithm and see if that yields better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# logistic regression prediction model\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train.values.ravel())\n",
    "score_train = accuracy_score(y_train, clf.predict(X_train))\n",
    "score_test = accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "# report logistic regression results\n",
    "print('\\nlogistic regression classifier:')\n",
    "print('-------------------------------\\n')\n",
    "print('prediction accuracy score on training data set: {:.2f}%'.format(score_train*100))\n",
    "print('prediction accuracy score on test data set: {:.2f}%'.format(score_test*100))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\ntotal run time: {:.2f} min\\n'.format(total_time/60))\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 2: Basic Article Metrics',\n",
    "                          'classifier': 'logistic regression',\n",
    "                          'result': score_test,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 32% accuracy the result is 4% below what has been achieved with an (grid search) optimised decision tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bag-of-Words - Word Count <a name=\"Model 3 Bag-of-Words - Word Count\"></a>\n",
    "\n",
    "In our next model we will use the vocabulary to build our prediction model. We will implement a the Bag-of-Words algorithm. Bag-of-Words is a very simple but often surprisingly effective algorithm. It takes all the different words in a training set and uses them as features. Each article is then transformed into a feature vector by marking the occurrence of each feature (=word). There are a few variations of Bag-of-Words. In it's basic form each vector counts the number of occurrences for each word. This is what we are going to use in this model 3. In the next section (model 4) will use the term frequency–inverse document frequency (TFIDF) approach to build the Bag-of-Words vectors.\n",
    "\n",
    "The code cell below defines a number of functions. The approach to define functions vs. directly executed code cells has been choosen to make the code reuseable for the next model in the following section.\n",
    "\n",
    "Besides the training, predicting and scoring functionalties the code implementes two additional capabilities: 1.) it allows trained classifiers to be saved to or loaded from disk. This saves time when the Notebook is run multiple times. The model needs to be trained only once. 2.) The model can be trained with different classifiers. During experiementation very often different classifiers (e.g. logistic regression, decision tree, SVM) are tested to evaluate which one is most suitable for a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(articles_train, pickle_file, classifiers, vectorizer_type):\n",
    "    \"\"\" This function is called to either load a trained model from disk\n",
    "    or to start the training. A trained model is loaded automatically \n",
    "    when the file exists.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(pickle_file):\n",
    "        vectorizer, trained_classifiers = load_trained_classifiers(pickle_file)\n",
    "    else:\n",
    "        vectorizer, trained_classifiers = train_classifiers(articles_train,\n",
    "                                                            classifiers,\n",
    "                                                            vectorizer_type)\n",
    "        save_trained_classifiers(vectorizer, trained_classifiers, pickle_file)\n",
    "    return vectorizer, trained_classifiers\n",
    "\n",
    "def load_trained_classifiers(pickle_file):\n",
    "    \"\"\" Load a trained model from disk.\n",
    "    \"\"\"\n",
    "    with open(pickle_file,'rb') as f:\n",
    "        saved_items = pickle.loads(f.read())\n",
    "    trained_classifiers = saved_items['trained_classifiers']\n",
    "    vectorizer = saved_items['vectorizer']\n",
    "    return vectorizer, trained_classifiers\n",
    "\n",
    "def save_trained_classifiers(vectorizer, trained_classifiers, pickle_file):\n",
    "    \"\"\" Save a trained model to disk. For Bag-of-Words there are two elements\n",
    "    that need to be saved: the classifiers that have been trained (e.g. logistic\n",
    "    regression, SVM) and the vectorizer. The vectorizer is the object that contains\n",
    "    the vocabulary that has been trained from the training data set. The vecorizer\n",
    "    is required later on to transform any given text into a vector representation.\n",
    "    \"\"\"\n",
    "    saved_items = {'trained_classifiers': trained_classifiers,\n",
    "                   'vectorizer': vectorizer}\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        s = pickle.dumps(saved_items)\n",
    "        f.write(s)\n",
    "\n",
    "def train_classifiers(articles_train, classifiers, vectorizer_type):\n",
    "    \"\"\" This function performs the training for each classifier. The desired \n",
    "    classifieres are given as a list of dictionaries. Each entry spacifies an\n",
    "    'algorithm' and the 'display_text'.\n",
    "    \"\"\"\n",
    "    vectorizer, X_train = fit_transform_vectors(articles_train, vectorizer_type)\n",
    "    y_train = articles_train[['author']]\n",
    "    trained_classifiers = []\n",
    "    for classifier in classifiers:\n",
    "        classifier_algorithm = classifier['algorithm']\n",
    "        display_text = classifier['display_text']\n",
    "        clf = train_classifier(X_train, y_train, classifier_algorithm)\n",
    "        trained_classifiers.append({'clf': clf, 'display_text': classifier['display_text']})\n",
    "    return vectorizer, trained_classifiers\n",
    "\n",
    "def train_classifier(data, labels, classifier_algorithm):\n",
    "    \"\"\" Train a single classifier.\n",
    "    \"\"\"\n",
    "    X_train = data\n",
    "    y_train = labels\n",
    "    clf = classifier_algorithm\n",
    "    clf = clf.fit(X_train, y_train.values.ravel())\n",
    "    return clf\n",
    "\n",
    "def fit_transform_vectors(articles, vectorizer):\n",
    "    \"\"\" Build the vocabulary during training.\n",
    "    \"\"\"\n",
    "    data = pre_process_articles(articles)\n",
    "    data_vectorized = vectorizer.fit_transform(data)\n",
    "    return vectorizer, data_vectorized\n",
    "\n",
    "def transform_vectors(articles, vectorizer):\n",
    "    \"\"\" Transform a list of articles into Bag-of-Words vectors. This is used\n",
    "    once a mopdel has been trained to transform articles in preparation for\n",
    "    prediction.\n",
    "    \"\"\"\n",
    "    articles = pre_process_articles(articles)\n",
    "    articles = vectorizer.transform(articles)\n",
    "    return articles\n",
    "\n",
    "def pre_process_articles(articles):\n",
    "    # join all articles to one string in preparation for hash calculation\n",
    "    data = ''.join([article['body'] for index, article in articles.iterrows()])\n",
    "    # calculate MD5 hash across all articles\n",
    "    m = hashlib.md5()\n",
    "    m.update(data.encode('utf-8'))\n",
    "    md5_digest = m.hexdigest()\n",
    "    pickle_file_name = 'pre_processed_articles_' + md5_digest + '.pickle'\n",
    "    # load from disk or pre-process articles and save to disk\n",
    "    if os.path.isfile(pickle_file_name):\n",
    "        with open(pickle_file_name,'rb') as f:\n",
    "            data = pickle.loads(f.read())\n",
    "    else:\n",
    "        data = [pre_process_text(article['body']) for index, article in articles.iterrows()]\n",
    "        with open(pickle_file_name, 'wb') as f:\n",
    "            s = pickle.dumps(data)\n",
    "            f.write(s)\n",
    "    return data\n",
    "\n",
    "# def pre_process_text(article): # buggy function\n",
    "#     \"\"\" Some preprocessing and cleaning tasks to be applied to the raw input text\n",
    "#     of an article.\n",
    "#     \"\"\"\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     porter = PorterStemmer()\n",
    "#     # tokenize article into words\n",
    "#     words = tokenize_words(article)['words']\n",
    "#     # remove stop words\n",
    "#     words = [word for word in words if not word in stop_words]\n",
    "#     # reduce words to its base and make all words lowercase\n",
    "#     words = set([porter.stem(word) for word in words]) <-- here's the bug, \n",
    "#                                                            funnily enough this unintended set() increases logistic \n",
    "#                                                            regression performance by 3%, need to investigate\n",
    "#                                                            this someday\n",
    "#     processed_text = ' '.join(words)\n",
    "#     return processed_text\n",
    "\n",
    "def pre_process_text(article): # fixed function\n",
    "    \"\"\" Some preprocessing and cleaning tasks to be applied to the raw input text\n",
    "    of an article.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    porter = PorterStemmer()\n",
    "    # tokenize article into words\n",
    "    words = tokenize_words(article)['words']\n",
    "    # lower case\n",
    "    words = [word.lower() for word in words]\n",
    "    # remove stop words\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    # reduce words to its base and make all words lowercase\n",
    "    words = [porter.stem(word) for word in words]\n",
    "    # remove punctuation from word list\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation + '—'))\n",
    "    words = [re_punc.sub('', w) for w in words]\n",
    "    # return as single string, required for, e.g., count or TFDIF vectorizers\n",
    "    processed_text = ' '.join(words)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will utilize the functions above to build our model. A model will be represented as a Python class. This design decision was driven by a view that our model should be \"productized\" to some degree. A user of our WhoWroteThis model will want to use a simple and straighforward interface. She should be able to pass in a list of articles in human readable text format and all the activities around text preprocessing, cleaning and vectorizing the data should be hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model_Bag_of_Words:\n",
    "    \n",
    "    def __init__(self, pickle_file):\n",
    "        \"\"\" The user of this class can specify the file name under which \n",
    "        the trained model will be saved to disk.\n",
    "        \n",
    "        As of now the model is fixed to autmatically train the following\n",
    "        classifiers: logistic regression, decision tree and support vector\n",
    "        machine (SVM).\n",
    "        \"\"\"\n",
    "        self.pickle_file = pickle_file\n",
    "        self.classifiers = [\n",
    "            {'algorithm': linear_model.LogisticRegression(),\n",
    "             'display_text': 'logistic regression'},\n",
    "            {'algorithm': tree.DecisionTreeClassifier(),\n",
    "             'display_text': 'decision tree'},\n",
    "            {'algorithm': svm.SVC(probability=True),\n",
    "             'display_text': 'SVM'}\n",
    "        ]\n",
    "        self.vectorizer = None\n",
    "        self.trained_classifiers = None\n",
    "\n",
    "    def train(self, articles_train, vectorizer_type):\n",
    "        \"\"\" Train the model on the various classifiers.\n",
    "        \"\"\"\n",
    "        self.vectorizer, self.trained_classifiers = train(articles_train,\n",
    "                                                          self.pickle_file,\n",
    "                                                          self.classifiers,\n",
    "                                                          vectorizer_type)\n",
    "\n",
    "    def predict(self, articles, classifier_type):\n",
    "        \"\"\" For a given list of articles and the classifier \n",
    "        type (logistic regression, decision tree and SVM) predict the\n",
    "        authors.\n",
    "        \"\"\"\n",
    "        for clf in self.trained_classifiers:\n",
    "            if clf['display_text'] == classifier_type:\n",
    "                break\n",
    "        articles = transform_vectors(articles, self.vectorizer)\n",
    "        prediction = pd.DataFrame({'author': clf['clf'].classes_,\n",
    "                                   'probability': clf['clf'].predict_proba(articles)[0]})\n",
    "        return prediction\n",
    "    \n",
    "    def score(self, articles):\n",
    "        \"\"\" Score prediction accuracy for a given list of articles.\n",
    "        \"\"\"\n",
    "        labels_actual = articles[['author']]\n",
    "        articles = transform_vectors(articles, self.vectorizer)\n",
    "        clf_predictions = []\n",
    "        for clf in self.trained_classifiers:\n",
    "            labels_predicted = clf['clf'].predict(articles)\n",
    "            score = accuracy_score(labels_actual, labels_predicted)\n",
    "            clf_predictions.append({'classifier': clf['display_text'],\n",
    "                                    'labels_predicted': labels_predicted,\n",
    "                                    'score': score})\n",
    "            print('{} classifier prediction accuracy score : {:.2f}%'.format(clf['display_text'], score*100))\n",
    "        return clf_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our Bag-of-Words model with word counts. Please note the strongly simplified interface by using the `Model_Bag_of_Words` class. It allows to work directly with the article texts and hides the machine learning \"pipeline\" (preprocessing, cleaning, vectorization, training of various classifiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_WordCount = Model_Bag_of_Words('Model 3 Bag-of-Words - Word Count.pickle')\n",
    "print('training the model\\n')\n",
    "model_WordCount.train(articles_train, CountVectorizer())\n",
    "print('number of training articles: {:,}'.format(len(articles_train)))\n",
    "print('size of the vocabulary = length of the feature vector: {:,}'.\\\n",
    "      format(len(model_WordCount.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_WordCount.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "clf_predictions = model_WordCount.score(articles_test)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\ntotal run time: {:.2f} min\\n'.format(total_time/60))\n",
    "\n",
    "# save results\n",
    "for clf in clf_predictions:\n",
    "    results = results.append({'group': 'Model 3: Bag-of-Words - Word Count',\n",
    "                              'classifier': clf['classifier'],\n",
    "                              'result': clf['score'],\n",
    "                              'time': total_time},\n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the logistic regression classifier gets us well above 92% prediction accuracy on the test data set. That's not bad! It appears that simply counting the word frequency is a very strong differentiator between authors. It seems to confirm our intuition that a person is able to differentiate between authors by what they write about. Different topics will naturally use different vocabulary. For future research (not pursued in this project) it might be interesting to research how the classifier works for authors that write about the same or similar topics.\n",
    "\n",
    "From the accuracy scores on training and test data we can see that the SVM classifier doesn't perform so well on this particular problem. This comes a bit as a surprise. According to the [scikit learn documentation](http://scikit-learn.org/stable/modules/svm.html) SVM is a classifier that generally is effective in high dimensional space and, more importantly, is \"still effective in cases where number of dimensions is greater than the number of samples\". In our case the number of 63,021 dimensions (= features) is significantly higher than the 5,756 samples (= number of training articles). Perhaps, because in our case the number of dimensions is almost ten times the number of samples make SVM unsuitable for our problem. The SVM library in scikit learn offers options for optimisation such as various classifiers (SVC, NuSVC and LinearSVC) and different kernel functions. For this project SVM has not been pursued further as the initial results (see above) are way below what the logistic regression classifier achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score as a single value doesn't give us much insight into what the model does well and where it fails. For this we will visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display confusion matrix\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams[\"figure.figsize\"] = [13, 13]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    im = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size=14)\n",
    "    cb = plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, size=14)\n",
    "    plt.yticks(tick_marks, classes, size=14)\n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max()/2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 size=12,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('true label', size=14)\n",
    "    plt.xlabel('predicted label', size=14)\n",
    "    \n",
    "# Compute confusion matrix\n",
    "y_test = articles_test[['author']]\n",
    "for clf in clf_predictions:\n",
    "    y_predict = clf['labels_predicted']\n",
    "    if clf['classifier'] == 'logistic regression':\n",
    "        break\n",
    "cnf_matrix = confusion_matrix(y_test, y_predict)\n",
    "np.set_printoptions(precision=2)\n",
    "class_names = list(set(labels['author']))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix allows us to see where exactly the misclassifications happening. We can see that Tim Boucher has been misclassified eight times as howardlindzon. Also, John Westenberg has been misclassified six times as Gary Vaynerchuk. The examples show that misclassification can be concentrated to a few classes. With this confusion matrices can provided valuable cues that are helpful for further fine tuning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Bag-of-Words - TFIDF <a name=\"Model 4 Bag-of-Words - TFIDF\"></a>\n",
    "\n",
    "With all the code for Model 3 developed with reusability in mind, we can now quickly implement another model that utilizes the term frequency–inverse document frequency (TFIDF) approach. In TFIDF, similar to word count, each word represents a feature. Each word is assigned a weight factor that is higher the more often a word occurs in an article. However, at the same time the weight of a word is reduced the more often it occurs across other articles. The idea behind TFIDF is that words that occur more across multiple articles carry less meaning then specialized words that occur within a smaller number of articles.\n",
    "\n",
    "In the code cell below we only  replace the `CountVectorizer()` vectorizer with the `TfidfVectorizer()` vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_TFIDF = Model_Bag_of_Words('Model 4 Bag-of-Words - TFIDF.pickle')\n",
    "print('training the model\\n')\n",
    "model_TFIDF.train(articles_train, TfidfVectorizer())\n",
    "print('number of training articles: {:,}'.format(len(articles_train)))\n",
    "print('size of the vocabulary = length of the feature vector: {:,}'.\\\n",
    "      format(len(model_TFIDF.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_TFIDF.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "clf_predictions = model_TFIDF.score(articles_test)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\ntotal run time: {:.2f} min\\n'.format(total_time/60))\n",
    "\n",
    "# save results\n",
    "for clf in clf_predictions:\n",
    "    results = results.append({'group': 'Model 4: Bag-of-Words - TFIDF',\n",
    "                              'classifier': clf['classifier'],\n",
    "                              'result': clf['score'],\n",
    "                              'time': total_time},\n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the TFIDF model with logistic regression is almost identical to the word count model described in the previous section. The TFIDF of words doesn't help to increase the accuracy in predicting the author. This can be explained by the fact that authors most probably write about specific topics (e.g. entrepreneurship, astrophysics). With this we can expect a high frequency of key topic words within the articles from the same author but less so across all articles in the training set. This in turn means that the IDF part of TFIDF has less impact on the learning. Which then means the TFIDF gets closer to what the word count does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Bag-of-Words - Reduced Vocabulary <a name=\"Model 5 Bag-of-Words - Reduced Vocabulary\"></a>\n",
    "\n",
    "During pre-processing model 3 extracts a vocabulary of over 63,000 words from 5,756 training articles. As the vocabulary is used as the feature vector this results in very large vectors. Large feature vectors require more time during the learning phase. In the following code cells we test to what degree a reduced vocabulary impacts our prediction accuracy. We only keep words in the vocabulary that occur at a minimum rate (specified by variable `min_occurrence`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(articles, ngrams):\n",
    "    data = pre_process_articles(articles)\n",
    "    vectorizer = CountVectorizer(ngram_range=(ngrams, ngrams))\n",
    "    data_vectorized = vectorizer.fit_transform(data)\n",
    "    vocabulary = pd.DataFrame({'token': vectorizer.get_feature_names(),\n",
    "                               'occurrence': np.asarray(data_vectorized.sum(axis=0)).ravel()})\n",
    "    vocabulary = vocabulary[['token', 'occurrence']].sort_values('occurrence', ascending=False)\n",
    "    vocabulary = vocabulary.reset_index().drop(['index'], axis=1)\n",
    "    return vocabulary\n",
    "\n",
    "def reduce_vocabulary(vocabulary, min_occurrence):\n",
    "    vocabulary_reduced = vocabulary[vocabulary['occurrence']>min_occurrence]\n",
    "    return vocabulary_reduced\n",
    "\n",
    "def vocabulary_to_dict(vocabulary):\n",
    "    # new index starting from 0 to total number of tokens\n",
    "    vocabulary = vocabulary.reset_index().drop(['occurrence'], axis=1)\n",
    "    # make column token the new index\n",
    "    vocabulary = vocabulary.set_index('token')\n",
    "    # transform into dict object\n",
    "    vocabulary_dict = vocabulary.to_dict()['index']\n",
    "    return vocabulary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "# reduce vocabulary\n",
    "min_occurrence = 200\n",
    "ngrams = 1\n",
    "vocab = get_vocabulary(articles_train, ngrams)\n",
    "vocab_reduced = reduce_vocabulary(vocab, min_occurrence)\n",
    "vocab_dict = vocabulary_to_dict(vocab_reduced)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# run the model\n",
    "model_ReducedVocab = Model_Bag_of_Words('Model 5 Bag-of-Words - Reduced Vocabulary.pickle')\n",
    "print('training the model\\n')\n",
    "model_ReducedVocab.train(articles_train, CountVectorizer(vocabulary=vocab_dict))\n",
    "print('number of training articles: {:,}'.format(len(articles_train)))\n",
    "print('size of the vocabulary = length of the feature vector: {:,}'.\\\n",
    "      format(len(model_ReducedVocab.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_ReducedVocab.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "clf_predictions = model_ReducedVocab.score(articles_test)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\ntotal run time: {:.2f} min\\n'.format(total_time/60))\n",
    "\n",
    "# save results\n",
    "for clf in clf_predictions:\n",
    "    results = results.append({'group': 'Model 5: Bag-of-Words - Reduced Vocabulary',\n",
    "                              'classifier': clf['classifier'],\n",
    "                              'result': clf['score'],\n",
    "                              'time': total_time},\n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of different thresholds for the minimum occurrence have been tested:\n",
    "\n",
    "|minimum occurrence|length feature vector|logistic regression accuracy|\n",
    "|------------------|---------------------|----------------------------|\n",
    "|      4           |              18,285 |          92.55%            |\n",
    "|     20           |               7,825 |          92.13%            |\n",
    "|    100           |               3,102 |          90.78%            |\n",
    "|    150           |               2,395 |          89.32%            |\n",
    "|    200           |               1,954 |          88.59%            |\n",
    "\n",
    "With a moderate threshold of four the logistic regression prediction accuracy remains with 92.55% at the same performance level as model 3. However, the length of the feature vector has been reduced dramatically to 18,285 tokens (compared to 63,021 tokens for model 3). Even with a drastic threshold of 200 the model still performs well above 88% prediction accuracy but with the benefit of a much smaller feature vector. This result shows that the feature vector can be reduced significantly without a large impact on the prediction accuray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Bag-of-Words - Bigrams <a name=\"Model 6 Bag-of-Words - Bigrams\"></a>\n",
    "\n",
    "The previous models 3 - 5 only utilized the frequency or count of individual words. These approaches do not try to represent any form of \"meaning\" in the text. Would our prediction accuracy increase if we find a way to represent relationships between words?\n",
    "\n",
    "One popular way to represent word relationships in NLP are bigrams (or more generalized n-grams). Bigrams count the occurrence of word pairs. The hypothesis is that certain word combinations used by individual authors in their writings will provide a strong indication of authorship.\n",
    "\n",
    "The following code cell tests this hypothesis. With `CountVectorizer(ngram_range=(2, 2))` we specify a count vectorizer that vectorizes a given text into bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# run the model\n",
    "model_Bigrams = Model_Bag_of_Words('Model 6 Bag-of-Words - Bigrams.pickle')\n",
    "print('training the model\\n')\n",
    "model_Bigrams.train(articles_train, CountVectorizer(ngram_range=(2, 2)))\n",
    "print('number of training articles: {:,}'.format(len(articles_train)))\n",
    "print('size of the bigram vocabulary = length of the feature vector: {:,}'.\\\n",
    "      format(len(model_Bigrams.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_Bigrams.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "clf_predictions = model_Bigrams.score(articles_test)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\ntotal run time: {:.2f} min\\n'.format(total_time/60))\n",
    "\n",
    "# save results\n",
    "for clf in clf_predictions:\n",
    "    results = results.append({'group': 'Model 6: Bag-of-Words - Bigrams',\n",
    "                              'classifier': clf['classifier'],\n",
    "                              'result': clf['score'],\n",
    "                              'time': total_time},\n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show one significant disadvantage of the bigram approach: the length of the feature vector literally \"explodes\" to 1,205,446 features. Given our 5,756 training articles we need to be aware that we are now cursed by dimensionality. In order for a classifier to learn effectively the number of training examples needs to grow with the dimensionality of the feature vector. The Wikipedia article about the [Curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Machine_learning) mentions an interesting rule of thumb: \"there should be at least 5 training examples for each dimension in the representation\". Applying this rule to our bigram model would mean to have somewhat over six million training articles. This will be impossible to get for our problem.\n",
    "\n",
    "Let's see how the prediction accuracy is impacted if we reduce the bigram feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Bag-of-Words - Reduced Bigrams <a name=\"Model 7 Bag-of-Words - Reduced Bigrams\"></a>\n",
    "\n",
    "As discussed in the results for the previous model, the use of bigrams during vectorization creates significantly larger feature vectors. This is a problem as it demands a significantly larger volume of training data for the classifier to learn something meaningful. In the following code cell we reduce the length of the feature vector by only keeping bigrams with a minimum number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# reduce vocabulary\n",
    "min_occurrence = 7\n",
    "ngrams = 2\n",
    "vocabulary = get_vocabulary(articles_train, ngrams)\n",
    "vocabulary = reduce_vocabulary(vocabulary, min_occurrence)\n",
    "vocabulary_dict = vocabulary_to_dict(vocabulary)\n",
    "\n",
    "# run the model\n",
    "model_Bigrams = Model_Bag_of_Words('Model 7 Bag-of-Words - Reduced Bigrams.pickle')\n",
    "print('training the model\\n')\n",
    "model_Bigrams.train(articles_train, CountVectorizer(vocabulary=vocabulary_dict,\n",
    "                                                    ngram_range=(2, 2)))\n",
    "print('number of training articles: {:,}'.format(len(articles_train)))\n",
    "print('size of the bigram vocabulary = length of the feature vector: {:,}'.\\\n",
    "      format(len(model_Bigrams.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_Bigrams.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "clf_predictions = model_Bigrams.score(articles_test)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\ntotal run time: {:.2f} min\\n'.format(total_time/60))\n",
    "\n",
    "# save results\n",
    "for clf in clf_predictions:\n",
    "    results = results.append({'group': 'Model 7: Bag-of-Words - Reduced Bigrams',\n",
    "                              'classifier': clf['classifier'],\n",
    "                              'result': clf['score'],\n",
    "                              'time': total_time},\n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of different thresholds for the minimum occurrence have been tested:\n",
    "\n",
    "|minimum occurrence|length feature vector|logistic regression accuracy|\n",
    "|------------------|---------------------|----------------------------|\n",
    "|      4           |              65,839 |          87.81%            |\n",
    "|      7           |              31,169 |          86.71%            |\n",
    "|     10           |              19,258 |          86.35%            |\n",
    "|     20           |               7,174 |          84.37%            |\n",
    "|    100           |                 657 |          70.24%            |\n",
    "\n",
    "With a threshold of four we can see that the prediction accuracy of the logistic regression classifier increases slightly compared to model 6. A possible explanantion for this behaviour is offered by the Hughes phenomenon. This phenomenon states that [\"the predictive power of a classifier or regressor first increases as number of dimensions/features used is increased but then decreases\"](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Machine_learning). It appears that we are far on the side of decreased performance. By reducing the lenght of the feature vector we have increased the power of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Learn Word Embeddings & CNN <a name=\"Model 8 Learn Word Embeddings & CNN\"></a>\n",
    "\n",
    "Another approach to numerically represent meaning in text is offered by using word embeddings. Word embedding algorithms such as Word2Vec and GloVe aim to find similar representations for words with similar meanings. Individual words are represented by vectors with tens or several hundred dimensions. These vectors are learned by processing a large amount of text through a neural network. During learning, the algorithm learns the embedding either by predicting the current word based on its context or by predicting the surrounding words given a current word. The result of the embeddings learning phase is a set of vectors where words with similar meanings have similar vectors. Essentially \"similarness\" is represented by the distance between word vectors. The vector representations of words with similar meanings have shorter distances between them.\n",
    "\n",
    "In recent years word embedding approaches have been one of the key breakthroughs in natural language processing.\n",
    "\n",
    "Based on this promising outlook we will test the word embeddings approach on our problem in the following two models. Model 8 will learn word embeddings as part of the convolutional neural network (CNN). In model 9 we will use pre-trained word embeddings.\n",
    "\n",
    "A very helpful resource for this topic was the book \"Deep Learning for Natural Language Processing, Develop Deep Learning Models for Natural Language in Python\" from Jason Brownlee. From this book  (page 163) the CNN used in this model has been taken. The model has been modified to a multiclass classifier (original model was a binary classifier for sentiment analysis).\n",
    "\n",
    "A key learning from implementing this model: The `one_hot` function from the `keras.preprocessing.text` library was originally used to convert the words in articles to integer values. This was done as preparation to train the CNN. However, `one_hot` does not encode collision free. During testing it has been found that against a text corpus with 54,619 unique words `one_hot` caused 22,793 collisions. Meaning 22,793 integers were not uniquely mapped to single words. It is believed that this high proportion of collisions significantly distorts the learning of the embedding CNN. To resolve this issue function `learn_vocabulary` has been implemented (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_article(article):\n",
    "    \"\"\" Some preprocessing and cleaning tasks to be applied to the raw input text\n",
    "    of an article.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = tokenize_words(article)['words']\n",
    "    # remove punctuation from word list\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation + '—'))\n",
    "    words = [re_punc.sub('', w) for w in words]\n",
    "    words = [word for word in words if not word=='']\n",
    "    # remove words that are not completely alphabetic\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    # transform to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    # remove stop words\n",
    "    words = [word for word in words if not word.lower() in stop_words]\n",
    "    # remove short words\n",
    "    words = [word for word in words if len(word)>1]\n",
    "    processed_article = words\n",
    "    return processed_article\n",
    "\n",
    "def learn_vocabulary(articles):\n",
    "    \"\"\" Learn the vocabulary for a given text corpus.\n",
    "    \n",
    "    The result is a dictionary that translates each unique word (the keys) into\n",
    "    unique integer values. This vocabulary can be be used to encode a text into\n",
    "    a series of integers. This is an activity required to prepare text for use\n",
    "    with word embeddings such as GloVe and Word2Vec.\n",
    "    \"\"\"\n",
    "    vocab = {'': 0}\n",
    "    curr_int = 1\n",
    "    for article in articles:\n",
    "        for word in article:\n",
    "            if not word in vocab.keys():\n",
    "                vocab[word] = curr_int\n",
    "                curr_int += 1\n",
    "    return vocab\n",
    "\n",
    "def encode_articles(pre_processed_articles, vocabulary):\n",
    "    \"\"\" Encode the words in a list of articles into integers.\n",
    "    \"\"\"\n",
    "    return [encode_article(article, vocabulary) for article in pre_processed_articles]\n",
    "\n",
    "def encode_article(article, vocabulary):\n",
    "    \"\"\" Encode a single word into its integer index.\n",
    "    The word-to-integer mapping needs to be given with the vocabulary dictionary.\n",
    "    \"\"\"    \n",
    "    return [vocabulary.get(word, 0) for word in article]\n",
    "\n",
    "def prep_training_data(articles_train):\n",
    "    \"\"\" Prepare the articles in the training data set for use with\n",
    "    word embeddings and CNN.\n",
    "    \"\"\"\n",
    "    # encode the author names to integers\n",
    "    author_to_int = {a: i for i, a in enumerate(set(articles_train['author']))}\n",
    "    authors_encoded = [author_to_int[author] for author in list(articles_train['author'])]\n",
    "    # pre-process the articles\n",
    "    pre_processed_articles = [pre_process_article(article['body']) for index, article in articles_train.iterrows()]\n",
    "    # encode words to integers\n",
    "    vocabulary = learn_vocabulary(pre_processed_articles)\n",
    "    vocab_size = len(vocabulary)\n",
    "    encoded_articles = encode_articles(pre_processed_articles, vocabulary)\n",
    "    # get length of longest article\n",
    "    article_lengths = pd.DataFrame([len(article) for article in encoded_articles],\n",
    "                                   columns=['length'])\n",
    "    max_length = article_lengths['length'].max()\n",
    "    # pad article lenghts to longest article\n",
    "    padded_articles = pad_sequences(encoded_articles, maxlen=max_length, padding='post')\n",
    "    return padded_articles, author_to_int, authors_encoded, vocabulary, vocab_size, max_length\n",
    "\n",
    "def prep_test_data(articles_test, vocabulary, max_length, author_to_int):\n",
    "    \"\"\" Prepare the articles in the test data set for use with\n",
    "    word embeddings and CNN.\n",
    "    \"\"\"\n",
    "    # pre-process the articles\n",
    "    pre_processed_articles = [pre_process_article(article['body']) for index, article in articles_test.iterrows()]\n",
    "    # encode words to integers\n",
    "    encoded_articles = encode_articles(pre_processed_articles, vocabulary)\n",
    "    # pad article lenghts to longest article \n",
    "    padded_articles = pad_sequences(encoded_articles, maxlen=max_length, padding='post')\n",
    "    # encode the author names to integers\n",
    "    authors_encoded = [author_to_int[author] for author in list(articles_test['author'])]\n",
    "    return padded_articles, authors_encoded\n",
    "\n",
    "def build_CNN(embedding):\n",
    "    \"\"\" The CNN is based on the CNN model with embedding layer shown in \n",
    "     \"Deep Learning for Natural Language Processing, Develop Deep Learning Models for\n",
    "     Natural Language in Python\" by Jason Brownlee, page 163. The model has been modified\n",
    "     to a multiclass classifier (original model was a binary classifier for sentiment\n",
    "     analysis).\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(25, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model_Embeddings_CNN:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def train(self, padded_articles, authors_encoded, model, file):\n",
    "        self.model = model\n",
    "        # fit CNN model and save learned model to file\n",
    "        self.model.fit(padded_articles, authors_encoded, epochs=10, verbose=1)\n",
    "        self.model.save(file)\n",
    "        \n",
    "    def predict(self, articles, vocabulary, author_to_int, max_length):\n",
    "        # pre-process the articles\n",
    "        pre_processed_articles = [pre_process_article(article) for article in articles]\n",
    "        # encode words to integers\n",
    "        encoded_articles = encode_articles(pre_processed_articles, vocabulary)\n",
    "        # pad article lenghts to longest article \n",
    "        padded_articles = pad_sequences(encoded_articles, maxlen=max_length, padding='post')\n",
    "        # make predictions\n",
    "        probabilities = self.model.predict(padded_articles, verbose=1)\n",
    "        predictions = pd.DataFrame({'author': list(author_to_int.keys()),\n",
    "                                    'probability': probabilities[0]})\n",
    "        index = predictions['probability'].idxmax()\n",
    "        author = predictions.loc[index]['author']\n",
    "        probability = predictions.loc[index]['probability']\n",
    "        return author, probability\n",
    "        \n",
    "    def score(self, padded_articles, authors_encoded):\n",
    "        # score the model\n",
    "        loss, accuracy = self.model.evaluate(padded_articles, authors_encoded, verbose=1)\n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# prepare the training data\n",
    "padded_articles, author_to_int, authors_encoded, vocabulary, vocab_size, max_length = prep_training_data(articles_train)\n",
    "\n",
    "# build and train the CNN model\n",
    "embedding = Embedding(vocab_size, 100, input_length=max_length)\n",
    "model = build_CNN(embedding)\n",
    "model.summary()\n",
    "model_Embeddings_CNN = Model_Embeddings_CNN()\n",
    "file = 'Model 8 Learn Word Embeddings & CNN.h5'\n",
    "print('training the model\\n')\n",
    "model_Embeddings_CNN.train(padded_articles, authors_encoded, model, file)\n",
    "\n",
    "# assess accuracy of the training data set\n",
    "print('\\nscoring the training data:\\n')\n",
    "loss, accuracy = model_Embeddings_CNN.score(padded_articles, authors_encoded)\n",
    "print('accuracy score : {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "# prepare the test data\n",
    "padded_articles, authors_encoded = prep_test_data(articles_test, vocabulary, max_length, author_to_int)\n",
    "\n",
    "# assess accuracy of the test data set\n",
    "print('\\nscoring the test data:\\n')\n",
    "loss, accuracy = model_Embeddings_CNN.score(padded_articles, authors_encoded)\n",
    "print('accuracy score : {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\ntotal run time: {:.2f} min\\n'.format(total_time/60))\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 8: Learn Word Embeddings & CNN',\n",
    "                          'classifier': 'CNN',\n",
    "                          'result': accuracy,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When predicting authors based on word embeddings learned from scratch we achieve a prediction accuracy on the test data of 13.18% (please note, results vary significantly with different runs of this notebook). This is a very low prediction accuray. This can be explained by the fact that the word embeddings were learned from only 5,756 articles (around 3.5 million words/tokens). Word embeddings are algorithms that require large amount of training data to properly learn relationships between words. Available pre-trained word embeddings were trained on significantly larger text corpora. For example the GloVe word embeddings file [glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip) contains 6 billion tokens and a vocabulary of 400,000 words and was trainind on the Wikipedia corpus.\n",
    "\n",
    "Let's use this model to make a prediction with the raw text as input. The code cell below demonstrates the simple interface to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "text = articles.iloc[3]['body']\n",
    "author = articles.iloc[3]['author']\n",
    "print('\\nI guess this text ...\\n')\n",
    "print(text)\n",
    "author, probability = model_Embeddings_CNN.predict([text], vocabulary, author_to_int, max_length)\n",
    "print('\\n... was written by {} with {:.2f}% confidence\\n'.format(author, probability*100))\n",
    "print('The actual author is {}'.format(author))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 9: GloVe Word Embeddings & CNN <a name=\"Model 9 GloVe Word Embeddings & CNN\"></a>\n",
    "\n",
    "In our last model we will utilize pre-trained word embeddings from the [GloVe algorithm](https://nlp.stanford.edu/projects/glove/). This is done based on the hypothesis that the GloVe word embeddings will provide much more accurate representations of word relationships than what we can train with our limited training data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_GloVe_weights(vocabulary):\n",
    "    \"\"\" Assign GloVe word vectors to a vocabulary of words\n",
    "    \"\"\"\n",
    "    vocab_size = len(vocabulary)\n",
    "    embeddings_index = load_trained_GloVe()\n",
    "    embedding_matrix = zeros((vocab_size, 300))\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_trained_GloVe():\n",
    "    \"\"\" Load the GloVe word vectors from file. In case the word vector file\n",
    "    doesn't exist, download it.\n",
    "    \"\"\"\n",
    "    # check if pre-trained word vectors file is available\n",
    "    file = 'glove.6B.zip'\n",
    "    if not os.path.isfile(file):\n",
    "        url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "        print('downloading pre-trained word vectors, url: {}'.format(url))\n",
    "        wget.download(url, file)\n",
    "    # load pre-trained GloVe embedding from ZIP file\n",
    "    embeddings_index = {}\n",
    "    with zipfile.ZipFile('glove.6B.zip', 'r') as zf:\n",
    "        with zf.open('glove.6B.300d.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0].decode(\"utf-8\")\n",
    "                coefs = asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# prepare the training data\n",
    "padded_articles, author_to_int, authors_encoded, vocabulary, vocab_size, max_length = prep_training_data(articles_train)\n",
    "\n",
    "# assign GloVe weights to words\n",
    "embedding_matrix = assign_GloVe_weights(vocabulary)\n",
    "\n",
    "# build and train the CNN model\n",
    "# TODO: test with trainable=True\n",
    "embedding = Embedding(vocab_size, 300, weights=[embedding_matrix],\n",
    "                      input_length=max_length, trainable=False)\n",
    "model = build_CNN(embedding)\n",
    "model.summary()\n",
    "model_GloVe = Model_Embeddings_CNN()\n",
    "file = 'Model 9 GloVe Word Embeddings & CNN.h5'\n",
    "print('training the model\\n')\n",
    "model_GloVe.train(padded_articles, authors_encoded, model, file)\n",
    "\n",
    "# assess accuracy of the training data set\n",
    "print('\\nscoring the training data:\\n')\n",
    "loss, accuracy = model_GloVe.score(padded_articles, authors_encoded)\n",
    "print('accuracy score : {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "# prepare the test data\n",
    "padded_articles, authors_encoded = prep_test_data(articles_test, vocabulary, max_length, author_to_int)\n",
    "\n",
    "# assess accuracy of the test data set\n",
    "print('\\nscoring the test data:\\n')\n",
    "loss, accuracy = model_GloVe.score(padded_articles, authors_encoded)\n",
    "print('accuracy score : {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\ntotal run time: {:.2f} min\\n'.format(total_time/60))\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 9: GloVe Word Embeddings & CNN',\n",
    "                          'classifier': 'CNN',\n",
    "                          'result': accuracy,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to a self trainined embeddings layer we achieve a significantly better prediction accuracy of 55.24% by using the pre-trained GloVe word vectors (prediction rate will vary with different runs of the notebook). However, we are still far-off from the the prediction performance of model 3 with 92.55% (count vectorizer and logistic regression classifier).\n",
    "\n",
    "It appears that word embeddings are not the right approach for our problem. Word embedding algorithms put emphasis on representation of meaning in text. The approach might be more useful if we want to classify articles by their content (e.g. astronomy, programming, finance, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion <a name=\"Conclusion\"></a>\n",
    "\n",
    "The chart below summarizes the test data prediction accuracy of the various models we have discussed throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_to_bar(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        width = rect.get_width()\n",
    "        if not np.isnan(width):\n",
    "            ax.text(rect.get_x() + width + 0.01, rect.get_y() + 0.1,\n",
    "                        '{:.2f}%'.format(width*100), fontsize=12,\n",
    "                        ha='left', va='center')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams[\"figure.figsize\"] = [13, 13]\n",
    "\n",
    "# group by model\n",
    "#results_percent = results.copy()\n",
    "#results_percent['result'] = results['result'] * 100\n",
    "result_groups = results.pivot(index='group', columns='classifier', values='result')\n",
    "# within each group we will show each classifier\n",
    "classifiers = ['CNN',\n",
    "               'random selection',\n",
    "               'grid search decision tree',\n",
    "               'decision tree',\n",
    "               'SVM',\n",
    "               'logistic regression']\n",
    "\n",
    "# set some general plotting parameters\n",
    "pos = list(range(len(list(result_groups.index))))\n",
    "height = 0.2\n",
    "fig, ax = plt.subplots(figsize=(10,12))\n",
    "# get some nice looking color scheme from https://coolors.co/162833-e2b161-d5573b-faffef-c7c9c8\n",
    "color_cycler = itertools.cycle(['#706c61', '#9e9889', '#c4b899', '#eddcaf', '#f4d681', '#f4ba1a'])\n",
    "\n",
    "# generate the bar charts\n",
    "x_values = result_groups['CNN']\n",
    "rects = plt.barh([p + 2 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "x_values = result_groups['random selection']\n",
    "rects = plt.barh([p + 2 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "x_values = result_groups['grid search decision tree']\n",
    "rects = plt.barh([p + 2 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "x_values = result_groups['decision tree']\n",
    "rects = plt.barh([p + 1 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "x_values = result_groups['SVM']\n",
    "rects = plt.barh([p + 2 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "x_values = result_groups['logistic regression']\n",
    "rects = plt.barh([p + 3 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "# set the chart's title\n",
    "ax.set_title('Prediction Accuracy', size=14)\n",
    "\n",
    "# configure the x axis\n",
    "ax.set_xlabel('accuracy predicting test data', size=14)\n",
    "ax.set_xlim([0,1.08])\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(14)\n",
    "\n",
    "# Set the labels for the y ticks\n",
    "model_names = list(result_groups.index)\n",
    "ax.set_yticklabels(list(model_names), size=14)\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:.0%}'.format(x))) \n",
    "\n",
    "# set the position of the y ticks\n",
    "ax.set_yticks([p + 0.5 for p in pos])\n",
    "\n",
    "# Adding the legend and showing the plot\n",
    "plt.legend(classifiers, loc='upper right', prop={'size': 12})\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see that the good old logistic regression classifier in model 3 has achieved a prediction accuracy of 92.55%. With that it has significantly outperformed all other classification approaches. This is a good reminder that, even with todays state of the art algorithms, it's always worth to look into simpler algorithms.\n",
    "\n",
    "The reason why the more sophisticated models 8 & 9 with word embeddings and CNNs didn't perform better, is probably because of the small size of training data available. Word embeddings require large volumes of training data to pick up the relationship between individual words. For the problem addressed in this project, predicting the authorship of a given text, the amount of training data will always be very limited. As shown in section [Exploring the Data Set](#Exploring the Data Set) the most active authors have produced a few hundred articles. Only six have produced 1,000 and more articles. If an application needs to be developed that needs to classify articles into topic categories (sport, gardening , science, ....) then the amount of training data will definitely be much higher as much more authors will write about the same topic. This would make the word embeddings approach worthwhile to revisit. \n",
    "\n",
    "It is worthwhile to note that model 2 achieved a prediction accuracy above 30%. This model was build only on features reflecting the number of words in articles, paragraphs and sentences. The model does not take anything of the article content into account. In future improvements this model might be used as part of an ensemble learner to improve the prediction accuracy.\n",
    "\n",
    "Throughout the development of this notebook some effort has been put into refactoring the code with re-usability in mind. Jupyter notebooks are a great tool for testing approaches and algorithms. Once a promising model has been identified, it is expected that a notebook like this will be handed over to an implementation developer who will transform the code into something that can be shipped to production. Models 3 - 9 have been implemented as Python classes with a consistent interface providing `train`, `predict` and `score` methods. Pre-processing activities have been \"packaged\" into separate functions which supports reuse and, hopefully, code readability. It is believed that instilling a \"production usable\" mindset in machine learning engineers will benefit the quick transition of new ideas into usable applications. The last section in this notebook demonstrates the simplicity with which the successful model 3 can be used in a simple application.\n",
    "\n",
    "An important learning from this project is to keep an eye on testing the different stages of the machine learning pipeline. For all of the models discussed above the raw text from the articles had to be transformed in several stages in order to be used with a classifier or a neural network. The initial text cleaning tasks (removal of punctuation and stop words, stemming, transform all words to lowercase) are fairly straightforward and can be validated by a simple visual inspection of a few samples. It gets a bit more tricky when text is transformed into numbers such as word counts or, even more so, integers that act as pointers to word vectors. Are the numbers I now see are integer indexes or still the word counts from the model I tried earlier? Am I sure that each word/token has now been encoded with its unique integer value? The challenge with machine learning implementation is that bugs not necessarily manifest themselves in hard stops of the code. How do you know whether the reported prediction accuracy of X% is because of the limits of the model or because of a bug in your vectorization code? To address this it is worthwhile for machine learning engineers to be mindful about testability of the different stages of a machine learning system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Notebook_total_time = time.time() - Notebook_start_time\n",
    "print('total run time for this Notebook: {:.1f} hours'.format(Notebook_total_time/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's have some fun: the WhoWroteThis app <a name=\"WhoWroteThis app\"></a>\n",
    "\n",
    "Now that we have a working prediction model let's build a little app that makes use of the model we have trained and tested.\n",
    "\n",
    "Run the cell below and follow the instructions shown in the output area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_button_clicked(button):\n",
    "    test_article = pd.DataFrame([text_input_box.value], columns=['body'])\n",
    "    predictions = model_WordCount.predict(test_article, 'logistic regression')\n",
    "    index = predictions['probability'].idxmax()\n",
    "    author = predictions.loc[index]['author']\n",
    "    probability = predictions.loc[index]['probability']*100\n",
    "    print('I\\'m {:.0f} percent certain that this was written by... {}.'.format(probability, author))\n",
    "\n",
    "text_input_box = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Please paste an article here.',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='700px', height='300px')\n",
    ")\n",
    "button = widgets.Button(\n",
    "    description='Who wrote this?',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    ")\n",
    "\n",
    "print('From the list of Medium authors below pick one. Select an article and copy the')\n",
    "print('text in the text box below. Press the \"Who wrote this?\" button and see whether')\n",
    "print('the right author is predicted.\\n')\n",
    "with open('Medium_authors_25.txt','r') as f:\n",
    "    author_urls = f.read()\n",
    "print(author_urls)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(text_input_box, button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sources <a name=\"Sources\"></a>\n",
    "\n",
    "The following sources served as inspiration for this project and provided valuable guidance for implementing NLP systems. \n",
    "\n",
    "[1] [Famous Authors and Their Writing Styles](https://www.craftyourcontent.com/famous-authors-writing-styles/)\n",
    "\n",
    "[2] [\"Who’s At The Keyboard? Authorship Attribution in Digital Evidence Investigations\"](www.utica.edu/academic/institutes/ecii/publications/articles/B49F9C4A-0362-765C-6A235CB8ABDFACFF.pdf)\n",
    "\n",
    "[3] [\"How a Computer Program Helped Show J.K. Rowling write A Cuckoo’s Calling\"](https://www.scientificamerican.com/article/how-a-computer-program-helped-show-jk-rowling-write-a-cuckoos-calling/)\n",
    "\n",
    "[4] [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)\n",
    "\n",
    "[5] [scikit learn documentation: confusion matrix](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)\n",
    "\n",
    "[6] [Deep Learning for Natural Language Processing, Develop Deep Learning Models for your Natural Language Problems](https://machinelearningmastery.com/deep-learning-for-nlp/)\n",
    "\n",
    "[7] [Deep Learning for NLP Best Practices](http://ruder.io/deep-learning-nlp-best-practices/index.html#wordembeddings)\n",
    "\n",
    "[8] [Multi-Class Classification Tutorial with the Keras Deep Learning Library](https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
