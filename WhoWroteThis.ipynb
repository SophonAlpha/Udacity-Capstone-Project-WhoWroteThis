{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project - Who Wrote This? \n",
    "... or how to use natural languange processing to identify the author of anonymous articles.\n",
    "\n",
    "Stefan Dittforth  \n",
    "February 27th, 2018\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "\n",
    "[1] [\"Who’s At The Keyboard? Authorship Attribution in Digital Evidence Investigations\"](www.utica.edu/academic/institutes/ecii/publications/articles/B49F9C4A-0362-765C-6A235CB8ABDFACFF.pdf): good reference regarding number of authors and text volume.\n",
    "\n",
    "[2] [\"How a Computer Program Helped Show J.K. Rowling write A Cuckoo’s Calling\"](https://www.scientificamerican.com/article/how-a-computer-program-helped-show-jk-rowling-write-a-cuckoos-calling/): some good ideas for feature to analyse.\n",
    "\n",
    "\n",
    "## Research Notes:\n",
    "\n",
    "- test approaches suggested in [1]\n",
    "- measure average word lentgh per article, distribution of word lengths, the 100 most common words, distribution of character 4-grams, word bigrams\n",
    "- test with support vector machines\n",
    "- using word embeddings as a much rich representation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "## Data Acquisition\n",
    "\n",
    "In order to allow the system to learn the writing characteristics of different authors we require a dataset that provides a large number of articles for individual authors. There are rich datasets for NLP research available in the public domain. A list, as an example, can be found [here](https://github.com/niderhoff/nlp-datasets). However, as part of this project we will build our own dataset. We will develop a web scraper that will collect articles from the publishing platform Medium. The articles on Medium seem to be reasonably long (at least several hundred words). There are enough authors that have published several hundreds articles. With this, it appears feasible to acquire a large enough data set to learn patterns in the writing characteristics to distinguish between individual authors.\n",
    "\n",
    "This approach has been chosen as an opportunity to develop practical experience not only in machine learning but also around data acquisition. In data science and machine learning the acquisition and preparation of high quality data is often the bigger challenge than the actual development of the machine learning system itself. In \"[Datasets Over Algorithms](https://www.edge.org/response-detail/26587)\" author Alexander Wissner-Gross notes that \n",
    "\n",
    ">*\"the average elapsed time between key [machine learning] algorithm proposals and corresponding advances was about eighteen years, whereas the average elapsed time between key dataset availabilities and corresponding advances was less than three years, or about six times faster, suggesting that datasets might have been limiting factors in the advances.\"*.\n",
    "\n",
    "Conveniently the website [Top Authors](https://topauthors.xyz/) has published a list of 300+ top Medium authors. The project folder contains the short script `get_list_of_Medium_authors.py` that has been used to extract the Medium URL for each author. The initial list of 300+ authors has been reduced to 25. The criteria for this reduction was the number of published articles. For the 25 authors there are at least 300 articles available. The Medium URLs for these authors can be found in file `Medium_authors_25.txt`.\n",
    "\n",
    "<img src=\"notebook/Top Authors.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<center>*getting a list of Medium \n",
    "authors*</center>\n",
    "\n",
    "The actual collection of the articles is done with the script `pull_Medium_articles.py`. The script performs two steps. First, it builds a list of all article URLs and for each article saves author URL and article URL in JSON format in the file `Medium_article_urls.json`. Below is an example how the entries for three articles look like.\n",
    "\n",
    "```javascript\n",
    "{\"author_URL\": \"https://medium.com/@tedr/latest\\n\",\n",
    " \"article_URL\": \"https://medium.com/season-of-the-witch/etiquette-and-the-cancer-patient-630a50047448?source=user_profile---------1----------------\"}\n",
    "{\"author_URL\": \"https://medium.com/@esterbloom/latest\\n\",\n",
    " \"article_URL\": \"https://medium.com/the-billfold/failing-at-shoplifting-life-with-kesha-bc2600b1f440?source=user_profile---------789----------------\"}\n",
    "{\"author_URL\": \"https://medium.com/@gassee/latest\",\n",
    " \"article_URL\": \"https://mondaynote.com/the-ny-times-un-free-at-last-df2eddba360b?source=user_profile---------281----------------\"}\n",
    "```\n",
    "\n",
    "The second part performs the actual download of the articles. The script reads the article URL saved in `Medium_article_urls.json`, navigates to the website and reads the text information from the html code of the article website. Each article is saved in text format in its own file. For each author a folder is generated that contains the articles for that author. Initially it was intended to store all articles in JSON format in one file. This turned out to be very cumbersome when troubleshooting the `pull_Medium_articles.py` script. Having a folder structure that allows to do quick visual inspections over the list of files in a file manager proved very helpful. In addition, the smaller article files made it easier to spot check the downloaded text information in a text editor.\n",
    "\n",
    "During research for this project several Python libraries for interacting with websites have been explored: [mechanize](https://pypi.python.org/pypi/mechanize/0.3.6), [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#), [scrapy](https://scrapy.org/) and the [Selenium WebDriver](https://www.seleniumhq.org/projects/webdriver/). Eventually the decision was made to use the Selenium WebDriver. The key reason for this was: the Medium website uses a two step login process. The users provides its email address and then receives a one time login link via this email. That made it difficult to automate the login via script and ruled out all the libraries that don't allow user interaction with the website.\n",
    "\n",
    "Once an article website is loaded, the required information can be pulled from the text attribute of specific html elements. The code snipped below shows the commands used to get the author name and the article text.\n",
    "\n",
    "```python\n",
    "author = self.browser.find_element_by_xpath('//a[@rel=\"author cc:attributionUrl\"]').text\n",
    "body = self.browser.find_element_by_xpath('//div[@class=\"postArticle-content js-postField js-notesSource js-trackedPost\"]').text\n",
    "```\n",
    "\n",
    "As shown in the code snippet above the right elements are addressed by their respective xpath. Finding these xpaths required a bit of trail and error. A valuable tool for this is the FireFox Inspector. It allows to inspect the code and structure of a website and to find the right path to the right html element.\n",
    "\n",
    "![Top Authors](notebook\\Firefox Inspector.jpg)\n",
    "<center>*finding the right xpath with Firefox Inspector*</center>\n",
    "\n",
    "After the `pull_Medium_articles.py` script completed, the folder `Medium_articles` containing all article files has been compressed into a ZIP archive to preserve storage. With [`zipfile`](https://docs.python.org/3/library/zipfile.html) Python provides a library to work with ZIP archives. Going forward in this Notebook we will make use of this library to work with the files directly within the ZIP archive without the need to extract the archive.\n",
    "\n",
    "Developing a web scraper script poses its own challenges. The initial idea is pretty straightforward: here is a list of URLs, go to each website, download the text part and save it in a file. As always, the pitfalls are discovered during implementation. Some time had to be invested to understand the structure of the Medium article websites and figure out the best way to find the right html elements that contain the required information. The Selenium WebDriver is not the most effective tool when it comes to scraping several thousand websites. The time to render each and every website adds up. An attempt has been made by parallelising the article download with multi-threading and spawning of several instances of the Firefox browser. This failed. It turned out that the fast sequence of websites caused Firefox to slowly consume all available memory and eventually Firefox stopped fetching new websites. In a parallelised version of the script the problem was only exaggerated. Finally, a pragamatic approach was taken and the script has been amended with the capability to continue the work where it has left off from a previous run. Over the course of several days the script has been restarted several times and eventually saved all articles.\n",
    "\n",
    "In defense for Selenium, it needs to be noted that Selenium first and foremost is a tool to automate testing of websites and not a tool for scaping several thousand websites. The primary goal behind the `pull_Medium_articles.py` script was to get the data for this capstone project and not to develop a sophisticated web scraper. In this respect Selenium did the job. Despite the challenges, developing the web scraper script has been a worthwhile learing experience. It provided an opportunity to develop practical experience not only in machine learning but also around data acquisition.\n",
    "\n",
    "In the next section we will explore the data set. We will also check what data preparation or cleaning activities might be required before we can apply algorithms  to the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exploring the Data Set\n",
    "\n",
    "All articles have been downloaded in individual text files and into folders for each author. This folder and file structure has been archived into `Medium_articles.zip`. Let's go through the files and get an overview about the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "\n",
    "def get_next_article_from_zip_file(file_name):\n",
    "    \"\"\"\n",
    "    A generator function that reads the files in a zip archive one by one.\n",
    "    \"\"\"\n",
    "    zf = zipfile.ZipFile(file_name, 'r')\n",
    "    names = zf.namelist()\n",
    "    for name in names:\n",
    "        if name.endswith('.json'):\n",
    "            data = zf.read(name)\n",
    "            yield data\n",
    "    zf.close()\n",
    "    return\n",
    "\n",
    "articles = []\n",
    "for article in get_next_article_from_zip_file('Medium_articles.zip'):\n",
    "    articles.append(json.loads(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles: 18,564\n",
      "Each article file contains the following attributes: ['url', 'author', 'headline', 'body']\n"
     ]
    }
   ],
   "source": [
    "print('Total number of articles: {:,}'.format(len(articles)))\n",
    "print('Each article file contains the following attributes: {}'\\\n",
    "      .format(list(articles[0].keys())))\n",
    "#\n",
    "# ToDo: list names of authors and the number of articles available from each author\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few example articles:\n",
      "----------------------\n",
      "Index: 13460\n",
      "URL: https://read.theheretic.org/a-gift-and-a-question-f725dfdd5035?source=user_profile---------521----------------\n",
      "Author: Pascal Finette\n",
      "Headline: A Gift And A Question\n",
      "Body: A Gift And A Question\n",
      "Today being Sunday — let’s do things differently today.\n",
      "First off: Fellow Heretic Lujie Chen wrote a book on mentorship (Seek to Keep) which is awesome (it goes into the why and how of mentorship and explains not only how to get a mentor but how to keep her/him as well).\n",
      "He kindly offered to make the PDF version available for free (!) to all Heretics.\n",
      "Here’s the deal — download the PDF here for the next 72 hours. And please, please consider buying the hardcopy on Amazon if you like it.\n",
      "Don’t upload the book on The Pirate Bay or any other public place. This is your personal copy; please be respectful. And say Hi to Lujie — it’s an incredible generous gift.\n",
      "Secondly: I’m looking for a talented co-author/editor who wants to work with me on The Heretic Coffee-Table Book. The person would become the co-author (similar to what Peter Thiel did with Blake Masters for Zero to One) and we split revenue (if there is such a thing in publishing anyway). Ideal candidates are for example literature students with writing experience. I’ve got a few interested publishers lined up — what needs to be done is: We need to go through the 500+ articles I published, fish for the good ones, get them into some form of order and do some edits.\n",
      "Know someone? Have them email me! :)\n",
      "----------------------\n",
      "Index: 18222\n",
      "URL: https://medium.com/thought-pills/the-focus-paradox-932a826750f0?source=user_profile---------476----------------\n",
      "Author: Yann Girard\n",
      "Headline: The focus paradox\n",
      "Body: The focus paradox\n",
      "The number one thing everybody tells you is that you should focus.\n",
      "No matter if it’s your teachers, your parents, your boss or whoever.\n",
      "You’ve gotta focus on one thing and one thing only.\n",
      "But what this advice usually leaves out is the fact that first of all you’ve gotta figure out what that one thing is.\n",
      "And reading books, attending classes or writing business plans won’t really help you to figure out what to focus on.\n",
      "They’re all as far away from reality as it gets.\n",
      "And the only way you’ll ever be able to figure out what to focus on is to do as many different things as possible.\n",
      "That’s the only way you’ll ever figure out what you’re passionate about, what you really enjoy doing, what the market really wants or what kind of marketing channels to focus on.\n",
      "If you start off with one thing and one thing only and focus on this one thing and nothing else, how will you ever know whether or not something else might work out a lot better? Whether you might enjoy doing something else a lot more?\n",
      "The basic rule is pretty simple but very counter-intuitive at the same time…\n",
      "The first thing you’ve gotta do to be able to figure out what to focus on is that you have to stop focusing….\n",
      "Originally published at yanngirard.typepad.com.\n",
      "----------------------\n",
      "Index: 16164\n",
      "URL: https://medium.com/invironment/fortune-amazon-to-create-five-new-solar-farms-in-virginia-af26a7759b40?source=user_profile---------153----------------\n",
      "Author: Tim Boucher\n",
      "Headline: Fortune: Amazon to create five new solar farms in Virginia\n",
      "Body: Fortune: Amazon to create five new solar farms in Virginia\n",
      "Amazon Brings on Five New Solar Projects to Power Its Cloud\n",
      "\n",
      "Amazon Web Services is bringing on five new solar farms to help power its massive cloud data centers. The new solar…\n",
      "fortune.com\n",
      "Amazon Web Services is bringing on five new solar farms to help power its massive cloud data centers. The new solar facilities, all in Virginia, will bring 180 megawatts of power onto the grid by the end of next year, the company announced late Thursday. Those new facilities, along with Amazon’s existing plants, are expected to provide 580,000 megawatt hours (MWh) of energy annually.\n",
      "Four of the farms are rated at 20 megawatts in capacity, while the fifth is a 100-megawatt facility. They are scattered across several Virginia counties. Why this state? For one thing, Amazon’s largest and oldest, data center farm is located outside of Washington D.C. in Virginia.\n",
      "See also:\n",
      "Fortune: Amazon building largest wind farm yet in Texas\n",
      "\n",
      "It’s interesting to watch these companies line up to make their commitments to renewable energy and see whose timelines…\n",
      "medium.com\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print('A few example articles:')\n",
    "for _ in range(1,4):\n",
    "    i = random.randrange(len(articles))\n",
    "    print('----------------------')\n",
    "    print('Index: {}'.format(i))\n",
    "    print('URL: {}'.format(articles[i]['url']))\n",
    "    print('Author: {}'.format(articles[i]['author']))\n",
    "    print('Headline: {}'.format(articles[i]['headline']))\n",
    "    print('Body: {}'.format(articles[i]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- add some discussion of spot checks: lentgh of articles, html formatting removed, lentgh nothing lost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- remove author name from body"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
