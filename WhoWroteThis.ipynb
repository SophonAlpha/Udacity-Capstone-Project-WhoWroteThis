{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project - Who Wrote This? \n",
    "... or how to identify the author of anonymous articles using natural languange processing.\n",
    "\n",
    "Stefan Dittforth  \n",
    "February 27th, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of contents\n",
    "\n",
    "1. [Project Overview](#Project Overview)\n",
    "2. [Data Acquisition](#Data Acquisition)\n",
    "3. [Exploring the Data Set](#Exploring the Data Set)\n",
    "4. [Cleaning the Dataset](#Cleaning the Dataset)\n",
    "5. [Balancing the Data Set](#Balancing the Data Set)\n",
    "6. [Models](#Models)  \n",
    "    6.1. [Model 1: The Baseline Model - Just Guess the Author](#Model 1 The Baseline Model)  \n",
    "    6.2. [Model 2: Basic Article Metrics](#Model 2 Basic Article Metrics)  \n",
    "    6.3. [Model 3: Bag-of-Words - Word Count](#Model 3 Bag-of-Words - Word Count)  \n",
    "    6.4. [Model 4: Bag-of-Words - TFIDF](#Model 4 Bag-of-Words - TFIDF)  \n",
    "    6.5. [Model 5: Bag-of-Words - Reduced Vocabulary](#Model 5 Bag-of-Words - Reduced Vocabulary)  \n",
    "    6.6. [Model 6: Bag-of-Words - Bigrams](#Model 6 Bag-of-Words - Bigrams)  \n",
    "    6.7. [Model 7: Bag-of-Words - Reduced Bigrams](#Model 7 Bag-of-Words - Reduced Bigrams)  \n",
    "    6.8. [Model 8: Learn Word Embeddings & CNN](#Model 8 Learn Word Embeddings & CNN)  \n",
    "    6.9. [Model 9: GloVe Word Embeddings & CNN](#Model 9 GloVe Word Embeddings & CNN)  \n",
    "7. [Conclusion](#Conclusion)\n",
    "8. [Let's have some fun: the WhoWroteThis app](#WhoWroteThis app)\n",
    "9. [Sources](#Sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------\n",
    "## Project Overview <a name=\"Project Overview\"></a>\n",
    "\n",
    "When writing, authors leave distinctive marks in their stories influenced by the style of their writing. Well know writers are famous for their techniques with which they express ideas and manipulate language. Erika Rasso published a well written introduction to [Famous Authors and Their Writing Styles](https://www.craftyourcontent.com/famous-authors-writing-styles/). We learn, for example, that Ernest Hemingway \"pioneered concise, objective prose in fiction—which had, up until then, primarily been used in journalism.\". Another author discussed in Rasso's article is Franz Kafka. His stories present \"surrealist, nightmarish writing in contemporary settings\" which invoke feelings of confusion and helplessness. Agatha Christie's style was influenced by \"mentions of war\". Furthermore, \"she utilized a variety of poisons to carry out the murders in her stories\". And being interested in archarology \"resulted in ancient artifacts and archaeologists being heavily featured in her novels\". A last author worthwhile to highlight here is Zora Neale Hurston. Her style is quite unique: \"She wrote in colloquial Southern dialects that mimicked the language she grew up hearing.\".\n",
    "\n",
    "Our intuition and experience as readers tells us that the style of writing is like a \"finger print\" that differentiates authors. If we come across a text with no information about the author or a text written under a preudonym, would we be able to tell the author based of the style of writing? An interesting article about [\"Who’s At The Keyboard? Authorship Attribution in Digital Evidence Investigations\"](www.utica.edu/academic/institutes/ecii/publications/articles/B49F9C4A-0362-765C-6A235CB8ABDFACFF.pdf) has been published by Carole E. Chaski. The article discusses the question to what extend text can be attributed to an author as part of crime investigations.\n",
    "\n",
    "In this project we will explore to what extend machine learning techniques can learn the style of writing for a set of authors. A learned system would then be given a text not seen before and tries to predict who the author is.\n",
    "\n",
    "This Jupyter Notebook has been structured in the sequential order in which the problem has been approached. The next few sections describe the data set that has been aquired and the data cleaning activities applied. This is then followed by the main section that discusses various machine learning models. The models utilise various natural language processing (NLP) and classification algorithms. The models represent different approaches to capture the \"uniqueness\" of writing of the different authors. The key metric against which we measure each model is the accuracy in correctly predicting the authors for a set of articles that have not been seen before.\n",
    "\n",
    "Lastly, we will have some fun and use the best performing model to build a small WhoWroteThis application. The user can copy and paste any text into text input field and ask the system to guess who the author is.\n",
    "\n",
    "### Runtime for this Notbook\n",
    "\n",
    "total run time for this Notebook: 7.3 hours\n",
    "\n",
    "----------------\n",
    "## Data Acquisition <a name=\"Data Acquisition\"></a>\n",
    "\n",
    "In order to allow the system to learn the writing characteristics of different authors we require a dataset that provides a large number of articles for individual authors. There are rich datasets for NLP research available in the public domain. A list, as an example, can be found [here](https://github.com/niderhoff/nlp-datasets). However, as part of this project we will build our own dataset. We will develop a web scraper that will collect articles from the publishing platform Medium. The articles on Medium seem to be reasonably long (at least several hundred words). There are enough authors that have published several hundreds articles. With this, it appears feasible to acquire a large enough data set to learn patterns in the writing characteristics to distinguish between individual authors.\n",
    "\n",
    "This approach has been chosen as an opportunity to develop practical experience not only in machine learning but also around data acquisition. In data science and machine learning the acquisition and preparation of high quality data is often the bigger challenge than the actual development of the machine learning system itself. In \"[Datasets Over Algorithms](https://www.edge.org/response-detail/26587)\" author Alexander Wissner-Gross notes that \n",
    "\n",
    ">*\"the average elapsed time between key [machine learning] algorithm proposals and corresponding advances was about eighteen years, whereas the average elapsed time between key dataset availabilities and corresponding advances was less than three years, or about six times faster, suggesting that datasets might have been limiting factors in the advances.\"*.\n",
    "\n",
    "Conveniently the website [Top Authors](https://topauthors.xyz/) has published a list of 300+ top Medium authors. The project folder contains the short script `get_list_of_Medium_authors.py` that has been used to extract the Medium URL for each author. The initial list of 300+ authors has been reduced to 25. The criteria for this reduction was the number of published articles. For the 25 authors there are at least 300 articles available. The Medium URLs for these authors can be found in file `Medium_authors_25.txt`.\n",
    "\n",
    "<img src=\"notebook/Top Authors.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<center>*getting a list of Medium authors*</center>\n",
    "\n",
    "The actual collection of the articles is done with the script `pull_Medium_articles.py`. The script performs two steps. First, it builds a list of all article URLs and for each article saves author URL and article URL in JSON format in the file `Medium_article_urls.json`. Below is an example how the entries for three articles look like.\n",
    "\n",
    "```javascript\n",
    "{\"author_URL\": \"https://medium.com/@tedr/latest\\n\",\n",
    " \"article_URL\": \"https://medium.com/season-of-the-witch/etiquette-and-the-cancer-patient-630a50047448?source=user_profile---------1----------------\"}\n",
    "{\"author_URL\": \"https://medium.com/@esterbloom/latest\\n\",\n",
    " \"article_URL\": \"https://medium.com/the-billfold/failing-at-shoplifting-life-with-kesha-bc2600b1f440?source=user_profile---------789----------------\"}\n",
    "{\"author_URL\": \"https://medium.com/@gassee/latest\",\n",
    " \"article_URL\": \"https://mondaynote.com/the-ny-times-un-free-at-last-df2eddba360b?source=user_profile---------281----------------\"}\n",
    "```\n",
    "\n",
    "The second part performs the actual download of the articles. The script reads the article URL saved in `Medium_article_urls.json`, navigates to the website and reads the text information from the html code. Each article is saved in text format in its own file. For each author a folder is generated that contains the articles for that author. Initially it was intended to store all articles in JSON format in one file. This turned out to be very cumbersome when troubleshooting the `pull_Medium_articles.py` script. Having a folder structure that allows to do quick visual inspections over the list of files in a file manager proved very helpful. In addition, the smaller article files made it easier to spot check the downloaded text information in a text editor.\n",
    "\n",
    "<table><tr><td><img src=\"notebook/article 1.PNG\" style=\"width: 300px\"></td>\n",
    "           <td><img src=\"notebook/article 2.PNG\" style=\"width: 300px\"></td>\n",
    "           <td><img src=\"notebook/article 3.PNG\" style=\"width: 300px\"></td></tr></table>\n",
    "<center>*Working with all these articles kept me distracted. I started reading them instead of getting on the with project.*</center>\n",
    "\n",
    "During research for this project several Python libraries for interacting with websites have been explored: [mechanize](https://pypi.python.org/pypi/mechanize/0.3.6), [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#), [scrapy](https://scrapy.org/) and the [Selenium WebDriver](https://www.seleniumhq.org/projects/webdriver/). Eventually the decision was made to use the Selenium WebDriver. The key reason for this was: the Medium website uses a two step login process. The users provides its email address and then receives a one time login link via this email. That made it difficult to automate the login via script and ruled out all the libraries that don't allow user interaction with the website.\n",
    "\n",
    "Once an article website is loaded, the required information can be pulled from the text attribute of specific html elements. The code snipped below shows the commands used to get the author name and the article text.\n",
    "\n",
    "```python\n",
    "author = self.browser.find_element_by_xpath('//a[@rel=\"author cc:attributionUrl\"]').text\n",
    "body = self.browser.find_element_by_xpath('//div[@class=\"postArticle-content js-postField js-notesSource js-trackedPost\"]').text\n",
    "```\n",
    "\n",
    "As shown in the code snippet above the right elements are addressed by their respective xpath. Finding these xpaths required a bit of trail and error. A valuable tool for this is the FireFox Inspector. It allows to inspect the code and structure of a website and to find the right path to the right html element.\n",
    "\n",
    "![Top Authors](notebook\\Firefox Inspector.jpg)\n",
    "<center>*finding the right xpath with Firefox Inspector*</center>\n",
    "\n",
    "After the `pull_Medium_articles.py` script completed, the folder `Medium_articles` containing all article files has been compressed into a ZIP archive to preserve storage. With [`zipfile`](https://docs.python.org/3/library/zipfile.html) Python provides a library to work with ZIP archives. Going forward in this Notebook we will make use of this library to work with the files directly within the ZIP archive without the need to extract the archive.\n",
    "\n",
    "Developing a web scraper script poses its own challenges. The initial idea is pretty straightforward: here is a list of URLs, go to each website, download the text part and save it in a file. As always, the pitfalls are discovered during implementation. Some time had to be invested to understand the structure of the Medium article websites and figure out the best way to find the right html elements that contain the required information. The Selenium WebDriver is not the most effective tool when it comes to scraping several thousand websites. The time to render each and every website adds up. An attempt has been made by parallelising the article download with multi-threading and spawning of several instances of the Firefox browser. This failed. It turned out that the fast sequence of websites caused Firefox to slowly consume all available memory and eventually Firefox stopped fetching new websites. In a parallelised version of the script the problem was only exaggerated. Finally, a pragamatic approach was taken and the script has been amended with the capability to continue the work where it has left off from a previous run. Over the course of several days the script has been restarted several times and eventually saved all articles.\n",
    "\n",
    "In defense for Selenium, it needs to be noted that Selenium first and foremost is a tool to automate testing of websites and not a tool for scaping several thousand websites. The primary goal behind the `pull_Medium_articles.py` script was to get the data for this capstone project and not to develop a sophisticated web scraper. In this respect Selenium did the job. Despite the challenges, developing the web scraper script has been a worthwhile learing experience. It provided an opportunity to develop practical experience around data acquisition.\n",
    "\n",
    "In the next section we will explore the data set. We will also check what data preparation or cleaning activities might be required before we can apply lkearning algorithms to the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------\n",
    "## Exploring the Data Set <a name=\"Exploring the Data Set\"></a>\n",
    "\n",
    "All articles have been downloaded in individual text files and into folders for each author. This folder and file structure has been archived into `Medium_articles.zip`. Let's go through the files and get an overview about the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this code block to load all the libraries we will need\n",
    "# throughout the Notebook. Keeping all library calls in one place at\n",
    "# beginning allows to run other code cells more independently.\n",
    "\n",
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import pprint\n",
    "import pickle\n",
    "import hashlib\n",
    "import time\n",
    "import itertools\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from cycler import cycler\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# some global settings for the Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams[\"figure.figsize\"] = [13, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_article_from_zip_file(file_name):\n",
    "    \"\"\"\n",
    "    A generator function that reads the files in a zip archive one by one.\n",
    "    \"\"\"\n",
    "    zf = zipfile.ZipFile(file_name, 'r')\n",
    "    names = zf.namelist()\n",
    "    for name in names:\n",
    "        if name.endswith('.json'):\n",
    "            data = zf.read(name)\n",
    "            yield data\n",
    "    zf.close()\n",
    "    return\n",
    "\n",
    "# read all articles into a pandas data frame\n",
    "articles = pd.DataFrame(columns=['url', 'author', 'headline', 'body'])\n",
    "for article_file in get_next_article_from_zip_file('Medium_articles.zip'):\n",
    "    article = json.loads(article_file)\n",
    "    articles = articles.append(json.loads(article_file), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a list of authors and the number of articles for each author\n",
    "summary = articles[['author', 'body']].groupby('author').count()\n",
    "summary.columns = ['number of articles']\n",
    "display(summary.sort_values('number of articles', ascending=False))\n",
    "print('Total number of articles: {:,}'.format(len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleaning the Dataset <a name=\"Cleaning the Dataset\"></a>\n",
    "\n",
    "The articles were downloaded by extracting the 'text' attribute from html elements that contain the body text of the article. Many articles contain images, URLs to other pages, etc. We are interested to see if the text extracts still contain html fragments we might need to clean out. We do a qick check by listing the articles that contain the '<' and '>' characters. Those two characters enclose HTML tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'<.*>' # find any HTML tag enclosing brackets\n",
    "counter = 0\n",
    "for index, article in articles.iterrows():\n",
    "    matches = re.finditer(regex, article['body'], re.DOTALL)\n",
    "    for matchNum, match in enumerate(matches):\n",
    "        counter = counter + 1\n",
    "        print('Match number: {}'.format(counter))\n",
    "        print('Match in article index: {}'.format(index))\n",
    "        print(article['url'])\n",
    "        # show the first and last 20 characters of the text found\n",
    "        print(match.group()[:20] + ' ... ' + match.group()[len(match.group())-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 156 occurences for the '<' and '>' characters in over 18,500 articles. Doing some spot checks it appears that the tag brackets are genuine parts of the article text (for example the text is about HTML coding). We will leave them in for now. No need to clean them out.\n",
    "\n",
    "Let's have a look at some article texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A few example articles:')\n",
    "for _ in range(1,4):\n",
    "    i = random.randrange(len(articles))\n",
    "    print('----------------------')\n",
    "    print('Index: {}'.format(i))\n",
    "    print('URL: {}'.format(articles.iloc[i]['url']))\n",
    "    print('Author: {}'.format(articles.iloc[i]['author']))\n",
    "    print('Headline: {}'.format(articles.iloc[i]['headline']))\n",
    "    print('Body: {}'.format(articles.iloc[i]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above randomly selects 3 articles. We run it a few times. At each run we spot check the text in the 'body' section. It appears that the text is clean and good to go for our next step.\n",
    "\n",
    "Typically NLP text cleaning tasks include removing punctuation characters. For now we will keep them. In one of our first classification approaches we will use them to engineer features around number and length of sentences and paragraphs in articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Balancing the Data Set <a name=\"Balancing the Data Set\"></a>\n",
    "\n",
    "As seen above in the table with the number of articles per authors the data set is skewed. The number of articles ranges from 1,948 for Nicole Dieker to 307 for Gary Vaynerchuk. To avoid that our system develops a bias towards authors with a high number of articles we will balance the data set. This will be done by keeping the number of articles for each author equal to the author with the lowest number of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the smallest number for articles for one author \n",
    "num_articles_to_keep = summary.min(axis=0)['number of articles']\n",
    "\n",
    "# only keep the smallest number of articles for each author\n",
    "indices_to_keep = []\n",
    "for author, num_of_articles in summary.iterrows():\n",
    "    indices_to_keep = indices_to_keep + \\\n",
    "                      list(articles[articles.author == author]\\\n",
    "                      [:num_articles_to_keep].index.values)\n",
    "articles = articles.iloc[indices_to_keep]\n",
    "\n",
    "# check number of articles for each author\n",
    "summary = articles[['author', 'body']].groupby('author').count()\n",
    "summary.columns = ['number of articles']\n",
    "display(summary.sort_values('number of articles', ascending=False))\n",
    "print('Total number of articles: {:,}'.format(len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Models <a name=\"Models\"></a>\n",
    "\n",
    "### Model 1: Baseline Model - Just Guess the Author <a name=\"Model 1 The Baseline Model\"></a>\n",
    "\n",
    "In the course of this notebook we will try several approaches to predict the author of a given text. To assess the quality of the prediction we will need to compare against some baseline. In our case we will simply do a random guess of who the author of an article is. This should get us in the order of $\\frac{1}{n}\\cdot100$ percent accuracy, where $n$ represents the number of authors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a global variable that will consolidate results from all models being tested\n",
    "results = pd.DataFrame(columns=['group', 'classifier', 'result', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "labels = articles[['author']]\n",
    "authors = labels.author.unique()\n",
    "y_predict = pd.DataFrame()\n",
    "y_predict['author'] = np.random.choice((authors), len(labels))\n",
    "score_test = accuracy_score(labels, y_predict)\n",
    "\n",
    "# report random guess results\n",
    "print('Just guess the author:')\n",
    "print('----------------------\\n')\n",
    "print('number of authors in data set: {}'.format(len(authors)))\n",
    "print('expected prediction accuracy around: {:.2f}%'.format(1/len(authors)*100))\n",
    "print('prediction accuracy score on data set: {:.2f}%'.format(score_test*100))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 1: Baseline Model',\n",
    "                          'classifier': 'random selection',\n",
    "                          'result': score_test,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Basic Article Metrics <a name=\"Model 2 Basic Article Metrics\"></a>\n",
    "\n",
    "Let's first come up with some simple metrics as features that might help us to predict the author of an article.  Some metrics that come to mind are: the total, mean, median, min and max number of words in paragraphs, sentences and the article itself. When looking at different authors it appears that these features might be useful differentiators. Some authors have a tendency to longer articles. Others use longer sentences or shorter paragraphs.\n",
    "\n",
    "The code below \"tokenizes\" the articles into paragraphs, sentences and words. In addition in counts the number of each. The tokenized article is returned as a nested dict object. The next code cell displays an example for a tokenized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "def tokenize_paragraphs(text):\n",
    "    paragraphs = [p for p in text.split('\\n')]\n",
    "    paragraphs_tokenized = []\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = tokenize_sentences(paragraph)\n",
    "        paragraphs_tokenized.append(sentences)\n",
    "    num_paragraphs = len(paragraphs_tokenized)\n",
    "    text_tokenized = {'num_paragraphs': num_paragraphs,\n",
    "                      'paragraphs': paragraphs_tokenized}\n",
    "    return text_tokenized\n",
    "\n",
    "def tokenize_sentences(paragraph):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    sentences_tokenized = []\n",
    "    for sentence in sentences:\n",
    "        words = tokenize_words(sentence)\n",
    "        sentences_tokenized.append(words)\n",
    "    num_sentences = len(sentences_tokenized)\n",
    "    paragraph_tokenized = {'num_sentences': num_sentences,\n",
    "                           'sentences': sentences_tokenized}\n",
    "    return paragraph_tokenized\n",
    "\n",
    "def tokenize_words(sentence):\n",
    "    words_tokenized = word_tokenize(sentence)\n",
    "    # remove punctuations from word list\n",
    "    words_tokenized = remove_punctuation(words_tokenized)\n",
    "    num_words = len(words_tokenized)\n",
    "    sentence_tokenized = {'num_words': num_words,\n",
    "                          'words': words_tokenized}\n",
    "    return sentence_tokenized\n",
    "\n",
    "def remove_punctuation(words_tokenized):\n",
    "    return [w for w in words_tokenized \n",
    "            if not re.fullmatch('[' + string.punctuation + '’“‘”–…' ']', w)]\n",
    "\n",
    "def get_article_metrics(body):\n",
    "    body_tokenized = tokenize_paragraphs(body)\n",
    "    paragraphs_sentences = []\n",
    "    sentences_words = []\n",
    "    for paragraph in body_tokenized['paragraphs']:\n",
    "        paragraphs_sentences.append(paragraph['num_sentences'])\n",
    "        for sentence in paragraph['sentences']:\n",
    "            sentences_words.append(sentence['num_words'])\n",
    "    paragraphs_sentences = pd.Series(paragraphs_sentences)\n",
    "    sentences_words = pd.Series(sentences_words)\n",
    "    metrics = {'article_num_paragraphs': body_tokenized['num_paragraphs'],\n",
    "               'article_num_sentences': paragraphs_sentences.sum(),\n",
    "               'article_num_words': sentences_words.sum(),\n",
    "               'paragraphs_min_sentences': paragraphs_sentences.min(),\n",
    "               'paragraphs_max_sentences': paragraphs_sentences.max(),\n",
    "               'paragraphs_mean_sentences': paragraphs_sentences.mean(),\n",
    "               'paragraphs_median_sentences': paragraphs_sentences.median(),\n",
    "               'sentences_min_words': sentences_words.min(),\n",
    "               'sentences_max_words': sentences_words.max(),\n",
    "               'sentences_mean_words': sentences_words.mean(),\n",
    "               'sentences_median_words': sentences_words.median()}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the basic metrics for each article\n",
    "articles_metrics = pd.DataFrame(list(articles['body'].map(get_article_metrics)),\n",
    "                                index=articles.index)\n",
    "articles_metrics.insert(loc=0, column='author', value=articles['author'])\n",
    "display(articles_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how a tokenized article looks like ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display an example for a tokenized article\n",
    "sample_article_index = 1 # just a random pick\n",
    "body_tokenized = tokenize_paragraphs(articles.loc[sample_article_index]['body'])\n",
    "print('\\nSample article tokenized:\\n')\n",
    "pp.pprint(body_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the basic metrics as features in a pandas data frame we will plot a few scatter diagrams to get a feeling for how strong they separate different authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_scatters(articles_list, authors, plot_features, plot_labels):\n",
    "    num_features = plot_features.__len__()\n",
    "    \n",
    "    # set up the figure \n",
    "    fig, axes = plt.subplots(nrows=num_features, ncols=num_features)\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    fig.suptitle(\"Feature Relationships\", fontsize=18)\n",
    "    \n",
    "    # set up the axes\n",
    "    for ax in axes.flat:\n",
    "        ax.tick_params(axis='both',\n",
    "                       bottom='off', top='off', left='off', right='off',\n",
    "                       labelbottom='off', labeltop='off',\n",
    "                       labelleft='off', labelright='off')\n",
    "        if ax.is_first_col():\n",
    "            ax.yaxis.set_ticks_position('left')\n",
    "            ax.tick_params(axis='y', left='on', labelleft='on')\n",
    "        if ax.is_last_col():\n",
    "            ax.yaxis.set_ticks_position('right')\n",
    "            ax.tick_params(axis='y', right='on', labelright='on')\n",
    "        if ax.is_first_row():\n",
    "            ax.xaxis.set_ticks_position('top')\n",
    "            ax.tick_params(axis='x', top='on', labeltop='on')\n",
    "        if ax.is_last_row():\n",
    "            ax.xaxis.set_ticks_position('bottom')\n",
    "            ax.tick_params(axis='x', bottom='on', labelbottom='on')\n",
    "        # set colour cycler, each author a different colour \n",
    "        ax.set_prop_cycle(cycler('color', ['blue', 'green', 'red', 'cyan',\n",
    "                                           'magenta', 'yellow', 'black', 'white']))\n",
    "        \n",
    "    # plot the scatter plots\n",
    "    for c in range(num_features):\n",
    "        for r in range(num_features):\n",
    "            if c == r:\n",
    "                # leave diagonal plots empty and remove ticks and labels\n",
    "                axes[r, c].xaxis.set_visible(False)\n",
    "                axes[r, c].yaxis.set_visible(False)\n",
    "            elif r > c:\n",
    "                axes[r, c].axis('off')\n",
    "            else:\n",
    "                axes[r, c].set_prop_cycle(None) # reset colour cycler\n",
    "                for author in authors:\n",
    "                    x = articles_list[articles_list.author == author][plot_features[c]]\n",
    "                    y = articles_list[articles_list.author == author][plot_features[r]]\n",
    "                    axes[r, c].scatter(x, y, s=2, alpha=0.5, label=author)\n",
    "\n",
    "    # place a legend below the top left plot\n",
    "    axes[0, 1].legend(bbox_to_anchor=(-1.1, 1), loc=3, prop={'size': 12}, markerscale=6)\n",
    "\n",
    "    # add axis labels in diagonal boxes\n",
    "    for i, label in enumerate(plot_labels):\n",
    "        axes[i,i].annotate(label, (0.5, 0.5), \n",
    "                           xycoords='axes fraction', ha='center', va='center',\n",
    "                           size=12)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1 = plot_feature_scatters(articles_metrics,\n",
    "                ['Brad Feld', 'Jean-Louis Gassée',\n",
    "                 'James Altucher', 'Gary Vaynerchuk', 'Ethan Siegel'],\n",
    "                ['article_num_paragraphs', 'article_num_sentences', 'article_num_words',\n",
    "                 'paragraphs_max_sentences', 'paragraphs_median_sentences',\n",
    "                 'sentences_max_words', 'sentences_median_words'],\n",
    "                ['paragraphs /\\narticle', 'sentences /\\narticle', 'words /\\narticle',\n",
    "                 'max sentences /\\nparagraph', 'median\\nsentences /\\nparagraph',\n",
    "                 'max words /\\nsentence', 'median words /\\nsentence'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plots above it appears that a separation to some degree would be possible. For example, the plot for \"words / article\" vs \"max words / sentence\" shows distinct clusters for individual authors. Although the clusters seem to overlap quite a bit.\n",
    "\n",
    "We will run a few classifier models and see what prediction accuracy we can achieve on a training data set. First, we will try the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split labels into separate data frame\n",
    "data = articles_metrics\n",
    "labels = data[['author']]\n",
    "data = data.drop('author', 1)\n",
    "\n",
    "# shuffle training test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=23)\n",
    "start_time = time.time()\n",
    "\n",
    "# decision tree prediction model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "score_train = accuracy_score(y_train, clf.predict(X_train))\n",
    "score_test = accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "# report decision tree results\n",
    "print('decision tree classifier:')\n",
    "print('-------------------------\\n')\n",
    "print('prediction accuracy score on training data set: {:.2f}%'.format(score_train*100))\n",
    "print('prediction accuracy score on test data set: {:.2f}%'.format(score_test*100))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 2: Basic Article Metrics',\n",
    "                          'classifier': 'decision tree',\n",
    "                          'result': score_test,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly 100% on the training data! That's awesome!\n",
    "\n",
    "Ok.... just joking ;-)\n",
    "\n",
    "On unseen data the accuracy is around 33% (the value will vary slightly with each run). I think that's not too bad for a model that tries to guess the author of an article only based on the number of words in articles, paragraphs and sentences. The model does not take anything of the article content into account. The 100% accuracy on the training data means the model is extremly biased. It would be interesting to see if a grid search can find a better set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(X_train, y_train):\n",
    "    # setup grid search\n",
    "    cv_sets = ShuffleSplit(n_splits=10, test_size = 0.25, random_state = 23)\n",
    "    learning_model = tree.DecisionTreeClassifier()\n",
    "    params = {'max_depth': list(range(1,30)),\n",
    "              'min_samples_split': list(range(2,50))}\n",
    "    scoring_fnc = make_scorer(accuracy_score, greater_is_better=True)\n",
    "    grid = GridSearchCV(estimator=learning_model,\n",
    "                        param_grid=params,\n",
    "                        scoring=scoring_fnc,\n",
    "                        cv=cv_sets,\n",
    "                        verbose=2)\n",
    "\n",
    "    # run grid search\n",
    "    grid = grid.fit(X_train, y_train)\n",
    "    best_clf = grid.best_estimator_\n",
    "    return best_clf\n",
    "\n",
    "def get_classifier():\n",
    "    '''\n",
    "    Get the best classifier. Run grid search and save the best classifier to\n",
    "    a pickle file. If the pickle file exists load the best classifier from file\n",
    "    instead of running grid search again. This saves time in subsequent runs \n",
    "    of the Notebook.\n",
    "    '''\n",
    "    pickle_file = 'Model 2 Basic Article Metrics - grid search best estimator.pickle'\n",
    "    if os.path.isfile(pickle_file):\n",
    "        with open(pickle_file,'rb') as f:\n",
    "            best_clf = pickle.loads(f.read())\n",
    "    else:\n",
    "        best_clf = grid_search(X_train, y_train)\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            s = pickle.dumps(best_clf)\n",
    "            f.write(s)\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "best_clf = get_classifier()\n",
    "score_test = accuracy_score(y_test, best_clf.predict(X_test))\n",
    "print('GridSearch prediction accuracy score on test data set: {:.2f}%'.format(score_test*100))\n",
    "print('best parameters:')\n",
    "print('    max_depth:{0}'.format(best_clf.get_params()['max_depth']))\n",
    "print('    min_samples_split:{0}'.format(best_clf.get_params()['min_samples_split']))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 2: Basic Article Metrics',\n",
    "                          'classifier': 'grid search decision tree',\n",
    "                          'result': score_test,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation of two parameters with grid search gains us three more percentage points to 36%. A little bit better.\n",
    "\n",
    "For comparison we will run the logistic regression algorithm and see if that yields better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# logistic regression prediction model\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train.values.ravel())\n",
    "score_train = accuracy_score(y_train, clf.predict(X_train))\n",
    "score_test = accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "# report logistic regression results\n",
    "print('\\nlogistic regression classifier:')\n",
    "print('-------------------------------\\n')\n",
    "print('prediction accuracy score on training data set: {:.2f}%'.format(score_train*100))\n",
    "print('prediction accuracy score on test data set: {:.2f}%'.format(score_test*100))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 2: Basic Article Metrics',\n",
    "                          'classifier': 'logistic regression',\n",
    "                          'result': score_test,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 32% accuracy the result is 4% below what has been achieved with an (grid search) optimised decision tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bag-of-Words - Word Count <a name=\"Model 3 Bag-of-Words - Word Count\"></a>\n",
    "\n",
    "In our next model we will use the vocabulary to build our prediction model. We will implement a the bag-of-words algorithm. Bag-of-Words is a very simple but often suprisingly effective algorithm. It takes all the different words in a training set and uses them as features. Each article is then transformed into a feature vector by marking the occurrance of each feature (=word). There are a few variations of Bag-of-Words. In it's basic form each vector counts the number of occurences for each word. This is what we are going to use in this Model 3. In the next section (Model 4) will use the term frequency–inverse document frequency (TFIDF) approach to build the Bag-of-Words vectors.\n",
    "\n",
    "The code cell below defines a number of functions. The approach to define functions vs. directly executed code cells has been choosen to make the code reuseable for the next model in the following section.\n",
    "\n",
    "Besides the training, predicting and scoring functionalties the code implementes two additional capabilities: 1.) it allows trained classifiers to be saved to or loaded from disk. This saves time when the Notebook is run multiple times. The model needs to be trained only once. 2.) The model can be trained with different classifiers. During experiementation very often different classifiers (e.g. logistic regression, decision tree, SVM) are tested to evaluate which one is most suitable for a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(articles_train, pickle_file, classifiers, vectorizer_type):\n",
    "    \"\"\" This function is called to either load a trained model from disk\n",
    "    or to start the training. A trained model is loaded automatically \n",
    "    when the file exists.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(pickle_file):\n",
    "        vectorizer, trained_classifiers = load_trained_classifiers(pickle_file)\n",
    "    else:\n",
    "        vectorizer, trained_classifiers = train_classifiers(articles_train,\n",
    "                                                            classifiers,\n",
    "                                                            vectorizer_type)\n",
    "        save_trained_classifiers(vectorizer, trained_classifiers, pickle_file)\n",
    "    return vectorizer, trained_classifiers\n",
    "\n",
    "def load_trained_classifiers(pickle_file):\n",
    "    \"\"\" Load a trained model from disk.\n",
    "    \"\"\"\n",
    "    with open(pickle_file,'rb') as f:\n",
    "        saved_items = pickle.loads(f.read())\n",
    "    trained_classifiers = saved_items['trained_classifiers']\n",
    "    vectorizer = saved_items['vectorizer']\n",
    "    return vectorizer, trained_classifiers\n",
    "\n",
    "def save_trained_classifiers(vectorizer, trained_classifiers, pickle_file):\n",
    "    \"\"\" Save a trained model to disk. For Bag-of-Words there are two elements\n",
    "    that need to be saved: the classifiers that have been trained (e.g. logistic\n",
    "    regression, SVM) and the vectorizer. The vectorizer is the object that contains\n",
    "    the vocabulary that has been trained from the training data set. The vecorizer\n",
    "    is required later on to transform any given text into a vector representation.\n",
    "    \"\"\"\n",
    "    saved_items = {'trained_classifiers': trained_classifiers,\n",
    "                   'vectorizer': vectorizer}\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        s = pickle.dumps(saved_items)\n",
    "        f.write(s)\n",
    "\n",
    "def train_classifiers(articles_train, classifiers, vectorizer_type):\n",
    "    \"\"\" This function performs the training for each classifier. The desired \n",
    "    classifieres are given as a list of dictionaries. Each entry spacifies an\n",
    "    'algorithm' and the 'display_text'.\n",
    "    \"\"\"\n",
    "    vectorizer, X_train = fit_transform_vectors(articles_train, vectorizer_type)\n",
    "    y_train = articles_train[['author']]\n",
    "    trained_classifiers = []\n",
    "    for classifier in classifiers:\n",
    "        classifier_algorithm = classifier['algorithm']\n",
    "        display_text = classifier['display_text']\n",
    "        clf = train_classifier(X_train, y_train, classifier_algorithm)\n",
    "        trained_classifiers.append({'clf': clf, 'display_text': classifier['display_text']})\n",
    "    return vectorizer, trained_classifiers\n",
    "\n",
    "def train_classifier(data, labels, classifier_algorithm):\n",
    "    \"\"\" Train a single classifier.\n",
    "    \"\"\"\n",
    "    X_train = data\n",
    "    y_train = labels\n",
    "    clf = classifier_algorithm\n",
    "    clf = clf.fit(X_train, y_train.values.ravel())\n",
    "    return clf\n",
    "\n",
    "def fit_transform_vectors(articles, vectorizer):\n",
    "    \"\"\" Build the vocabulary during training.\n",
    "    \"\"\"\n",
    "    data = pre_process_articles(articles)\n",
    "    data_vectorized = vectorizer.fit_transform(data)\n",
    "    return vectorizer, data_vectorized\n",
    "\n",
    "def transform_vectors(articles, vectorizer):\n",
    "    \"\"\" Transform a list of articles into Bag-of-Words vectors. This is used\n",
    "    once a mopdel has been trained to transform articles in preparation for\n",
    "    prediction.\n",
    "    \"\"\"\n",
    "    articles = pre_process_articles(articles)\n",
    "    articles = vectorizer.transform(articles)\n",
    "    return articles\n",
    "\n",
    "def pre_process_articles(articles, as_string=True):\n",
    "    # join all articles to one string in preparation for hash calculation\n",
    "    data = ''.join([article['body'] for index, article in articles.iterrows()])\n",
    "    # calculate MD5 hash across all articles\n",
    "    m = hashlib.md5()\n",
    "    m.update(data.encode('utf-8'))\n",
    "    md5_digest = m.hexdigest()\n",
    "    pickle_file_name = 'pre_processed_articles_' + md5_digest + '.pickle'\n",
    "    # load from disk or pre-process articles and save to disk\n",
    "    if os.path.isfile(pickle_file_name):\n",
    "        with open(pickle_file_name,'rb') as f:\n",
    "            data = pickle.loads(f.read())\n",
    "    else:\n",
    "        data = [pre_process_text(article['body'], as_string) for index, article in articles.iterrows()]\n",
    "        with open(pickle_file_name, 'wb') as f:\n",
    "            s = pickle.dumps(data)\n",
    "            f.write(s)\n",
    "    return data\n",
    "\n",
    "# def pre_process_text(article): # buggy function\n",
    "#     \"\"\" Some preprocessing and cleaning tasks to be applied to the raw input text\n",
    "#     of an article.\n",
    "#     \"\"\"\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     porter = PorterStemmer()\n",
    "#     # tokenize article into words\n",
    "#     words = tokenize_words(article)['words']\n",
    "#     # remove stop words\n",
    "#     words = [word for word in words if not word in stop_words]\n",
    "#     # reduce words to its base and make all words lowercase\n",
    "#     words = set([porter.stem(word) for word in words])\n",
    "#     processed_text = ' '.join(words)\n",
    "#     return processed_text\n",
    "\n",
    "def pre_process_text(article, as_string=True): # fixed function\n",
    "    \"\"\" Some preprocessing and cleaning tasks to be applied to the raw input text\n",
    "    of an article.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    porter = PorterStemmer()\n",
    "    # tokenize article into words\n",
    "    words = tokenize_words(article)['words']\n",
    "    # lower case\n",
    "    words = [word.lower() for word in words]\n",
    "    # remove stop words\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    # reduce words to its base and make all words lowercase\n",
    "    words = [porter.stem(word) for word in words]\n",
    "    # remove punctuation from word list\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation + '—'))\n",
    "    words = [re_punc.sub('', w) for w in words]\n",
    "    if as_string:\n",
    "        # return as single string, required for, e.g., count or TFDIF vectorizers\n",
    "        processed_text = ' '.join(words)\n",
    "    else:\n",
    "        # return list of words, required for, e.g., Word2Vec models\n",
    "        processed_text = words\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will utilize the functions above to build our model. A model will be represented as a Python class. This design decision was driven by a view that our model should be \"productized\" to some degree. A user of our WhoWroteThis model will want to use a simple and straighforward interface. She should be able to pass in a list of articles in human readable text format and all the activities around text preprocessing, cleaning or vectorizing the data will be hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Bag_of_Words:\n",
    "    \n",
    "    def __init__(self, pickle_file):\n",
    "        \"\"\" The user of this class can specify the file name under which \n",
    "        the trained model will be saved to disk.\n",
    "        \n",
    "        As of now the model is fixed to autmatically train the following\n",
    "        classifiers: logistic regression, decision tree and support vector\n",
    "        machine (SVM).\n",
    "        \"\"\"\n",
    "        self.pickle_file = pickle_file\n",
    "        self.classifiers = [\n",
    "            {'algorithm': linear_model.LogisticRegression(),\n",
    "             'display_text': 'logistic regression'},\n",
    "            {'algorithm': tree.DecisionTreeClassifier(),\n",
    "             'display_text': 'decision tree'},\n",
    "            {'algorithm': svm.SVC(probability=True),\n",
    "             'display_text': 'SVM'}\n",
    "        ]\n",
    "        self.vectorizer = None\n",
    "        self.trained_classifiers = None\n",
    "\n",
    "    def train(self, articles_train, vectorizer_type):\n",
    "        \"\"\" Train the model on the various classifiers.\n",
    "        \"\"\"\n",
    "        self.vectorizer, self.trained_classifiers = train(articles_train,\n",
    "                                                          self.pickle_file,\n",
    "                                                          self.classifiers,\n",
    "                                                          vectorizer_type)\n",
    "\n",
    "    def predict(self, articles, classifier_type):\n",
    "        \"\"\" For a given list of articles and the classifier \n",
    "        type (logistic regression, decision tree and SVM) predict the\n",
    "        authors.\n",
    "        \"\"\"\n",
    "        for clf in self.trained_classifiers:\n",
    "            if clf['display_text'] == classifier_type:\n",
    "                break\n",
    "        articles = transform_vectors(articles, self.vectorizer)\n",
    "        prediction = pd.DataFrame({'author': clf['clf'].classes_,\n",
    "                                   'probability': clf['clf'].predict_proba(articles)[0]})\n",
    "        return prediction\n",
    "    \n",
    "    def score(self, articles):\n",
    "        \"\"\" Score prediction accuracy for a given list of articles.\n",
    "        \"\"\"\n",
    "        labels_actual = articles[['author']]\n",
    "        articles = transform_vectors(articles, self.vectorizer)\n",
    "        clf_predictions = []\n",
    "        for clf in self.trained_classifiers:\n",
    "            labels_predicted = clf['clf'].predict(articles)\n",
    "            score = accuracy_score(labels_actual, labels_predicted)\n",
    "            clf_predictions.append({'classifier': clf['display_text'],\n",
    "                                    'labels_predicted': labels_predicted,\n",
    "                                    'score': score})\n",
    "            print('{} classifier prediction accuracy score : {:.2f}%'.format(clf['display_text'], score*100))\n",
    "        return clf_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our Bag-of-Words model with word counts. Please note the strongly simplified interface by using the `Model_Bag_of_Words` class. It allows to work directly with the article texts and hides the machine learning \"pipeline\" (preprocessing, cleaning, vectorization, training of various classifiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction based on word count\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_WordCount = Model_Bag_of_Words('Model 3 Bag-of-Words - Word Count.pickle')\n",
    "print('training the model\\n')\n",
    "model_WordCount.train(articles_train, CountVectorizer())\n",
    "print('number of training articles: {}'.format(len(articles_train)))\n",
    "print('size of the vocabulary = length of the feature vector: {}'.\\\n",
    "      format(len(model_WordCount.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_WordCount.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "clf_predictions = model_WordCount.score(articles_test)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# save results\n",
    "for clf in clf_predictions:\n",
    "    results = results.append({'group': 'Model 3: Bag-of-Words - Word Count',\n",
    "                              'classifier': clf['classifier'],\n",
    "                              'result': clf['score'],\n",
    "                              'time': total_time},\n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the logistic regression classifier gets us well above 93% prediction accuracy on the test data set. That's not bad! It appears that simply counting the word frequency is already a very strong differentiator between authors. It seems to confirm our intuition that a person is able to differentiate between authors by what they write about. Different topics will naturally use different vocabulary. For future research (not pursued in this project) it might be interesting to research how the classifier works for authors that write about the same or similar topics.\n",
    "\n",
    "From the accuracy scores on training and test data we can see that the SVM classifier doesn't perform so well on this particular problem. This comes a bit as a surprise. According to the [scikit learn documentation](http://scikit-learn.org/stable/modules/svm.html) SVM is a classifier that generally is effective in high dimensional space and, more importantly, is \"still effective in cases where number of dimensions is greater than the number of samples\". In our case the number of 51,770 dimensions (= features) is significantly higher than the 5,756 samples (= number of training articles). Perhaps, because in our case the number of dimensions is almost ten times the number of samples make SVM unsuitable for our problem. The SVM library in scikit learn offers options for optimisation such as various classifiers (SVC, NuSVC and LinearSVC) and different kernel functions. For this project SVM has not been pursued further as the initial results (see above) are way below what the logistic regression classifier achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score as a single value doesn't give us much insight into what the model does well and where it fails. For this we will visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    im = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size=14)\n",
    "    cb = plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, size=14)\n",
    "    plt.yticks(tick_marks, classes, size=14)\n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max()/2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 size=12,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('true label', size=14)\n",
    "    plt.xlabel('predicted label', size=14)\n",
    "    \n",
    "# Compute confusion matrix\n",
    "y_test = articles_test[['author']]\n",
    "for clf in clf_predictions:\n",
    "    y_predict = clf['labels_predicted']\n",
    "    if clf['classifier'] == 'logistic regression classifier':\n",
    "        break\n",
    "cnf_matrix = confusion_matrix(y_test, y_predict)\n",
    "np.set_printoptions(precision=2)\n",
    "class_names = list(set(labels['author']))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "#TODO: Check the total in the confusion matrix. I have the feeling it doesn't add up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix allows us to see where exactlky the misclassifications happening. We can see that M.G. Siegler has been misclassified nine times as Umair Haque. Also, Johnson Kee has been misclassified five times as Chris Messina. The two examples show most of misclassification to one class. With this type of information confusion matrices can provided valuable cues that are helpful for further fine tuning of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Bag-of-Words - TFIDF <a name=\"Model 4 Bag-of-Words - TFIDF\"></a>\n",
    "\n",
    "With all the code developed for Model 3, with reusability in mind, we can now quickly implement another model that utlizes the term frequency–inverse document frequency (TFIDF) approach. In TFIDF, similar to word count, each word represents a feature. Each word is assigned a weight factor that is higher the more often a word occurs in an article. However, at the same time the weight of a word is reduced the more often it occurs across other articles. The idea behind TFIDF is that words that occur more across multiple articles carry less meaning then specialised words that occur within a smaller number of articles.\n",
    "\n",
    "In the code cell below we only  replace the `CountVectorizer()` vectorizer with the `TfidfVectorizer()` vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction based on term frequency–inverse document frequency (TFIDF)\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_TFIDF = Model_Bag_of_Words('Model 4 Bag-of-Words - TFIDF.pickle')\n",
    "print('training the model\\n')\n",
    "vectorizer = TfidfVectorizer()\n",
    "model_TFIDF.train(articles_train, vectorizer)\n",
    "print('number of training articles: {}'.format(len(articles_train)))\n",
    "print('size of the vocabulary = length of the feature vector: {}'.\\\n",
    "      format(len(model_TFIDF.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_TFIDF.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "clf_predictions = model_TFIDF.score(articles_test)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# save results\n",
    "for clf in clf_predictions:\n",
    "    results = results.append({'group': 'Model 4: Bag-of-Words - TFIDF',\n",
    "                              'classifier': clf['classifier'],\n",
    "                              'result': clf['score'],\n",
    "                              'time': total_time},\n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the TFIDF model with logistic regression is almost identical to the word count model described in the previous section. The TFIDF of words doesn't help to increase the accuracy in predicting the author. This can be explained by the fact that authors most probably write about specific topics such as entrepreneurship or astrophysics. With this we can expect a high frequency of key topic words within the articles from the same author but less so across all articles in the training set. This in turn means that the IDF part of TFIDF has less impact on the learning. Which then means the TFIDF gets closer to what the word count does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Bag-of-Words - Reduced Vocabulary <a name=\"Model 5 Bag-of-Words - Reduced Vocabulary\"></a>\n",
    "\n",
    "During pre-processing model 3 extracts a vocabulary 62,797 words from 5,756 training articles. As the vocabulary is used as the feature vector this results in very large vectors. Large vectors impact the performance ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(articles, ngrams):\n",
    "    data = pre_process_articles(articles)\n",
    "    vectorizer = CountVectorizer(ngram_range=(ngrams, ngrams))\n",
    "    data_vectorized = vectorizer.fit_transform(data)\n",
    "    vocabulary = pd.DataFrame({'token': vectorizer.get_feature_names(),\n",
    "                               'occurrence': np.asarray(data_vectorized.sum(axis=0)).ravel()})\n",
    "    vocabulary = vocabulary[['token', 'occurrence']].sort_values('occurrence', ascending=False)\n",
    "    vocabulary = vocabulary.reset_index().drop(['index'], axis=1)\n",
    "    return vocabulary\n",
    "\n",
    "def reduce_vocabulary(vocabulary, min_occurrence):\n",
    "    vocabulary_reduced = vocabulary[vocabulary['occurrence']>min_occurrence]\n",
    "    return vocabulary_reduced\n",
    "\n",
    "def vocabulary_to_dict(vocabulary):\n",
    "    # new index starting from 0 to total number of tokens\n",
    "    vocabulary = vocabulary.reset_index().drop(['occurrence'], axis=1)\n",
    "    # make column token the new index\n",
    "    vocabulary = vocabulary.set_index('token')\n",
    "    # transform into dict object\n",
    "    vocabulary_dict = vocabulary.to_dict()['index']\n",
    "    return vocabulary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "# reduce vocabulary\n",
    "min_occurrence = 4\n",
    "ngrams = 1\n",
    "vocabulary = get_vocabulary(articles_train, ngrams)\n",
    "vocabulary = reduce_vocabulary(vocabulary, min_occurrence)\n",
    "vocabulary_dict = vocabulary_to_dict(vocabulary)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# run the model\n",
    "model_ReducedVocab = Model_Bag_of_Words('Model 5 Bag-of-Words - Reduced Vocabulary.pickle')\n",
    "print('training the model\\n')\n",
    "model_ReducedVocab.train(articles_train, CountVectorizer(vocabulary=vocabulary_dict))\n",
    "print('number of training articles: {}'.format(len(articles_train)))\n",
    "print('size of the vocabulary = length of the feature vector: {}'.\\\n",
    "      format(len(model_ReducedVocab.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_ReducedVocab.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "clf_predictions = model_ReducedVocab.score(articles_test)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# save results\n",
    "for clf in clf_predictions:\n",
    "    results = results.append({'group': 'Model 5: Bag-of-Words - Reduced Vocabulary',\n",
    "                              'classifier': clf['classifier'],\n",
    "                              'result': clf['score'],\n",
    "                              'time': total_time},\n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Bag-of-Words - Bigrams <a name=\"Model 6 Bag-of-Words - Bigrams\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# run the model\n",
    "model_Bigrams = Model_Bag_of_Words('Model 6 Bag-of-Words - Bigrams.pickle')\n",
    "print('training the model\\n')\n",
    "model_Bigrams.train(articles_train, CountVectorizer(ngram_range=(2, 2)))\n",
    "print('number of training articles: {}'.format(len(articles_train)))\n",
    "print('size of the bigram vocabulary = length of the feature vector: {}'.\\\n",
    "      format(len(model_Bigrams.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_Bigrams.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "clf_predictions = model_Bigrams.score(articles_test)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# save results\n",
    "for clf in clf_predictions:\n",
    "    results = results.append({'group': 'Model 6: Bag-of-Words - Bigrams',\n",
    "                              'classifier': clf['classifier'],\n",
    "                              'result': clf['score'],\n",
    "                              'time': total_time},\n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Bag-of-Words - Reduced Bigrams <a name=\"Model 7 Bag-of-Words - Reduced Bigrams\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# reduce vocabulary\n",
    "min_occurrence = 4\n",
    "ngrams = 2\n",
    "vocabulary = get_vocabulary(articles_train, ngrams)\n",
    "vocabulary = reduce_vocabulary(vocabulary, min_occurrence)\n",
    "vocabulary_dict = vocabulary_to_dict(vocabulary)\n",
    "\n",
    "# run the model\n",
    "model_Bigrams = Model_Bag_of_Words('Model 7 Bag-of-Words - Reduced Bigrams.pickle')\n",
    "print('training the model\\n')\n",
    "model_Bigrams.train(articles_train, CountVectorizer(vocabulary=vocabulary_dict,\n",
    "                                                    ngram_range=(2, 2)))\n",
    "print('number of training articles: {}'.format(len(articles_train)))\n",
    "print('size of the bigram vocabulary = length of the feature vector: {}'.\\\n",
    "      format(len(model_Bigrams.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_Bigrams.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "clf_predictions = model_Bigrams.score(articles_test)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# save results\n",
    "for clf in clf_predictions:\n",
    "    results = results.append({'group': 'Model 7: Bag-of-Words - Reduced Bigrams',\n",
    "                              'classifier': clf['classifier'],\n",
    "                              'result': clf['score'],\n",
    "                              'time': total_time},\n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Learn Word Embeddings & CNN <a name=\"Model 8 Learn Word Embeddings & CNN\"></a>\n",
    "\n",
    "The 7,675 articles contain 3,519,027 words (after pre-processing).\n",
    "\n",
    "TODO: test if keeping stop words improves accuracy.\n",
    "\n",
    "\"Yoav Goldberg, in his primer on deep learning for natural language\n",
    "processing, comments that neural networks in general offer better performance than classical\n",
    "linear classifiers, especially when used with pre-trained word embeddings.\"\n",
    "\n",
    "We need a slightly different preprocessing, no stemming keep lower and case\n",
    "\n",
    "Find the paper that from where we took the CNN architecture\n",
    "\n",
    "\n",
    "\n",
    "The one_hot function from the keras.preprocessing.text library was originally used to convert the words in articles to integer values. This was done as preparation to train a CNN. However, one_hot does not encode collision free. The code below has been used to access to what extend collisions occur. Running against a text corpus with 54,619 unique words one_hot caused 22,793 collisions. Meaning 22,793 integers were not uniquely mapped to single words. It is believed that this percentage of collisions significantly distorted the learning of the embedding CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_article(article):\n",
    "    \"\"\" Some preprocessing and cleaning tasks to be applied to the raw input text\n",
    "    of an article.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = tokenize_words(article)['words']\n",
    "    # remove punctuation from word list\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation + '—'))\n",
    "    words = [re_punc.sub('', w) for w in words]\n",
    "    words = [word for word in words if not word=='']\n",
    "    # remove words that are not completely alphabetic\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    # transform to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    # remove stop words\n",
    "    words = [word for word in words if not word.lower() in stop_words]\n",
    "    # remove short words\n",
    "    words = [word for word in words if len(word)>1]\n",
    "    processed_article = words\n",
    "    return processed_article\n",
    "\n",
    "def learn_vocabulary(articles):\n",
    "    vocab = {'': 0}\n",
    "    curr_int = 1\n",
    "    for article in articles:\n",
    "        for word in article:\n",
    "            if not word in vocab.keys():\n",
    "                vocab[word] = curr_int\n",
    "                curr_int += 1\n",
    "    return vocab\n",
    "\n",
    "def encode_articles(pre_processed_articles, vocabulary):\n",
    "    return [encode_article(article, vocabulary) for article in pre_processed_articles]\n",
    "\n",
    "def encode_article(article, vocabulary):\n",
    "    return [vocabulary.get(word, 0) for word in article]\n",
    "\n",
    "def prep_training_data(articles_train):\n",
    "    # encode the author names to integers\n",
    "    author_to_int = {a: i for i, a in enumerate(set(articles_train['author']))}\n",
    "    authors_encoded = [author_to_int[author] for author in list(articles_train['author'])]\n",
    "    # pre-process the articles\n",
    "    pre_processed_articles = [pre_process_article(article['body']) for index, article in articles_train.iterrows()]\n",
    "    # encode words to integers\n",
    "    vocabulary = learn_vocabulary(pre_processed_articles)\n",
    "    vocab_size = len(vocabulary)\n",
    "    encoded_articles = encode_articles(pre_processed_articles, vocabulary)\n",
    "    # get length of longest article\n",
    "    article_lengths = pd.DataFrame([len(article) for article in encoded_articles],\n",
    "                                   columns=['length'])\n",
    "    max_length = article_lengths['length'].max()\n",
    "    # pad article lenghts to longest article \n",
    "    padded_articles = pad_sequences(encoded_articles, maxlen=max_length, padding='post')\n",
    "    return padded_articles, author_to_int, authors_encoded, vocabulary, vocab_size, max_length\n",
    "\n",
    "def prep_test_data(articles_test, vocabulary, max_length, author_to_int):\n",
    "    # pre-process the articles\n",
    "    pre_processed_articles = [pre_process_article(article['body']) for index, article in articles_test.iterrows()]\n",
    "    # encode words to integers\n",
    "    encoded_articles = encode_articles(pre_processed_articles, vocabulary)\n",
    "    # pad article lenghts to longest article \n",
    "    padded_articles = pad_sequences(encoded_articles, maxlen=max_length, padding='post')\n",
    "    # encode the author names to integers\n",
    "    authors_encoded = [author_to_int[author] for author in list(articles_test['author'])]\n",
    "    return padded_articles, authors_encoded\n",
    "\n",
    "def build_CNN(embedding):\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(25, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model_Embeddings_CNN:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def train(self, padded_articles, authors_encoded, model, file):\n",
    "        self.model = model\n",
    "        # fit CNN model and save learned model to file\n",
    "        self.model.fit(padded_articles, authors_encoded, epochs=10, verbose=1)\n",
    "        self.model.save(file)\n",
    "        \n",
    "    def predict(self, articles, vocabulary, author_to_int, max_length):\n",
    "        # pre-process the articles\n",
    "        pre_processed_articles = [pre_process_article(article) for article in articles]\n",
    "        # encode words to integers\n",
    "        encoded_articles = encode_articles(pre_processed_articles, vocabulary)\n",
    "        # pad article lenghts to longest article \n",
    "        padded_articles = pad_sequences(encoded_articles, maxlen=max_length, padding='post')\n",
    "        # make predictions\n",
    "        probabilities = self.model.predict(padded_articles, verbose=1)\n",
    "        predictions = pd.DataFrame({'author': list(author_to_int.keys()),\n",
    "                                    'probability': probabilities[0]})\n",
    "        index = predictions['probability'].idxmax()\n",
    "        author = predictions.loc[index]['author']\n",
    "        probability = predictions.loc[index]['probability']\n",
    "        return author, probability\n",
    "        \n",
    "    def score(self, padded_articles, authors_encoded):\n",
    "        # score the model\n",
    "        loss, accuracy = self.model.evaluate(padded_articles, authors_encoded, verbose=1)\n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "# articles_train = articles_train[:1000]\n",
    "# articles_test = articles_test[:1000]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# prepare the training data\n",
    "padded_articles, author_to_int, authors_encoded, vocabulary, vocab_size, max_length = prep_training_data(articles_train)\n",
    "\n",
    "# build and train the CNN model\n",
    "embedding = Embedding(vocab_size, 100, input_length=max_length)\n",
    "model = build_CNN(embedding)\n",
    "model.summary()\n",
    "model_Embeddings_CNN = Model_Embeddings_CNN()\n",
    "file = 'Model 8 Learn Word Embeddings & CNN.h5'\n",
    "print('training the model\\n')\n",
    "model_Embeddings_CNN.train(padded_articles, authors_encoded, model, file)\n",
    "\n",
    "# assess accuracy of the training data set\n",
    "print('\\nscoring the training data:\\n')\n",
    "loss, accuracy = model_Embeddings_CNN.score(padded_articles, authors_encoded)\n",
    "print('accuracy score : {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "# prepare the test data\n",
    "padded_articles, authors_encoded = prep_test_data(articles_test, vocabulary, max_length, author_to_int)\n",
    "\n",
    "# assess accuracy of the test data set\n",
    "print('\\nscoring the test data:\\n')\n",
    "loss, accuracy = model_Embeddings_CNN.score(padded_articles, authors_encoded)\n",
    "print('accuracy score : {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 8: Learn Word Embeddings & CNN',\n",
    "                          'classifier': 'CNN',\n",
    "                          'result': accuracy,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "text = articles.iloc[3]['body']\n",
    "author = articles.iloc[3]['author']\n",
    "print('\\nthis text ...\\n')\n",
    "print(author)\n",
    "print(text)\n",
    "author, probability = model_Embeddings_CNN.predict([text], vocabulary, author_to_int, max_length)\n",
    "print('\\n... was written by ...\\n')\n",
    "print('{} with {:.2f}% confidence'.format(author, probability*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 9: GloVe Word Embeddings & CNN <a name=\"Model 9 GloVe Word Embeddings & CNN\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_GloVe_weights(vocabulary):\n",
    "    pickle_file = 'embeddings_matrix.pickle'\n",
    "    if os.path.isfile(pickle_file):\n",
    "        # get the embedding matrix from file\n",
    "        embedding_matrix = load_embedding_matrix(pickle_file)\n",
    "    else:\n",
    "        # create embedding matrix and save to file\n",
    "        embeddings_index = load_trained_GloVe()\n",
    "        embedding_matrix = zeros((len(vocabulary), 300))\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        # save embedding matrix to file\n",
    "        save_embedding_matrix(embedding_matrix)\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_embedding_matrix(pickle_file):\n",
    "    with open(pickle_file,'rb') as f:\n",
    "        embedding_matrix = pickle.loads(f.read())\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_trained_GloVe():\n",
    "    # load pre-trained GloVe embedding from ZIP file\n",
    "    # The zip file with the pre-trained word vectors were downloaded \n",
    "    # from http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    embeddings_index = {}\n",
    "    with zipfile.ZipFile('glove.6B.zip', 'r') as zf:\n",
    "        with zf.open('glove.6B.300d.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0].decode(\"utf-8\")\n",
    "                coefs = asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def save_embedding_matrix(embedding_matrix):\n",
    "    pickle_file = 'embeddings_matrix.pickle'\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        s = pickle.dumps(embedding_matrix)\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, vocabulary, _, _ = prep_training_data(articles)\n",
    "\n",
    "# assign GloVe weights to words\n",
    "embedding_matrix = assign_GloVe_weights(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "# articles_train = articles_train[:100]\n",
    "# articles_test = articles_test[:100]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# prepare the training data\n",
    "padded_articles, author_to_int, authors_encoded, vocabulary, vocab_size, max_length = prep_training_data(articles_train)\n",
    "\n",
    "# assign GloVe weights to words\n",
    "embedding_matrix = assign_GloVe_weights(vocabulary)\n",
    "\n",
    "# build and train the CNN model\n",
    "# TODO: test with trainable=True\n",
    "embedding = Embedding(vocab_size, 300, weights=[embedding_matrix],\n",
    "                      input_length=max_length, trainable=False)\n",
    "model = build_CNN(embedding)\n",
    "model.summary()\n",
    "model_GloVe = Model_Embeddings_CNN()\n",
    "file = 'Model 9 GloVe Word Embeddings & CNN.h5'\n",
    "print('training the model\\n')\n",
    "model_GloVe.train(padded_articles, authors_encoded, model, file)\n",
    "\n",
    "# assess accuracy of the training data set\n",
    "print('\\nscoring the training data:\\n')\n",
    "loss, accuracy = model_GloVe.score(padded_articles, authors_encoded)\n",
    "print('accuracy score : {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "# prepare the test data\n",
    "padded_articles, authors_encoded = prep_test_data(articles_test, vocabulary, max_length, author_to_int)\n",
    "\n",
    "# assess accuracy of the test data set\n",
    "print('\\nscoring the test data:\\n')\n",
    "loss, accuracy = model_GloVe.score(padded_articles, authors_encoded)\n",
    "print('accuracy score : {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# save results\n",
    "results = results.append({'group': 'Model 9: GloVe Word Embeddings & CNN',\n",
    "                          'classifier': 'CNN',\n",
    "                          'result': accuracy,\n",
    "                          'time': total_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion <a name=\"Conclusion\"></a>\n",
    "\n",
    "TODO: test with smaller padding of the articles\n",
    "TODO: test what if longest article is in test data set\n",
    "TODO: check how many words are encoded with 0\n",
    "\n",
    "\n",
    "combine with text metrics from model 2 for refinement\n",
    "\n",
    "effort in refactoring code\n",
    "\n",
    "Free-Form Visualization\n",
    "\t\n",
    "A visualization has been provided that emphasizes an important quality about the project with thorough discussion. Visual cues are clearly defined.\n",
    "\n",
    "Reflection\n",
    "\t\n",
    "Student adequately summarizes the end-to-end problem solution and discusses one or two particular aspects of the project they found interesting or difficult.\n",
    "\n",
    "Improvement\n",
    "\t\n",
    "Discussion is made as to how one aspect of the implementation could be improved. Potential solutions resulting from these improvements are considered and compared/contrasted to the current solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_to_bar(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        width = rect.get_width()\n",
    "        if not np.isnan(width):\n",
    "            ax.text(rect.get_x() + width + 1, rect.get_y() + 0.125,\n",
    "                        '{:.2f}%'.format(width), fontsize=12,\n",
    "                        ha='left', va='center')\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# group by model\n",
    "results_percent = results.copy()\n",
    "results_percent['result'] = results['result'] * 100\n",
    "result_groups = results_percent.pivot(index='group', columns='classifier', values='result')\n",
    "# within each group we will show each classifier\n",
    "classifiers = ['CNN',\n",
    "               'random selection',\n",
    "               'grid search decision tree',\n",
    "               'decision tree',\n",
    "               'SVM',\n",
    "               'logistic regression']\n",
    "\n",
    "# set some general plotting parameters\n",
    "pos = list(range(len(list(result_groups.index))))\n",
    "height = 0.25\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "# get some nice looking color scheme from https://coolors.co/162833-e2b161-d5573b-faffef-c7c9c8\n",
    "color_cycler = itertools.cycle(['#706c61', '#9e9889', '#c4b899', '#eddcaf', '#f4d681', '#f4ba1a'])\n",
    "\n",
    "# generate the bar charts\n",
    "x_values = result_groups['CNN']\n",
    "rects = plt.barh([p + 2 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "x_values = result_groups['random selection']\n",
    "rects = plt.barh([p + 2 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "x_values = result_groups['grid search decision tree']\n",
    "rects = plt.barh([p + 2 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "x_values = result_groups['decision tree']\n",
    "rects = plt.barh([p + 1 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "x_values = result_groups['SVM']\n",
    "rects = plt.barh([p + 2 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "x_values = result_groups['logistic regression']\n",
    "rects = plt.barh([p + 3 * height for p in pos], x_values, height=height, color=next(color_cycler))\n",
    "percentage_to_bar(rects)\n",
    "\n",
    "# set the chart's title\n",
    "ax.set_title('Prediction Accuracy')\n",
    "\n",
    "# set the x axis label\n",
    "ax.set_xlabel('accuracy predicting test data set in percent')\n",
    "\n",
    "# set the position of the y ticks\n",
    "ax.set_yticks([p + 0.5 for p in pos])\n",
    "\n",
    "# Set the labels for the x ticks\n",
    "model_names = list(result_groups.index)\n",
    "ax.set_yticklabels(list(model_names))\n",
    "\n",
    "# Adding the legend and showing the plot\n",
    "plt.legend(classifiers, loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Notebook_total_time = time.time() - Notebook_start_time\n",
    "print('total run time for this Notebook: {:.1f} hours'.format(Notebook_total_time/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's have some fun: the WhoWroteThis app <a name=\"WhoWroteThis app\"></a>\n",
    "\n",
    "Now that we have a working prediction model let's build a little app that makes use of the model we have trained and tested.\n",
    "\n",
    "Run the cell below and follow the instructions shown in the output area.\n",
    "\n",
    "Used glove.42B.300d.zip as this was trained on Wikipedia. Probably close to our problem in terms of article length and content structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the list of Medium authors below pick one. Select an article and copy the\n",
      "text in the text box below. Press the \"Who wrote this?\" button and see whether\n",
      "the right author is predicted.\n",
      "\n",
      "https://medium.com/@tedr/latest\n",
      "https://medium.com/@garyvee/latest\n",
      "https://medium.com/@NicoleDieker/latest\n",
      "https://medium.com/@fabriciot/latest\n",
      "https://medium.com/@esterbloom/latest\n",
      "https://medium.com/@startswithabang/latest\n",
      "https://medium.com/@girard_yann/latest\n",
      "https://medium.com/@pfinette/latest\n",
      "https://medium.com/@jonwestenberg/latest\n",
      "https://medium.com/@chrismessina/latest\n",
      "https://medium.com/@msuster/latest\n",
      "https://medium.com/@howardlindzon/latest\n",
      "https://medium.com/@umairh/latest\n",
      "https://medium.com/@jaltucher/latest\n",
      "https://medium.com/@JohnsonKee/latest\n",
      "https://medium.com/@larrykim/latest\n",
      "https://medium.com/@seanmeverett/latest\n",
      "https://medium.com/@timboucher/latest\n",
      "https://medium.com/@mgsiegler/latest\n",
      "https://medium.com/@nireyal/latest\n",
      "https://medium.com/@skooloflife/latest\n",
      "https://medium.com/@bfeld/latest\n",
      "https://medium.com/@ToddBrison/latest\n",
      "https://medium.com/@ebonstorm/latest\n",
      "https://medium.com/@gassee/latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b856bfedf8f41079b877a7aa842a7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23286acd2d054e6c97959e82635d9fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def on_button_clicked(button):\n",
    "    test_article = pd.DataFrame([text_input_box.value], columns=['body'])\n",
    "    predictions = model_WordCount.predict(test_article, 'logistic regression classifier')\n",
    "    index = predictions['probability'].idxmax()\n",
    "    author = predictions.loc[index]['author']\n",
    "    probability = predictions.loc[index]['probability']*100\n",
    "    print('I\\'m {:.0f} percent certain that this was written by... {}.'.format(probability, author))\n",
    "\n",
    "text_input_box = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Please paste an article here.',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='700px', height='300px')\n",
    ")\n",
    "button = widgets.Button(\n",
    "    description='Who wrote this?',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    ")\n",
    "\n",
    "print('From the list of Medium authors below pick one. Select an article and copy the')\n",
    "print('text in the text box below. Press the \"Who wrote this?\" button and see whether')\n",
    "print('the right author is predicted.\\n')\n",
    "with open('Medium_authors_25.txt','r') as f:\n",
    "    author_urls = f.read()\n",
    "print(author_urls)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(text_input_box, button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*\"I can guarantee you a purity of 96%. I'm proud of that figure. It's a hard-earned figure, 96. However, this other product is 99 maybe even a touch beyond that. [...] but that last 3%, it may not sound like a lot but it is. It's tremendous. It's a tremendous gulf.\"*\n",
    "\n",
    ">Gale Boetticher, Breaking Bad, season 4, episode 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sources <a name=\"Sources\"></a>\n",
    "\n",
    "[1] [Famous Authors and Their Writing Styles](https://www.craftyourcontent.com/famous-authors-writing-styles/)\n",
    "\n",
    "[2] [\"Who’s At The Keyboard? Authorship Attribution in Digital Evidence Investigations\"](www.utica.edu/academic/institutes/ecii/publications/articles/B49F9C4A-0362-765C-6A235CB8ABDFACFF.pdf): good reference regarding number of authors and text volume.\n",
    "\n",
    "[3] [\"How a Computer Program Helped Show J.K. Rowling write A Cuckoo’s Calling\"](https://www.scientificamerican.com/article/how-a-computer-program-helped-show-jk-rowling-write-a-cuckoos-calling/): some good ideas for feature to analyse.\n",
    "\n",
    "[4] [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)\n",
    "\n",
    "[5] [scikit learn documentation: confusion matrix](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)\n",
    "\n",
    "[6] Jason Brownlee book\n",
    "\n",
    "[7] http://ruder.io/deep-learning-nlp-best-practices/index.html#wordembeddings\n",
    "\n",
    "[8] https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix\n",
    "\n",
    "This section contains notes and code fragments not required for the Capstone project. These were often developed as part of trouble shooting activities and kept here for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 2 required positional arguments: 'model' and 'file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-a89fc3913c64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mauthor_to_int\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel_Embeddings_CNN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel_Embeddings_CNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0marticles_processed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_articles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_Embeddings_CNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauthor_to_int\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mencoded_to_word_mapping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 2 required positional arguments: 'model' and 'file'"
     ]
    }
   ],
   "source": [
    "#\n",
    "# check for token collisions\n",
    "#\n",
    "# The one_hot function from the keras.preprocessing.text library was originally used\n",
    "# to convert the words in articles to integer values. This was done as preparation to\n",
    "# train a CNN. However, one_hot does not encode collision free. The code below has been\n",
    "# used to access to what extend collisions occur.\n",
    "#\n",
    "# Running against a text corpus with 54,619 unique words one_hot caused 22,793 collisions.\n",
    "# Meaning 22,793 integers were not uniquely mapped to single words. It is believed that this\n",
    "# percentage of collisions significantly distorted the learning of the embedding CNN.\n",
    "#\n",
    "\n",
    "author_to_int = {a: i for i, a in enumerate(set(articles['author']))}\n",
    "model_Embeddings_CNN = Model_Embeddings_CNN()\n",
    "articles_processed, encoded_articles = model_Embeddings_CNN.train(articles, author_to_int)\n",
    "\n",
    "encoded_to_word_mapping = {}\n",
    "for i, article in enumerate(articles_processed):\n",
    "    # process every article, this takes a while, show status update every 100 articles\n",
    "    if i%100 == 0:\n",
    "        print('{} articles processed'.format(i))\n",
    "    # for each article generate a table that shows integer to word mapping\n",
    "    encoded_to_word = pd.DataFrame({'encoded': encoded_articles[i],\n",
    "                                    'word': article})\n",
    "    # get the list of unique integer values in an article\n",
    "    unique_encodes = set(encoded_to_word['encoded'])\n",
    "    for unique_encode in unique_encodes:\n",
    "        # For each integer get all the words that have been mapped against this\n",
    "        # integer. In a collision free mapping this should generate a list of \n",
    "        # 1 - n occurrences of the same word (e.g. ['the', 'the', 'the', 'the'])\n",
    "        words = list(encoded_to_word[encoded_to_word['encoded']==unique_encode]['word'].values)\n",
    "        words = [word.lower() for word in words]\n",
    "        # Add the list of words to a global dict that maps an integer\n",
    "        # to the words. Again, in a collision free mapping this should be \n",
    "        # list of the same word listed one or more times.\n",
    "        if encoded_to_word_mapping.get(unique_encode) is None:\n",
    "            encoded_to_word_mapping[unique_encode] = words\n",
    "        else:\n",
    "            encoded_to_word_mapping[unique_encode] = encoded_to_word_mapping[unique_encode] + words\n",
    "\n",
    "print('number of entries in encode to word map: {}'.format(len(encoded_to_word_mapping)))\n",
    "\n",
    "# Now go through the list of integers and check that all articles\n",
    "# mapped the same word against this integer.\n",
    "found_something = False\n",
    "for key in encoded_to_word_mapping.keys():\n",
    "    if len(set(encoded_to_word_mapping[key]))>1:\n",
    "        # There is more than one word mapped against one integer. This is\n",
    "        # a collision!\n",
    "        found_something = True\n",
    "        print('{}: {}'.format(key, set(encoded_to_word_mapping[key])))\n",
    "if not found_something:\n",
    "    print('No collisions found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# show article pre-processing and encoding of words to integer\n",
    "#\n",
    "\n",
    "# split training and test data\n",
    "articles_train, articles_test = train_test_split(articles[:100], test_size=0.25, random_state=23)\n",
    "# create a mapping from author names to integers\n",
    "author_to_int = {a: i for i, a in enumerate(set(articles['author']))}\n",
    "\n",
    "model_Embeddings_CNN = Model_Embeddings_CNN()\n",
    "proc_art_train, enc_art_train = model_Embeddings_CNN.train(articles_train, author_to_int)\n",
    "proc_art_test, enc_art_test = model_Embeddings_CNN.predict(articles_test)\n",
    "\n",
    "i = 6\n",
    "\n",
    "print('processed train article ({} words):'.format(len(proc_art_train[i])))\n",
    "print('---------------------------------------')\n",
    "print(proc_art_train[i])\n",
    "print('\\n')\n",
    "print('encoded train article ({} numbers):'.format(len(enc_art_train[i])))\n",
    "print('---------------------------------------')\n",
    "print(enc_art_train[i])\n",
    "print('\\n')\n",
    "print('processed test article ({} words):'.format(len(proc_art_test[i])))\n",
    "print('---------------------------------------')\n",
    "print(proc_art_test[i])\n",
    "print('\\n')\n",
    "print('encoded test article ({} numbers):'.format(len(enc_art_test[i])))\n",
    "print('---------------------------------------')\n",
    "print(enc_art_test[i])\n",
    "print('\\n')\n",
    "\n",
    "train_zip = pd.DataFrame({'encoded': enc_art_train[i],\n",
    "                          'word': proc_art_train[i]})\n",
    "test_zip = pd.DataFrame({'encoded': enc_art_test[i],\n",
    "                          'word': proc_art_test[i]})\n",
    "print('train zip:')\n",
    "display(train_zip.sort_values('encoded'))\n",
    "print('\\n')\n",
    "print('test zip:')\n",
    "display(test_zip.sort_values('encoded'))\n",
    "print('\\n')\n",
    "\n",
    "print('word ID matches between train and test article:')\n",
    "matches = list(set(enc_art_train[i]) & set(enc_art_test[i]))\n",
    "print(matches)\n",
    "print('\\n')\n",
    "for match in matches:\n",
    "    print('index: {}'.format(match))\n",
    "    print('encoded train word: {}'.format(train_zip[train_zip['encoded']==match]['word'].values))\n",
    "    print('encoded test word : {}'.format(test_zip[test_zip['encoded']==match]['word'].values))\n",
    "print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
