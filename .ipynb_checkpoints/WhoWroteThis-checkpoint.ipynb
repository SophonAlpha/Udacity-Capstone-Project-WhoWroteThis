{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project - Who Wrote This? \n",
    "... or how to identify the author of anonymous articles using natural languange processing.\n",
    "\n",
    "Stefan Dittforth  \n",
    "February 27th, 2018\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "\n",
    "[1] [\"Who’s At The Keyboard? Authorship Attribution in Digital Evidence Investigations\"](www.utica.edu/academic/institutes/ecii/publications/articles/B49F9C4A-0362-765C-6A235CB8ABDFACFF.pdf): good reference regarding number of authors and text volume.\n",
    "\n",
    "[2] [\"How a Computer Program Helped Show J.K. Rowling write A Cuckoo’s Calling\"](https://www.scientificamerican.com/article/how-a-computer-program-helped-show-jk-rowling-write-a-cuckoos-calling/): some good ideas for feature to analyse.\n",
    "\n",
    "[3] [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)\n",
    "\n",
    "[4] [scikit learn documentation: confusion matrix](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)\n",
    "\n",
    "[5] Jason Brownlee book\n",
    "\n",
    "## Research Notes:\n",
    "\n",
    "- add some article screen shots to the text\n",
    "- test approaches suggested in [1]\n",
    "- measure average word lentgh per article, distribution of word lengths, the 100 most common words, distribution of character 4-grams, word bigrams\n",
    "- test with support vector machines\n",
    "- using word embeddings as a much rich representation\n",
    "Some good tips from [3]:\n",
    "- Clean data:\n",
    "    - Remove all irrelevant characters such as any non alphanumeric characters\n",
    "    - Tokenize your text by separating it into individual words\n",
    "    - Remove words that are not relevant, such as “@” twitter mentions or urls\n",
    "    - Convert all characters to lowercase, in order to treat words such as “hello”, “Hello”, and “HELLO” the same\n",
    "    - Consider combining misspelled or alternately spelled words to a single representation (e.g. “cool”/”kewl”/”cooool”)\n",
    "    - Consider lemmatization (reduce words such as “am”, “are”, and “is” to a common form such as “be”)\n",
    "- do random guess model\n",
    "- One-hot encoding (Bag of Words)\n",
    "- do we do bag-of-words on sentence level or article level? hmm ...\n",
    "- apply PCA to visualise bag of words\n",
    "- start with testing logistic regression for classification\n",
    "- visualise confusion matrix\n",
    "- vocabulary structure TF-IDF\n",
    "- Word2Vec?\n",
    "- LSTM\n",
    "- Convolutional Neural Networks for Sentence Classification\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------\n",
    "## Introduction\n",
    "\n",
    "write some introduction\n",
    "\n",
    "----------------\n",
    "## Data Acquisition\n",
    "\n",
    "In order to allow the system to learn the writing characteristics of different authors we require a dataset that provides a large number of articles for individual authors. There are rich datasets for NLP research available in the public domain. A list, as an example, can be found [here](https://github.com/niderhoff/nlp-datasets). However, as part of this project we will build our own dataset. We will develop a web scraper that will collect articles from the publishing platform Medium. The articles on Medium seem to be reasonably long (at least several hundred words). There are enough authors that have published several hundreds articles. With this, it appears feasible to acquire a large enough data set to learn patterns in the writing characteristics to distinguish between individual authors.\n",
    "\n",
    "This approach has been chosen as an opportunity to develop practical experience not only in machine learning but also around data acquisition. In data science and machine learning the acquisition and preparation of high quality data is often the bigger challenge than the actual development of the machine learning system itself. In \"[Datasets Over Algorithms](https://www.edge.org/response-detail/26587)\" author Alexander Wissner-Gross notes that \n",
    "\n",
    ">*\"the average elapsed time between key [machine learning] algorithm proposals and corresponding advances was about eighteen years, whereas the average elapsed time between key dataset availabilities and corresponding advances was less than three years, or about six times faster, suggesting that datasets might have been limiting factors in the advances.\"*.\n",
    "\n",
    "Conveniently the website [Top Authors](https://topauthors.xyz/) has published a list of 300+ top Medium authors. The project folder contains the short script `get_list_of_Medium_authors.py` that has been used to extract the Medium URL for each author. The initial list of 300+ authors has been reduced to 25. The criteria for this reduction was the number of published articles. For the 25 authors there are at least 300 articles available. The Medium URLs for these authors can be found in file `Medium_authors_25.txt`.\n",
    "\n",
    "<img src=\"notebook/Top Authors.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<center>*getting a list of Medium \n",
    "authors*</center>\n",
    "\n",
    "The actual collection of the articles is done with the script `pull_Medium_articles.py`. The script performs two steps. First, it builds a list of all article URLs and for each article saves author URL and article URL in JSON format in the file `Medium_article_urls.json`. Below is an example how the entries for three articles look like.\n",
    "\n",
    "```javascript\n",
    "{\"author_URL\": \"https://medium.com/@tedr/latest\\n\",\n",
    " \"article_URL\": \"https://medium.com/season-of-the-witch/etiquette-and-the-cancer-patient-630a50047448?source=user_profile---------1----------------\"}\n",
    "{\"author_URL\": \"https://medium.com/@esterbloom/latest\\n\",\n",
    " \"article_URL\": \"https://medium.com/the-billfold/failing-at-shoplifting-life-with-kesha-bc2600b1f440?source=user_profile---------789----------------\"}\n",
    "{\"author_URL\": \"https://medium.com/@gassee/latest\",\n",
    " \"article_URL\": \"https://mondaynote.com/the-ny-times-un-free-at-last-df2eddba360b?source=user_profile---------281----------------\"}\n",
    "```\n",
    "\n",
    "The second part performs the actual download of the articles. The script reads the article URL saved in `Medium_article_urls.json`, navigates to the website and reads the text information from the html code of the article website. Each article is saved in text format in its own file. For each author a folder is generated that contains the articles for that author. Initially it was intended to store all articles in JSON format in one file. This turned out to be very cumbersome when troubleshooting the `pull_Medium_articles.py` script. Having a folder structure that allows to do quick visual inspections over the list of files in a file manager proved very helpful. In addition, the smaller article files made it easier to spot check the downloaded text information in a text editor.\n",
    "\n",
    "During research for this project several Python libraries for interacting with websites have been explored: [mechanize](https://pypi.python.org/pypi/mechanize/0.3.6), [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#), [scrapy](https://scrapy.org/) and the [Selenium WebDriver](https://www.seleniumhq.org/projects/webdriver/). Eventually the decision was made to use the Selenium WebDriver. The key reason for this was: the Medium website uses a two step login process. The users provides its email address and then receives a one time login link via this email. That made it difficult to automate the login via script and ruled out all the libraries that don't allow user interaction with the website.\n",
    "\n",
    "Once an article website is loaded, the required information can be pulled from the text attribute of specific html elements. The code snipped below shows the commands used to get the author name and the article text.\n",
    "\n",
    "```python\n",
    "author = self.browser.find_element_by_xpath('//a[@rel=\"author cc:attributionUrl\"]').text\n",
    "body = self.browser.find_element_by_xpath('//div[@class=\"postArticle-content js-postField js-notesSource js-trackedPost\"]').text\n",
    "```\n",
    "\n",
    "As shown in the code snippet above the right elements are addressed by their respective xpath. Finding these xpaths required a bit of trail and error. A valuable tool for this is the FireFox Inspector. It allows to inspect the code and structure of a website and to find the right path to the right html element.\n",
    "\n",
    "![Top Authors](notebook\\Firefox Inspector.jpg)\n",
    "<center>*finding the right xpath with Firefox Inspector*</center>\n",
    "\n",
    "After the `pull_Medium_articles.py` script completed, the folder `Medium_articles` containing all article files has been compressed into a ZIP archive to preserve storage. With [`zipfile`](https://docs.python.org/3/library/zipfile.html) Python provides a library to work with ZIP archives. Going forward in this Notebook we will make use of this library to work with the files directly within the ZIP archive without the need to extract the archive.\n",
    "\n",
    "Developing a web scraper script poses its own challenges. The initial idea is pretty straightforward: here is a list of URLs, go to each website, download the text part and save it in a file. As always, the pitfalls are discovered during implementation. Some time had to be invested to understand the structure of the Medium article websites and figure out the best way to find the right html elements that contain the required information. The Selenium WebDriver is not the most effective tool when it comes to scraping several thousand websites. The time to render each and every website adds up. An attempt has been made by parallelising the article download with multi-threading and spawning of several instances of the Firefox browser. This failed. It turned out that the fast sequence of websites caused Firefox to slowly consume all available memory and eventually Firefox stopped fetching new websites. In a parallelised version of the script the problem was only exaggerated. Finally, a pragamatic approach was taken and the script has been amended with the capability to continue the work where it has left off from a previous run. Over the course of several days the script has been restarted several times and eventually saved all articles.\n",
    "\n",
    "In defense for Selenium, it needs to be noted that Selenium first and foremost is a tool to automate testing of websites and not a tool for scaping several thousand websites. The primary goal behind the `pull_Medium_articles.py` script was to get the data for this capstone project and not to develop a sophisticated web scraper. In this respect Selenium did the job. Despite the challenges, developing the web scraper script has been a worthwhile learing experience. It provided an opportunity to develop practical experience not only in machine learning but also around data acquisition.\n",
    "\n",
    "In the next section we will explore the data set. We will also check what data preparation or cleaning activities might be required before we can apply algorithms  to the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------\n",
    "## Exploring the Data Set\n",
    "\n",
    "All articles have been downloaded in individual text files and into folders for each author. This folder and file structure has been archived into `Medium_articles.zip`. Let's go through the files and get an overview about the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use this code block to load all the libraries we will need\n",
    "# throughout the Notebook. Keeping all library calls in one place at\n",
    "# beginning allows to run other code cells more independently.\n",
    "\n",
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import pprint\n",
    "import pickle\n",
    "import itertools\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from cycler import cycler\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# some global settings for the Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams[\"figure.figsize\"] = [13, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_article_from_zip_file(file_name):\n",
    "    \"\"\"\n",
    "    A generator function that reads the files in a zip archive one by one.\n",
    "    \"\"\"\n",
    "    zf = zipfile.ZipFile(file_name, 'r')\n",
    "    names = zf.namelist()\n",
    "    for name in names:\n",
    "        if name.endswith('.json'):\n",
    "            data = zf.read(name)\n",
    "            yield data\n",
    "    zf.close()\n",
    "    return\n",
    "\n",
    "# read all articles into a pandas data frame\n",
    "articles = pd.DataFrame(columns=['url', 'author', 'headline', 'body'])\n",
    "for article_file in get_next_article_from_zip_file('Medium_articles.zip'):\n",
    "    article = json.loads(article_file)\n",
    "    articles = articles.append(json.loads(article_file), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a list of authors and the number of articles for each author\n",
    "summary = articles[['author', 'body']].groupby('author').count()\n",
    "summary.columns = ['number of articles']\n",
    "display(summary.sort_values('number of articles', ascending=False))\n",
    "print('Total number of articles: {:,}'.format(len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Dataset\n",
    "\n",
    "The articles were downloaded by extracting the 'text' attribute from html elements that contain the body text of the article. Many articles contain images, URLs to other pages, etc. We are interested to see if the text extracts still contain html fragments we might need to clean out. We do a qick check by listing the articles that contain the '<' and '>' characters. Those two characters enclose HTML tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'<.*>' # find any HTML tag enclosing brackets\n",
    "counter = 0\n",
    "for index, article in articles.iterrows():\n",
    "    matches = re.finditer(regex, article['body'], re.DOTALL)\n",
    "    for matchNum, match in enumerate(matches):\n",
    "        counter = counter + 1\n",
    "        print('Match number: {}'.format(counter))\n",
    "        print('Match in article index: {}'.format(index))\n",
    "        print(article['url'])\n",
    "        print(match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 156 occurences for the '<' and '>' characters in over 18,500 articles. Doing some spot checks it appears that the tag brackets are genuine parts of the article text (for example the text is about HTML coding). We will leave them in for now. No need to clean them out.\n",
    "\n",
    "Let's have a look at some article texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A few example articles:')\n",
    "for _ in range(1,4):\n",
    "    i = random.randrange(len(articles))\n",
    "    print('----------------------')\n",
    "    print('Index: {}'.format(i))\n",
    "    print('URL: {}'.format(articles.iloc[i]['url']))\n",
    "    print('Author: {}'.format(articles.iloc[i]['author']))\n",
    "    print('Headline: {}'.format(articles.iloc[i]['headline']))\n",
    "    print('Body: {}'.format(articles.iloc[i]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above randomly selects 3 articles. We run it a few times. At each run we spot check the text in the 'Body' section. It appears that the text is clean and good to go for our next step.\n",
    "\n",
    "Typically NLP text cleaning tasks include removing punctuation characters. For now we will keep them. In one of our first classification approaches we will use them to engineer features around number and length of sentences and paragraphs in articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Balancing the Data Set\n",
    "\n",
    "As seen above in the table with the number of articles per authors the data set is skewed. The number of articles ranges from 1,948 for Nicole Dieker and 307 for Gary Vaynerchuk. To avoid that our system develops a bias towards authors with a high number of articles we will balance the data set. This will be done by keeping the number of articles for each author equal to the author with the lowest number of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the smallest number for articles for one author \n",
    "num_articles_to_keep = summary.min(axis=0)['number of articles']\n",
    "\n",
    "# only keep the smallest number of articles for each author\n",
    "indices_to_keep = []\n",
    "for author, num_of_articles in summary.iterrows():\n",
    "    indices_to_keep = indices_to_keep + \\\n",
    "                      list(articles[articles.author == author]\\\n",
    "                      [:num_articles_to_keep].index.values)\n",
    "articles = articles.iloc[indices_to_keep]\n",
    "\n",
    "# check number of articles for each author\n",
    "summary = articles[['author', 'body']].groupby('author').count()\n",
    "summary.columns = ['number of articles']\n",
    "display(summary.sort_values('number of articles', ascending=False))\n",
    "print('Total number of articles: {:,}'.format(len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: The Baseline Model - Just Guess the Author\n",
    "\n",
    "In the course of this notebook we will try several approaches to predict the author of a given text. To assess the quality of the prediction we will need to compare against some baseline. In our case we will simply do a random guess of who the author of an article is. This should get us in the order of $\\frac{1}{n}\\cdot100$ percent accuracy, where $n$ represents the number of authors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = articles[['author']]\n",
    "authors = labels.author.unique()\n",
    "y_predict = pd.DataFrame()\n",
    "y_predict['author'] = np.random.choice((authors), len(labels))\n",
    "score_test = accuracy_score(labels, y_predict)\n",
    "\n",
    "# report random guess results\n",
    "print('Just guess the author:')\n",
    "print('----------------------\\n')\n",
    "print('number of authors in data set: {}'.format(len(authors)))\n",
    "print('expected prediction accuracy around: {:.2f}%'.format(1/len(authors)*100))\n",
    "print('prediction accuracy score on data set: {:.2f}%'.format(score_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Basic Article Metrics\n",
    "\n",
    "Let's first come up with some simple metrics as features that might help us to predict the author of an article.  Some metrics that come to mind are: the total, mean, median, min and max number of words in paragraphs, sentences and the article itself. When looking at different authors it appears that these features might be useful differentiators. Some authors have a tendency to longer articles. Others use longer sentences or shorter paragraphs.\n",
    "\n",
    "The code below \"tokenizes\" the articles into paragraphs, sentences and words. In addition in counts the number of each. The tokenized article is returned as a nested dict object. The next code cell displays an example for a tokenized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "def tokenize_paragraphs(text):\n",
    "    paragraphs = [p for p in text.split('\\n')]\n",
    "    paragraphs_tokenized = []\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = tokenize_sentences(paragraph)\n",
    "        paragraphs_tokenized.append(sentences)\n",
    "    num_paragraphs = len(paragraphs_tokenized)\n",
    "    text_tokenized = {'num_paragraphs': num_paragraphs,\n",
    "                      'paragraphs': paragraphs_tokenized}\n",
    "    return text_tokenized\n",
    "\n",
    "def tokenize_sentences(paragraph):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    sentences_tokenized = []\n",
    "    for sentence in sentences:\n",
    "        words = tokenize_words(sentence)\n",
    "        sentences_tokenized.append(words)\n",
    "    num_sentences = len(sentences_tokenized)\n",
    "    paragraph_tokenized = {'num_sentences': num_sentences,\n",
    "                           'sentences': sentences_tokenized}\n",
    "    return paragraph_tokenized\n",
    "\n",
    "def tokenize_words(sentence):\n",
    "    words_tokenized = word_tokenize(sentence)\n",
    "    # remove punctuations from word list\n",
    "    words_tokenized = remove_punctuation(words_tokenized)\n",
    "    num_words = len(words_tokenized)\n",
    "    sentence_tokenized = {'num_words': num_words,\n",
    "                          'words': words_tokenized}\n",
    "    return sentence_tokenized\n",
    "\n",
    "def remove_punctuation(words_tokenized):\n",
    "    return [w for w in words_tokenized \n",
    "            if not re.fullmatch('[' + string.punctuation + '’“‘”–…' ']', w)]\n",
    "\n",
    "def get_article_metrics(body):\n",
    "    body_tokenized = tokenize_paragraphs(body)\n",
    "    paragraphs_sentences = []\n",
    "    sentences_words = []\n",
    "    for paragraph in body_tokenized['paragraphs']:\n",
    "        paragraphs_sentences.append(paragraph['num_sentences'])\n",
    "        for sentence in paragraph['sentences']:\n",
    "            sentences_words.append(sentence['num_words'])\n",
    "    paragraphs_sentences = pd.Series(paragraphs_sentences)\n",
    "    sentences_words = pd.Series(sentences_words)\n",
    "    metrics = {'article_num_paragraphs': body_tokenized['num_paragraphs'],\n",
    "               'article_num_sentences': paragraphs_sentences.sum(),\n",
    "               'article_num_words': sentences_words.sum(),\n",
    "               'paragraphs_min_sentences': paragraphs_sentences.min(),\n",
    "               'paragraphs_max_sentences': paragraphs_sentences.max(),\n",
    "               'paragraphs_mean_sentences': paragraphs_sentences.mean(),\n",
    "               'paragraphs_median_sentences': paragraphs_sentences.median(),\n",
    "               'sentences_min_words': sentences_words.min(),\n",
    "               'sentences_max_words': sentences_words.max(),\n",
    "               'sentences_mean_words': sentences_words.mean(),\n",
    "               'sentences_median_words': sentences_words.median()}\n",
    "    return metrics\n",
    "\n",
    "# calculate the basic metrics for each article\n",
    "articles_metrics = pd.DataFrame(list(articles['body'].map(get_article_metrics)),\n",
    "                                index=articles.index)\n",
    "articles_metrics.insert(loc=0, column='author', value=articles['author'])\n",
    "display(articles_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how a tokenized article looks like ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display an example for a tokenized article\n",
    "sample_article_index = 17125 # just a random pick\n",
    "body_tokenized = tokenize_paragraphs(articles.loc[sample_article_index]['body'])\n",
    "print('\\nSample article tokenized:\\n')\n",
    "pp.pprint(body_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the basic metrics as features in a pandas data frame we will plot a few scatter diagrams to get a feeling for how strong they separate different authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_scatters(articles_list, authors, plot_features, plot_labels):\n",
    "    num_features = plot_features.__len__()\n",
    "    \n",
    "    # set up the figure \n",
    "    fig, axes = plt.subplots(nrows=num_features, ncols=num_features)\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    fig.suptitle(\"Feature Relationships\", fontsize=18)\n",
    "    \n",
    "    # set up the axes\n",
    "    for ax in axes.flat:\n",
    "        ax.tick_params(axis='both',\n",
    "                       bottom='off', top='off', left='off', right='off',\n",
    "                       labelbottom='off', labeltop='off',\n",
    "                       labelleft='off', labelright='off')\n",
    "        if ax.is_first_col():\n",
    "            ax.yaxis.set_ticks_position('left')\n",
    "            ax.tick_params(axis='y', left='on', labelleft='on')\n",
    "        if ax.is_last_col():\n",
    "            ax.yaxis.set_ticks_position('right')\n",
    "            ax.tick_params(axis='y', right='on', labelright='on')\n",
    "        if ax.is_first_row():\n",
    "            ax.xaxis.set_ticks_position('top')\n",
    "            ax.tick_params(axis='x', top='on', labeltop='on')\n",
    "        if ax.is_last_row():\n",
    "            ax.xaxis.set_ticks_position('bottom')\n",
    "            ax.tick_params(axis='x', bottom='on', labelbottom='on')\n",
    "        # set colour cycler, each author a different colour \n",
    "        ax.set_prop_cycle(cycler('color', ['blue', 'green', 'red', 'cyan',\n",
    "                                           'magenta', 'yellow', 'black', 'white']))\n",
    "        \n",
    "    # plot the scatter plots\n",
    "    for c in range(num_features):\n",
    "        for r in range(num_features):\n",
    "            if c == r:\n",
    "                # leave diagonal plots empty and remove ticks and labels\n",
    "                axes[r, c].xaxis.set_visible(False)\n",
    "                axes[r, c].yaxis.set_visible(False)\n",
    "            elif r > c:\n",
    "                axes[r, c].axis('off')\n",
    "            else:\n",
    "                axes[r, c].set_prop_cycle(None) # reset colour cycler\n",
    "                for author in authors:\n",
    "                    x = articles_list[articles_list.author == author][plot_features[c]]\n",
    "                    y = articles_list[articles_list.author == author][plot_features[r]]\n",
    "                    axes[r, c].scatter(x, y, s=2, alpha=0.5, label=author)\n",
    "\n",
    "    # place a legend below the top left plot\n",
    "    axes[0, 1].legend(bbox_to_anchor=(-1.1, 1), loc=3, prop={'size': 12}, markerscale=6)\n",
    "\n",
    "    # add axis labels in diagonal boxes\n",
    "    for i, label in enumerate(plot_labels):\n",
    "        axes[i,i].annotate(label, (0.5, 0.5), \n",
    "                           xycoords='axes fraction', ha='center', va='center',\n",
    "                           size=12)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1 = plot_feature_scatters(articles_metrics,\n",
    "                ['Brad Feld', 'Jean-Louis Gassée',\n",
    "                 'James Altucher', 'Gary Vaynerchuk', 'Ethan Siegel'],\n",
    "                ['article_num_paragraphs', 'article_num_sentences', 'article_num_words',\n",
    "                 'paragraphs_max_sentences', 'paragraphs_median_sentences',\n",
    "                 'sentences_max_words', 'sentences_median_words'],\n",
    "                ['paragraphs /\\narticle', 'sentences /\\narticle', 'words /\\narticle',\n",
    "                 'max sentences /\\nparagraph', 'median\\nsentences /\\nparagraph',\n",
    "                 'max words /\\nsentence', 'median words /\\nsentence'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plots above it appears that a separation to some degree would be possible. For example, the plot for \"words / article\" vs \"max words / sentence\" shows distinct clusters for individual authors. Although the clusters seem to overlap quite a bit.\n",
    "\n",
    "We will run a few classifier models and see what prediction accuracy we can achieve on a training data set. First, we will try the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels into separate data frame\n",
    "data = articles_metrics\n",
    "labels = data[['author']]\n",
    "data = data.drop('author', 1)\n",
    "\n",
    "# shuffle training test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=23)\n",
    "\n",
    "# decision tree prediction model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "score_train = accuracy_score(y_train, clf.predict(X_train))\n",
    "score_test = accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "# report decision tree results\n",
    "print('decision tree classifier:')\n",
    "print('-------------------------\\n')\n",
    "print('prediction accuracy score on training data set: {:.2f}%'.format(score_train*100))\n",
    "print('prediction accuracy score on test data set: {:.2f}%'.format(score_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly 100% on the training data! That's awesome!\n",
    "\n",
    "Ok.... just joking ;-)\n",
    "\n",
    "On unseen data the accuracy is around 33% (the value will vary slightly with each run). I think that's not too bad for a model that tries to guess the author of an article only based on the number of words in articles, paragraphs and sentences. The model does not take anything of the article content into account. The 100% accuracy on the training data means the model is extremly biased. It would be interesting to see if a grid search can find a better set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_train, y_train):\n",
    "    # setup grid search\n",
    "    cv_sets = ShuffleSplit(n_splits=10, test_size = 0.25, random_state = 23)\n",
    "    learning_model = tree.DecisionTreeClassifier()\n",
    "    params = {'max_depth': list(range(1,30)),\n",
    "              'min_samples_split': list(range(2,50))}\n",
    "    scoring_fnc = make_scorer(accuracy_score, greater_is_better=True)\n",
    "    grid = GridSearchCV(estimator=learning_model,\n",
    "                        param_grid=params,\n",
    "                        scoring=scoring_fnc,\n",
    "                        cv=cv_sets,\n",
    "                        verbose=2)\n",
    "\n",
    "    # run grid search\n",
    "    grid = grid.fit(X_train, y_train)\n",
    "    best_clf = grid.best_estimator_\n",
    "    return best_clf\n",
    "\n",
    "def get_classifier():\n",
    "    '''\n",
    "    Get the best classifier. Run grid search and save the best classifier to\n",
    "    a pickle file. If the pickle file exists load the best classifier from file\n",
    "    instead of running grid search again. This saves time in subsequent runs \n",
    "    of the Notebook.\n",
    "    '''\n",
    "    pickle_file = 'Model 2 Basic Article Metrics - grid search best estimator.pickle'\n",
    "    if os.path.isfile(pickle_file):\n",
    "        with open(pickle_file,'rb') as f:\n",
    "            best_clf = pickle.loads(f.read())\n",
    "    else:\n",
    "        best_clf = grid_search(X_train, y_train)\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            s = pickle.dumps(best_clf)\n",
    "            f.write(s)\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = get_classifier()\n",
    "score_test = accuracy_score(y_test, best_clf.predict(X_test))\n",
    "print('GridSearch prediction accuracy score on test data set: {:.2f}%'.format(score_test*100))\n",
    "print('best parameters:')\n",
    "print('    max_depth:{0}'.format(best_clf.get_params()['max_depth']))\n",
    "print('    min_samples_split:{0}'.format(best_clf.get_params()['min_samples_split']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation of two parameters with grid search gains us three more percentage points to 36%. A little bit better.\n",
    "\n",
    "For comparison we will run the logistic regression algorithm and see if that yields better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression prediction model\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train.values.ravel())\n",
    "score_train = accuracy_score(y_train, clf.predict(X_train))\n",
    "score_test = accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "# report logistic regression results\n",
    "print('\\nlogistic regression classifier:')\n",
    "print('-------------------------------\\n')\n",
    "print('prediction accuracy score on training data set: {:.2f}%'.format(score_train*100))\n",
    "print('prediction accuracy score on test data set: {:.2f}%'.format(score_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 32% accuracy the result is 4% below what has been achieved with an (grid search) optimised decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: use PCA to find the most citical features of the basic article metrics model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Bag-of-Words - Word Count\n",
    "\n",
    "In our next model we will make use of the of the actual words in our prediction model. We will implement a the bag-of-words algorithm. Bag-of-Words is a very simple but often suprisingly effective algorithm. It takes all the different words in a training set and uses them as features. Each article is then transformed into a feature vector by marking the occurrance of each feature (=word). There are a few variations of Bag-of-Words. In it's basic form each vector counts the number of occurences for each word. This is what we are going to use in this Model 3. In the next section (Model 4) will use the term frequency–inverse document frequency (TFIDF) approach to build the Bag-of-Words vectors.\n",
    "\n",
    "The code cell below defines a number of functions. The approach to define functions vs. directly executed code cells has been choosen to make the code reuseable for the next model in the following section.\n",
    "\n",
    "Besides the training, predicting and scoring functionalties the code implementes two additional capabilities: 1.) it allows to save to and load from disk trained classifiers. This saves time when the Notebook is run multiple times. The model needs to be trained only once. 2.) The model can be trained with different classifiers. During experiementation very often different classifiers (e.g. logistic regression, decision tree, SVM) are tested to evaluate which one is most suitable for the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(articles_train, pickle_file, classifiers, vectorizer_type):\n",
    "    \"\"\" This function is called to either load a trained model from disk\n",
    "    or to start the training. A trained model is loaded automatically \n",
    "    when the file exists.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(pickle_file):\n",
    "        vectorizer, trained_classifiers = load_trained_classifiers(pickle_file)\n",
    "    else:\n",
    "        vectorizer, trained_classifiers = train_classifiers(articles_train,\n",
    "                                                            classifiers,\n",
    "                                                            vectorizer_type)\n",
    "        save_trained_classifiers(vectorizer, trained_classifiers, pickle_file)\n",
    "    return vectorizer, trained_classifiers\n",
    "\n",
    "def load_trained_classifiers(pickle_file):\n",
    "    \"\"\" Load a trained model from disk.\n",
    "    \"\"\"\n",
    "    with open(pickle_file,'rb') as f:\n",
    "        saved_items = pickle.loads(f.read())\n",
    "    trained_classifiers = saved_items['trained_classifiers']\n",
    "    vectorizer = saved_items['vectorizer']\n",
    "    return vectorizer, trained_classifiers\n",
    "\n",
    "def save_trained_classifiers(vectorizer, trained_classifiers, pickle_file):\n",
    "    \"\"\" Save a trained model to disk. For Bag-of-Words there are two elements\n",
    "    that need to be saved: the classifiers that have been trained (e.g. logistic\n",
    "    regression, SVM) and the vectorizer. The vectorizer is the object that contains\n",
    "    the vocabulary that has been trained from the training data set. The vecorizer\n",
    "    is required later on to transform any given text into a vector representation.\n",
    "    \"\"\"\n",
    "    saved_items = {'trained_classifiers': trained_classifiers,\n",
    "                   'vectorizer': vectorizer}\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        s = pickle.dumps(saved_items)\n",
    "        f.write(s)\n",
    "\n",
    "def train_classifiers(articles_train, classifiers, vectorizer_type):\n",
    "    \"\"\" This function performs the training for each classifier. The desired \n",
    "    classifieres are given as a list of dictionaries. Each entry spacifies an\n",
    "    'algorithm' and the 'display_text'.\n",
    "    \"\"\"\n",
    "    vectorizer, X_train = fit_transform_vectors(articles_train, vectorizer_type)\n",
    "    y_train = articles_train[['author']]\n",
    "    trained_classifiers = []\n",
    "    for classifier in classifiers:\n",
    "        classifier_algorithm = classifier['algorithm']\n",
    "        display_text = classifier['display_text']\n",
    "        clf = train_classifier(X_train, y_train, classifier_algorithm)\n",
    "        trained_classifiers.append({'clf': clf, 'display_text': classifier['display_text']})\n",
    "    return vectorizer, trained_classifiers\n",
    "\n",
    "def train_classifier(data, labels, classifier_algorithm):\n",
    "    \"\"\" Train a single classifier.\n",
    "    \"\"\"\n",
    "    X_train = data\n",
    "    y_train = labels\n",
    "    clf = classifier_algorithm\n",
    "    clf = clf.fit(X_train, y_train.values.ravel())\n",
    "    return clf\n",
    "\n",
    "def fit_transform_vectors(articles, vectorizer):\n",
    "    \"\"\" Build the vocabulary during training.\n",
    "    \"\"\"\n",
    "    data = [pre_process_text(article['body']) for index, article in articles.iterrows()]\n",
    "    data_vectorized = vectorizer.fit_transform(data)\n",
    "    return vectorizer, data_vectorized\n",
    "\n",
    "def transform_vectors(articles, vectorizer):\n",
    "    \"\"\" Transform a list of articles into Bag-of-Words vectors. This is used\n",
    "    once a mopdel has been trained to transform articles in preparation for\n",
    "    prediction.\n",
    "    \"\"\"\n",
    "    articles = [pre_process_text(article['body']) for index, article in articles.iterrows()]\n",
    "    articles = vectorizer.transform(articles)\n",
    "    return articles\n",
    "\n",
    "def pre_process_text(article):\n",
    "    \"\"\" Some preprocessing and cleaning tasks to be applied to the raw input text\n",
    "    of an article.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    porter = PorterStemmer()\n",
    "    # tokenize article into words\n",
    "    words = tokenize_words(article)['words']\n",
    "    # remove stop words\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    # reduce words to its base and make all words lowercase\n",
    "    words = set([porter.stem(word) for word in words])\n",
    "    processed_text = ' '.join(words)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will utilize the functions above to build our model. An approach has been choosen to define the model as a Python class. This design decision was driven by a view that our model should be \"productized\" to some degree. A user of our WhoWroteThis model will want to use a simple and straighforward interface. She should be able to pass in a list of articles with the raw text and all the activities around text preprocessing, cleaning or vectorizing the data will be hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Bag_of_Words:\n",
    "    \n",
    "    def __init__(self, pickle_file):\n",
    "        \"\"\" The user of this class can specify the file name under which \n",
    "        the trained model will be saved to disk.\n",
    "        \n",
    "        As of now the model is fixed to autmatically train the following\n",
    "        classifiers: logistic regression, decision tree and support vector\n",
    "        machine (SVM).\n",
    "        \"\"\"\n",
    "        self.pickle_file = pickle_file\n",
    "        self.classifiers = [\n",
    "            {'algorithm': linear_model.LogisticRegression(),\n",
    "             'display_text': 'logistic regression classifier'},\n",
    "            {'algorithm': tree.DecisionTreeClassifier(),\n",
    "             'display_text': 'decision tree classifier'},\n",
    "            {'algorithm': svm.SVC(probability=True),\n",
    "             'display_text': 'SVM classifier'}\n",
    "        ]\n",
    "        self.vectorizer = None\n",
    "        self.trained_classifiers = None\n",
    "\n",
    "    def train(self, articles_train, vectorizer_type):\n",
    "        \"\"\" Train the model on the various classifiers.\n",
    "        \"\"\"\n",
    "        self.vectorizer, self.trained_classifiers = train(articles_train,\n",
    "                                                          self.pickle_file,\n",
    "                                                          self.classifiers,\n",
    "                                                          vectorizer_type)\n",
    "\n",
    "    def predict(self, articles, classifier_type):\n",
    "        \"\"\" For a given list of articles and the classifier \n",
    "        type (logistic regression, decision tree and SVM) predict the\n",
    "        authors.\n",
    "        \"\"\"\n",
    "        for clf in self.trained_classifiers:\n",
    "            if clf['display_text'] == classifier_type:\n",
    "                break\n",
    "        articles = transform_vectors(articles, self.vectorizer)\n",
    "        prediction = pd.DataFrame({'author': clf['clf'].classes_,\n",
    "                                   'probability': clf['clf'].predict_proba(articles)[0]})\n",
    "        return prediction\n",
    "    \n",
    "    def score(self, articles):\n",
    "        \"\"\" Score prediction accuracy for a given list of articles.\n",
    "        \"\"\"\n",
    "        label = articles[['author']]\n",
    "        articles = transform_vectors(articles, self.vectorizer)\n",
    "        for clf in self.trained_classifiers:\n",
    "            score = accuracy_score(label, clf['clf'].predict(articles))\n",
    "            print('{} prediction accuracy score : {:.2f}%'.format(clf['display_text'], score*100))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our Bag-of-Words model with word counts. Please note the strongly simplified interface by using the `Model_Bag_of_Words` class. It allows to work directly with the raw article texts and hides the machine learning \"pipeline\" (preprocessing, cleaning, vectorization, training of various classifiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction based on word count\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "model_WordCount = Model_Bag_of_Words('Model 3 Bag-of-Words - Word Count.pickle')\n",
    "print('training the model\\n')\n",
    "model_WordCount.train(articles_train, CountVectorizer())\n",
    "print('number of training articles: {}'.format(len(articles_train)))\n",
    "print('size of the vocabulary = length of the feature vector: {}'.\\\n",
    "      format(len(model_WordCount.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_WordCount.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "model_WordCount.score(articles_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the logistic regression classifier gets us well above 96% prediction accuracy on the test data set. That's not bad! It appears that simply counting the word frequency is already a very strong differentiator between authors. It seems to confirm our intuition that a person is able to differentiate between authors by what they write about. Different topics will naturally use different vocabulary. For future research (not pursued in this project) it might be interesting to research how the classifier works for authors that write about the same or similar topics.\n",
    "\n",
    "From the accuracy scores on training and test data we can see that the SVM classifier doesn't perform so well on this particular problem. This comes a bit as a surprise. According to the [scikit learn documentation](http://scikit-learn.org/stable/modules/svm.html) SVM is a classifier that generally is effective in high dimensional space and, more importantly, is \"still effective in cases where number of dimensions is greater than the number of samples\". In our case the number of 51,770 dimensions (= features) is significantly higher than the 5,756 samples (= number of training articles). Perhaps, because in our case the number of dimensions is almost ten times the number of samples make SVM unsuitable for our problem. The SVM library in scikit learn offers options for optimisation such as various classifiers (SVC, NuSVC and LinearSVC) and different kernel functions. For this project SVM has not been pursued further as the initial results (see above) are way below what the logistic regression classifier achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Bag-of-Words - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction based on term frequency–inverse document frequency (TFIDF)\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.25, random_state=23)\n",
    "model_TFIDF = Model_Bag_of_Words('Model 4 Bag-of-Words - TFIDF.pickle')\n",
    "print('training the model\\n')\n",
    "vectorizer = TfidfVectorizer()\n",
    "model_TFIDF.train(articles_train, vectorizer)\n",
    "print('number of training articles: {}'.format(len(articles_train)))\n",
    "print('size of the vocabulary = length of the feature vector: {}'.\\\n",
    "      format(len(model_TFIDF.vectorizer.vocabulary_)))\n",
    "print('\\nscoring the training data:\\n')\n",
    "model_TFIDF.score(articles_train)\n",
    "print('\\nscoring the test data:\\n')\n",
    "model_TFIDF.score(articles_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have some fun: the WhoWroteThis app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_button_clicked(button):\n",
    "    test_article = pd.DataFrame([text_input_box.value], columns=['body'])\n",
    "    predictions = model_WordCount.predict(test_article, 'logistic regression classifier')\n",
    "    index = prediction['probability'].idxmax()\n",
    "    author = prediction.loc[index]['author']\n",
    "    probability = prediction.loc[index]['probability']*100\n",
    "    print('I\\'m {:.0f} percent certain that this was written by... {}.'.format(probability, author))\n",
    "\n",
    "text_input_box = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Please paste an article here.',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='700px', height='300px')\n",
    ")\n",
    "button = widgets.Button(\n",
    "    description='Who wrote this?',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    ")\n",
    "\n",
    "print('From the list of Medium authors below pick one. Select an article and copy the')\n",
    "print('text in the text box below. Press the \"Who wrote this?\" button and see whether')\n",
    "print('the right author is predicted.\\n')\n",
    "with open('Medium_authors_25.txt','r') as f:\n",
    "    author_urls = f.read()\n",
    "print(author_urls)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(text_input_box, button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display confusion matris\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    im = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size=14)\n",
    "    cb = plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, size=14)\n",
    "    plt.yticks(tick_marks, classes, size=14)\n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max()/2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 size=12,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('true label', size=14)\n",
    "    plt.xlabel('predicted label', size=14)\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_predict)\n",
    "np.set_printoptions(precision=2)\n",
    "class_names = list(set(labels['author']))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "#TODO: Check the total in the confusion matrix. I have the feeling it doesn't add up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*\"I can guarantee you a purity of 96%. I'm proud of that figure. It's a hard-earned figure, 96. However, this other product is 99 maybe even a touch beyond that. [...] but that last 3%, it may not sound like a lot but it is. It's tremendous. It's a tremendous gulf.\"*\n",
    "\n",
    ">Gale Boetticher, Breaking Bad, season 4, episode 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
